# model.py

import inspect
from abc import ABC, abstractmethod
from typing import Dict, Tuple, Type

import numpy as np
from scipy.stats import loguniform
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import (
    ConstantKernel as C,
    Product,
    RBF,
    Sum,
    WhiteKernel,
)
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.neural_network import MLPRegressor


import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import numpy as np
from data_gen import StatefulDataGenerator


# ======================================================================
#                        –ë–ê–ó–û–í–ê –°–¢–†–ê–¢–ï–ì–Ü–Ø
# ======================================================================
class _BaseKernelModel(ABC):
    def __init__(self):
        self._kernel: str | None = None

    # ob–æ–≤‚Äô—è–∑–∫–æ–≤—ñ API-–º–µ—Ç–æ–¥–∏
    @abstractmethod
    def fit(self, X: np.ndarray, Y: np.ndarray) -> None: ...

    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray: ...

    @abstractmethod
    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]: ...

    # kernel –∑ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—é set
    @property
    def kernel(self) -> str | None:  # noqa: D401
        return self._kernel

    @kernel.setter
    def kernel(self, value: str) -> None:
        self._kernel = value.lower() if value else None


# ======================================================================
#                    KRR (Kernel Ridge Regression)
# ======================================================================
class _KRRModel(_BaseKernelModel):
    def __init__(
        self,
        kernel: str = "linear",
        alpha: float = 1.0,
        gamma: float | None = None,
        find_optimal_params: bool = False,
        n_iter_random_search: int = 20,
    ):
        super().__init__()
        self.kernel = kernel
        if self.kernel not in ("linear", "rbf"):
            raise ValueError(
                f"–ù–µ–≤—ñ–¥–æ–º–µ —è–¥—Ä–æ '{self.kernel}' –¥–ª—è KRR. –ü—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è 'linear', 'rbf'."
            )

        self.alpha = alpha
        self.gamma = gamma
        self.find_optimal_params = find_optimal_params
        self.n_iter_random_search = n_iter_random_search

        self.model: KernelRidge | None = None
        self.X_train_: np.ndarray | None = None
        self.dual_coef_: np.ndarray | None = None
        self.coef_: np.ndarray | None = None
        self.intercept_: np.ndarray | None = None

    # ------------------------------------------------------------------
    def fit(self, X: np.ndarray, Y: np.ndarray, config_params: dict = None) -> None:
        """
        –ù–∞–≤—á–∞–Ω–Ω—è KRR –º–æ–¥–µ–ª—ñ.
        
        Args:
            X: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏
            Y: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ
            config_params: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è KRR, –∞–ª–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ)
        """
        # –í—Å—è —ñ—Å–Ω—É—é—á–∞ –ª–æ–≥—ñ–∫–∞ –º–µ—Ç–æ–¥—É –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –±–µ–∑ –∑–º—ñ–Ω
        if self.find_optimal_params:
            self.model = self._run_random_search(X, Y)
        else:
            gamma_eff = (
                self.gamma
                if self.gamma is not None
                else (
                    self._median_gamma(X)
                    if self.kernel == "rbf"
                    else None
                )
            )
            self.model = KernelRidge(alpha=self.alpha, kernel=self.kernel, gamma=gamma_eff)
            self.model.fit(X, Y)
    
        self.X_train_ = X.copy()
        self.dual_coef_ = self.model.dual_coef_
        if self.kernel == "linear":
            self.coef_ = X.T @ self.dual_coef_
            self.intercept_ = np.zeros(Y.shape[1])
        else:
            self.coef_ = None
            self.intercept_ = np.zeros(Y.shape[1])
            
    def predict(self, X: np.ndarray) -> np.ndarray:
        if self.model is None:
            raise RuntimeError("–ú–æ–¥–µ–ª—å KRR –Ω–µ –Ω–∞–≤—á–µ–Ω–∞.")
        return self.model.predict(X)

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        if X0.ndim == 1:
            X0 = X0.reshape(1, -1)

        if self.kernel == "linear":
            return self.coef_, self.intercept_

        gamma_eff = getattr(self.model, "gamma", None) or self._median_gamma(self.X_train_)
        diffs = X0[:, None, :] - self.X_train_[None, :, :]  # shape (1,n,d)
        sq = np.sum(diffs**2, axis=-1)
        K_row = np.exp(-gamma_eff * sq)
        dK_dX = -2 * gamma_eff * diffs * K_row[..., None]  # (1,n,d)

        W = np.einsum("ijk,ji->ki", dK_dX, self.dual_coef_)  # (d,m)
        b = (self.predict(X0) - X0 @ W).flatten()
        return np.clip(W, -1e3, 1e3), np.clip(b, -1e3, 1e3)

    # ------------------------------------------------------------------
    def _run_random_search(self, X, Y) -> KernelRidge:
        base = KernelRidge(kernel=self.kernel)
        if self.kernel == "linear":
            param_dist = {"alpha": loguniform(1e-3, 1e2)}
        else:
            param_dist = {
                "alpha": loguniform(1e-2, 1e2),
                "gamma": loguniform(1e-3, 1e1),
            }

        rs = RandomizedSearchCV(
            base,
            param_dist,
            n_iter=self.n_iter_random_search,
            cv=3,
            scoring="neg_mean_squared_error",
            random_state=42,
            n_jobs=-1,
            verbose=0,
        )
        rs.fit(X, Y)
        return rs.best_estimator_

    @staticmethod
    def _median_gamma(X: np.ndarray) -> float:
        subset = X if X.shape[0] <= 1000 else X[np.random.choice(X.shape[0], 1000, False)]
        d2 = np.sum((subset[:, None] - subset[None]) ** 2, axis=-1)
        med = np.median(d2[np.triu_indices_from(d2, 1)]) or 1.0
        return 1.0 / med


# ======================================================================
#                Gaussian Process Regression
# ======================================================================
class _GPRModel(_BaseKernelModel):
    def __init__(self):
        super().__init__()
        self.models: list[GaussianProcessRegressor] = []

    def fit(self, X: np.ndarray, Y: np.ndarray, config_params: dict = None) -> None:
        """
        –ù–∞–≤—á–∞–Ω–Ω—è GPR –º–æ–¥–µ–ª—ñ.
        
        Args:
            X: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏
            Y: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ
            config_params: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è GPR, –∞–ª–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ)
        """
        # –í—Å—è —ñ—Å–Ω—É—é—á–∞ –ª–æ–≥—ñ–∫–∞ –º–µ—Ç–æ–¥—É –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –±–µ–∑ –∑–º—ñ–Ω
        k_base = (
            C(1.0, (1e-3, 1e3))
            * RBF(1.0, (0.1, 20.0))
            + WhiteKernel(0.1, (1e-5, 1e1))
        )
        self.models = []
        for i in range(Y.shape[1]):
            gpr = GaussianProcessRegressor(
                kernel=k_base,
                alpha=0,
                normalize_y=True,
                n_restarts_optimizer=2,
            )
            gpr.fit(X, Y[:, i])
            self.models.append(gpr)

    def predict(self, X: np.ndarray) -> np.ndarray:
        return np.vstack([m.predict(X) for m in self.models]).T

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        if X0.ndim == 1:
            X0 = X0[None, :]
        W_cols, b_cols = [], []
        for gpr in self.models:
            rbf = self._find_rbf(gpr.kernel_)
            if rbf is None:
                raise RuntimeError("RBF-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç —è–¥—Ä–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
            gamma = 1.0 / (2 * rbf.length_scale**2)
            Xtr, alpha = gpr.X_train_, gpr.alpha_

            diffs = X0[:, None] - Xtr[None]
            sq = np.sum(diffs**2, axis=-1)
            K_row = np.exp(-gamma * sq)
            dK = -2 * gamma * diffs * K_row[..., None]

            W = (dK.squeeze(0).T @ alpha).reshape(-1, 1)
            y0 = gpr.predict(X0)
            b = (y0 - X0 @ W).flatten()
            W_cols.append(W)
            b_cols.append(b)
        return np.hstack(W_cols), np.hstack(b_cols)

    # ------------------------------------------------------------------
    @staticmethod
    def _find_rbf(kernel):
        if isinstance(kernel, RBF):
            return kernel
        if isinstance(kernel, (Product, Sum)):
            return _GPRModel._find_rbf(kernel.k1) or _GPRModel._find_rbf(kernel.k2)
        return None


# ======================================================================
#                   –ù–û–í–ê –ú–û–î–ï–õ–¨ ‚Äì SVR (Support-Vector Regression)
# ======================================================================
class _SVRModel(_BaseKernelModel):
    def __init__(
        self,
        kernel: str = "rbf",
        C: float = 100.0,        # ‚úÖ –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –∫—Ä–∞—â–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ
        epsilon: float = 0.01,   # ‚úÖ –ó–º–µ–Ω—à–µ–Ω–æ –¥–ª—è –º–µ–Ω—à–æ—ó —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—ñ
        gamma: float | None = None,
        degree: int = 3,
        find_optimal_params: bool = False,
        n_iter_random_search: int = 30,
    ):
        super().__init__()
        self.kernel = kernel.lower()
        if self.kernel not in ("linear", "rbf", "poly"):
            raise ValueError("SVR –ø—ñ–¥—Ç—Ä–∏–º—É—î –ª–∏—à–µ 'linear', 'rbf', 'poly'.")

        self.C = C
        self.epsilon = epsilon
        self.gamma = gamma
        self.degree = degree
        self.find_optimal_params = find_optimal_params
        self.n_iter_random_search = n_iter_random_search
        self.models: list[SVR] = []

    def fit(self, X: np.ndarray, Y: np.ndarray, config_params: dict = None) -> None:
        """
        –ù–∞–≤—á–∞–Ω–Ω—è SVR –º–æ–¥–µ–ª—ñ.
        
        Args:
            X: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏
            Y: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ
            config_params: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è SVR, –∞–ª–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ)
        """
        # –í—Å—è —ñ—Å–Ω—É—é—á–∞ –ª–æ–≥—ñ–∫–∞ –º–µ—Ç–æ–¥—É –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –±–µ–∑ –∑–º—ñ–Ω
        n_targets = Y.shape[1]
        self.models.clear()
        self.X_train_ = X.copy()
    
        for k in range(n_targets):
            y = Y[:, k]
    
            if self.find_optimal_params:
                mdl = self._run_random_search(X, y)
            else:
                # –í—Å—ñ —ñ—Å–Ω—É—é—á—ñ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è gamma_eff —Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
                if self.kernel == "rbf":
                    if self.gamma is not None:
                        gamma_eff = self.gamma
                    else:
                        gamma_eff = 1.0 / (X.shape[1] * X.var())
                else:
                    gamma_eff = self.gamma
    
                mdl = SVR(
                    kernel=self.kernel,
                    C=self.C,
                    epsilon=self.epsilon,
                    gamma=gamma_eff,
                    degree=self.degree,
                )
                mdl.fit(X, y)
                mdl._actual_gamma = gamma_eff if gamma_eff is not None else 'scale'
    
            self.models.append(mdl)

    def predict(self, X: np.ndarray) -> np.ndarray:
        if not self.models:
            raise RuntimeError("SVRModel –Ω–µ –Ω–∞–≤—á–µ–Ω–∞.")
        preds = [m.predict(X) for m in self.models]
        return np.vstack(preds).T

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        if X0.ndim == 1:
            X0 = X0[None, :]
    
        W_cols, b_cols = [], []
        for mdl in self.models:
            if self.kernel == "linear":
                W = mdl.coef_.reshape(-1, 1)
                b = mdl.intercept_.copy()
                
            elif self.kernel == "rbf":
                # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ê –ª–æ–≥—ñ–∫–∞ gamma –≤ linearize()
                if hasattr(mdl, '_actual_gamma') and isinstance(mdl._actual_gamma, float):
                    gamma_eff = mdl._actual_gamma
                else:
                    # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ê sklearn —Ñ–æ—Ä–º—É–ª–∞
                    gamma_eff = 1.0 / (self.X_train_.shape[1] * self.X_train_.var())
                
                sv = mdl.support_vectors_
                coef = mdl.dual_coef_.ravel()
    
                # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ï –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤:
                diffs = X0[:, None, :] - sv[None, :, :]  # (1, n_sv, n_features)
                sq = np.sum(diffs**2, axis=-1)           # (1, n_sv)
                K_row = np.exp(-gamma_eff * sq)          # (1, n_sv)
                dK = -2 * gamma_eff * diffs * K_row[..., None]  # (1, n_sv, n_features)
    
                # ‚úÖ –ë–ï–ó–ü–ï–ß–ù–ï –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤:
                if dK.shape[0] == 1:
                    dK_2d = dK[0]  # (n_sv, n_features) 
                else:
                    dK_2d = dK.squeeze(0)  # Backup
                    
                W = (dK_2d.T @ coef).reshape(-1, 1)  # (n_features, 1)
                y0 = mdl.predict(X0)
                b = (y0 - X0 @ W).flatten()
                
            else:
                raise NotImplementedError(
                    "–õ—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—è SVR –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è –ª–∏—à–µ –¥–ª—è 'linear' —Ç–∞ 'rbf'."
                )
    
            W_cols.append(np.clip(W, -1e3, 1e3))
            b_cols.append(np.clip(b, -1e3, 1e3))
    
        W_local = np.hstack(W_cols)
        b_local = np.hstack(b_cols)
        return W_local, b_local

    def _run_random_search(self, X, y) -> SVR:
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –≤–µ—Ä—Å—ñ—è –∑ —à–≤–∏–¥—à–æ—é –æ–±—Ä–æ–±–∫–æ—é linear kernel"""
        
        # üöÄ –®–í–ò–î–ö–ê –õ–û–ì–Ü–ö–ê –î–õ–Ø LINEAR KERNEL
        if self.kernel == "linear":
            # Linear kernel –Ω–µ –ø–æ—Ç—Ä–µ–±—É—î —Å–∫–ª–∞–¥–Ω–æ—ó –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
            param_dist = {
                "C": [1.0, 10.0, 100.0],           # –î–∏—Å–∫—Ä–µ—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
                "epsilon": [0.001, 0.01, 0.1]      # –¢–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
            }
            n_iter = 9  # 3x3 = –≤—Å—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó
            cv_folds = 2  # –ú–µ–Ω—à–µ —Ñ–æ–ª–¥—ñ–≤
            
            print(f"üöÄ SVR Linear: —à–≤–∏–¥–∫–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è {n_iter} –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π...")
            
        elif self.kernel == "rbf":
            # –ü–æ–≤–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è RBF
            param_dist = {
                "C": loguniform(10, 1000),      
                "epsilon": loguniform(1e-3, 0.1),
                "gamma": loguniform(1e-4, 1e-1)
            }
            n_iter = self.n_iter_random_search
            cv_folds = min(3, len(y) // 50)
            
            print(f"üîß SVR RBF: –ø–æ–≤–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è {n_iter} —ñ—Ç–µ—Ä–∞—Ü—ñ–π...")
            
        elif self.kernel == "poly":
            # –û–±–º–µ–∂–µ–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è poly
            param_dist = {
                "C": loguniform(10, 500),        # –ú–µ–Ω—à–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω
                "epsilon": loguniform(1e-3, 0.05),
                "gamma": loguniform(1e-4, 1e-2), # –ú–µ–Ω—à–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω
                "degree": [2, 3]                 # –¢—ñ–ª—å–∫–∏ 2-3 —Å—Ç–µ–ø–µ–Ω—ñ
            }
            n_iter = min(20, self.n_iter_random_search)
            cv_folds = min(3, len(y) // 50)
            
            print(f"‚öôÔ∏è SVR Poly: –æ–±–º–µ–∂–µ–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è {n_iter} —ñ—Ç–µ—Ä–∞—Ü—ñ–π...")
        
        else:
            raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π kernel: {self.kernel}")
    
        base = SVR(kernel=self.kernel, degree=self.degree)
        
        # üöÄ –í–ò–ö–û–†–ò–°–¢–û–í–£–Ñ–ú–û GridSearchCV –¥–ª—è linear (—à–≤–∏–¥—à–µ)
        if self.kernel == "linear":
            from sklearn.model_selection import GridSearchCV
            rs = GridSearchCV(
                base,
                param_dist,  # –¢—É—Ç —Ü–µ –±—É–¥–µ dict –∑ lists
                cv=cv_folds,
                scoring="neg_mean_squared_error",
                n_jobs=-1,
                verbose=1  # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
            )
        else:
            # RandomizedSearchCV –¥–ª—è —ñ–Ω—à–∏—Ö kernels
            rs = RandomizedSearchCV(
                base,
                param_dist,
                n_iter=n_iter,
                cv=cv_folds,
                scoring="neg_mean_squared_error",
                random_state=42,
                n_jobs=-1,
                verbose=1  # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
            )
        
        rs.fit(X, y)
        
        # ‚úÖ –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–µ gamma
        best_model = rs.best_estimator_
        if hasattr(best_model, 'gamma'):
            best_model._actual_gamma = best_model.gamma
        
        print(f"‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏: {rs.best_params_}")
        
        return best_model

# ======================================================================
#                   LINEAR MODEL (–¥–ª—è L-MPC)
# ======================================================================

class _LinearModel(_BaseKernelModel):
    """
    –õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è L-MPC –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é —Ä—ñ–∑–Ω–∏—Ö —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ–π —Ç–∞ –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫
    """
    
    def __init__(
        self,
        linear_type: str = "ols",         # "ols", "ridge", "lasso" 
        alpha: float = 1.0,               # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó –¥–ª—è Ridge/Lasso
        poly_degree: int = 1,             # –°—Ç–µ–ø—ñ–Ω—å –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ (1=–ª—ñ–Ω—ñ–π–Ω–∞)
        include_bias: bool = True,        # –í–∫–ª—é—á–∞—Ç–∏ bias —Ç–µ—Ä–º–∏–Ω
        find_optimal_params: bool = False, # –ü–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        n_iter_random_search: int = 20,
    ):
        super().__init__()
        self.linear_type = linear_type.lower()
        self.alpha = alpha
        self.poly_degree = poly_degree
        self.include_bias = include_bias
        self.find_optimal_params = find_optimal_params
        self.n_iter_random_search = n_iter_random_search
        
        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
        if self.linear_type not in ("ols", "ridge", "lasso"):
            raise ValueError("linear_type –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ 'ols', 'ridge' –∞–±–æ 'lasso'")
        
        if self.poly_degree < 1 or self.poly_degree > 3:
            raise ValueError("poly_degree –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –≤—ñ–¥ 1 –¥–æ 3")
        
        # –í–Ω—É—Ç—Ä—ñ—à–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏
        self.model: LinearRegression | Ridge | Lasso | None = None
        self.poly_features: PolynomialFeatures | None = None
        self.coef_: np.ndarray | None = None
        self.intercept_: np.ndarray | None = None
        
        # –î–ª—è compatibility –∑ kernel –º–æ–¥–µ–ª—è–º–∏
        self._kernel = "linear"  # –ü–æ–∑–Ω–∞—á–∞—î–º–æ —è–∫ –ª—ñ–Ω—ñ–π–Ω—É

    def fit(self, X: np.ndarray, Y: np.ndarray, config_params: dict = None) -> None:
        """
        –ù–∞–≤—á–∞–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ.
        
        Args:
            X: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏
            Y: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ
            config_params: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è Linear, –∞–ª–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ)
        """
        # –í—Å—è —ñ—Å–Ω—É—é—á–∞ –ª–æ–≥—ñ–∫–∞ –º–µ—Ç–æ–¥—É –∑–∞–ª–∏—à–∞—î—Ç—å—Å—è –±–µ–∑ –∑–º—ñ–Ω
        print(f"üîß –ù–∞–≤—á–∞–Ω–Ω—è Linear Model: {self.linear_type}, poly_degree={self.poly_degree}")
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if self.poly_degree > 1:
            self.poly_features = PolynomialFeatures(
                degree=self.poly_degree,
                include_bias=False
            )
            X_features = self.poly_features.fit_transform(X)
        else:
            self.poly_features = None
            X_features = X
            
        # –í–∏–±—ñ—Ä —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        if self.find_optimal_params:
            self.model = self._run_random_search(X_features, Y)
        else:
            if self.linear_type == "ols":
                self.model = LinearRegression(fit_intercept=self.include_bias)
            elif self.linear_type == "ridge":
                self.model = Ridge(alpha=self.alpha, fit_intercept=self.include_bias)
            elif self.linear_type == "lasso":
                self.model = Lasso(alpha=self.alpha, fit_intercept=self.include_bias, max_iter=2000)
        
        # –ù–∞–≤—á–∞–Ω–Ω—è
        self.model.fit(X_features, Y)
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É
        self.coef_ = self.model.coef_.T if Y.ndim > 1 else self.model.coef_.reshape(-1, 1)
        self.intercept_ = (
            self.model.intercept_ if hasattr(self.model, 'intercept_') 
            else np.zeros(Y.shape[1] if Y.ndim > 1 else 1)
        )
        
        print(f"‚úÖ Linear Model –Ω–∞–≤—á–µ–Ω–∞: {self.linear_type}, poly_degree={self.poly_degree}")
        print(f"   –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ shape: {self.coef_.shape}, Intercept: {self.intercept_.shape}")
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ"""
        if self.model is None:
            raise RuntimeError("Linear Model –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
            
        # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if self.poly_features is not None:
            X_features = self.poly_features.transform(X)
        else:
            X_features = X
            
        return self.model.predict(X_features)

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        –õ—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—è –¥–ª—è –ª—ñ–Ω—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ
        
        Returns:
            W: (n_features, n_outputs) - –≥—Ä–∞–¥—ñ—î–Ω—Ç –º–∞—Ç—Ä–∏—Ü—è (—è–∫ —É K-MPC)
            b: (n_outputs,) - –∑–º—ñ—â–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä
        """
        if X0.ndim == 1:
            X0 = X0.reshape(1, -1)
            
        if self.coef_ is None:
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
            
        # üéØ –õ–Ü–ù–Ü–ô–ù–ò–ô –í–ò–ü–ê–î–û–ö (poly_degree = 1)
        if self.poly_degree == 1:
            # sklearn LinearRegression: coef_ = (n_features, n_outputs)
            W = self.coef_  # ‚úÖ –ó–∞–ª–∏—à–∞—î–º–æ (n_features, n_outputs) —è–∫ —É K-MPC
            b = self.intercept_  # (n_outputs,)
            return W, b
            
        # üéØ –ü–û–õ–Ü–ù–û–ú–Ü–ê–õ–¨–ù–ò–ô –í–ò–ü–ê–î–û–ö (poly_degree > 1)
        else:
            # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–û: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ include_bias=False
            grad_poly = self._compute_polynomial_gradient(X0)  # (n_samples, n_features, n_poly_features_no_bias)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ
            n_samples, n_features, n_poly_features = grad_poly.shape
            n_coef_features, n_outputs = self.coef_.shape
            
            if n_poly_features != n_coef_features:
                raise ValueError(f"–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ {n_poly_features} != —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤ {n_coef_features}")
            
            # W = –≥—Ä–∞–¥—ñ—î–Ω—Ç * –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
            W = np.einsum('ijk,kl->ijl', grad_poly, self.coef_)  # (n_samples, n_features, n_outputs)
            
            # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à–∏–π —Å–µ–º–ø–ª: (n_features, n_outputs) - —è–∫ —É K-MPC
            W_local = W[0]  # ‚úÖ (n_features, n_outputs)
            
            # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ï –ú–ù–û–ñ–ï–ù–ù–Ø: X @ W (—è–∫ —É K-MPC)
            y0 = self.predict(X0)
            b_local = y0[0] - X0[0] @ W_local
            
            return W_local, b_local

    def _compute_polynomial_gradient(self, X0: np.ndarray) -> np.ndarray:
        """
        –û–±—á–∏—Å–ª—é—î –≥—Ä–∞–¥—ñ—î–Ω—Ç –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ –≤ —Ç–æ—á—Ü—ñ X0
        
        Returns:
            grad_poly: (n_samples, n_features, n_poly_features) - –≥—Ä–∞–¥—ñ—î–Ω—Ç –º–∞—Ç—Ä–∏—Ü—è
        """
        from sklearn.preprocessing import PolynomialFeatures
        
        n_samples, n_features = X0.shape
        
        # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–û: include_bias=False, —â–æ–± –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ sklearn coef_
        poly = PolynomialFeatures(degree=self.poly_degree, include_bias=False)
        
        # –§—ñ—Ç—É—î–º–æ –Ω–∞ dummy –¥–∞–Ω–∏—Ö —â–æ–± –æ—Ç—Ä–∏–º–∞—Ç–∏ powers_
        dummy_X = np.ones((1, n_features))
        poly.fit(dummy_X)
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –ë–ï–ó bias
        n_poly_features = len(poly.powers_)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç
        grad_poly = np.zeros((n_samples, n_features, n_poly_features))
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç –¥–ª—è –∫–æ–∂–Ω–æ—ó –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–æ—ó –æ–∑–Ω–∞–∫–∏
        for i, powers in enumerate(poly.powers_):
            # powers - –º–∞—Å–∏–≤ —Å—Ç–µ–ø–µ–Ω—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ—ó –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ—ó –æ–∑–Ω–∞–∫–∏
            for j in range(n_features):
                if powers[j] > 0:
                    # ‚àÇ(x‚ÇÅ^p‚ÇÅ * x‚ÇÇ^p‚ÇÇ * ... * x‚±º^p‚±º * ...)/‚àÇx‚±º = p‚±º * x‚ÇÅ^p‚ÇÅ * ... * x‚±º^(p‚±º-1) * ...
                    grad_powers = powers.copy()
                    grad_powers[j] -= 1
                    
                    # –û–±—á–∏—Å–ª—é—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞
                    grad_value = powers[j]  # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –≤—ñ–¥ –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ—é–≤–∞–Ω–Ω—è
                    
                    for k in range(n_features):
                        if grad_powers[k] > 0:
                            grad_value *= (X0[:, k] ** grad_powers[k])
                        # –Ø–∫—â–æ grad_powers[k] == 0, —Ç–æ x^0 = 1 (–Ω–µ –º–Ω–æ–∂–∏–º–æ)
                    
                    grad_poly[:, j, i] = grad_value
        
        return grad_poly

    def _run_random_search(self, X, Y):
        """–ü–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤"""
        
        if self.linear_type == "ols":
            # OLS –Ω–µ –º–∞—î –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
            return LinearRegression(fit_intercept=self.include_bias)
            
        elif self.linear_type == "ridge":
            param_dist = {"alpha": loguniform(1e-3, 1e3)}
            base = Ridge(fit_intercept=self.include_bias)
            
        elif self.linear_type == "lasso":
            param_dist = {"alpha": loguniform(1e-4, 1e1)}
            base = Lasso(fit_intercept=self.include_bias, max_iter=2000)
            
        rs = RandomizedSearchCV(
            base,
            param_dist,
            n_iter=self.n_iter_random_search,
            cv=3,
            scoring="neg_mean_squared_error",
            random_state=42,
            n_jobs=-1,
            verbose=1
        )
        
        rs.fit(X, Y)
        print(f"‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ Linear {self.linear_type}: {rs.best_params_}")
        
        return rs.best_estimator_
    
class _NeuralNetworkModel(_BaseKernelModel):
    """
    –ù–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞ (MLP) –¥–ª—è —Å–∏—Å—Ç–µ–º–∏ MPC –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó.
    –†–µ–∞–ª—ñ–∑—É—î —Ç–æ–π —Å–∞–º–∏–π —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å, —â–æ –π —ñ–Ω—à—ñ –º–æ–¥–µ–ª—ñ —É —Å–∏—Å—Ç–µ–º—ñ.
    """
    
    def __init__(
        self,
        hidden_layer_sizes: tuple = (50, 25),  # –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –º–µ—Ä–µ–∂—ñ
        activation: str = 'relu',              # –§—É–Ω–∫—Ü—ñ—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó
        solver: str = 'adam',                  # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä
        alpha: float = 0.001,                  # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
        learning_rate_init: float = 0.001,     # –ü–æ—á–∞—Ç–∫–æ–≤–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è
        max_iter: int = 2000,                  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö
        early_stopping: bool = True,           # –†–∞–Ω–Ω—è –∑—É–ø–∏–Ω–∫–∞
        validation_fraction: float = 0.1,      # –ß–∞—Å—Ç–∫–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –¥–ª—è —Ä–∞–Ω–Ω—å–æ—ó –∑—É–ø–∏–Ω–∫–∏
        n_iter_no_change: int = 20,           # –¢–µ—Ä–ø—ñ–Ω–Ω—è –¥–ª—è —Ä–∞–Ω–Ω—å–æ—ó –∑—É–ø–∏–Ω–∫–∏
        find_optimal_params: bool = False,     # –ê–≤—Ç–æ–ø–æ—à—É–∫ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        n_iter_random_search: int = 30,        # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π –≤–∏–ø–∞–¥–∫–æ–≤–æ–≥–æ –ø–æ—à—É–∫—É
        random_state: int = 42                 # –§—ñ–∫—Å–∞—Ü—ñ—è –≤–∏–ø–∞–¥–∫–æ–≤–æ—Å—Ç—ñ
    ):
        super().__init__()
        self.hidden_layer_sizes = hidden_layer_sizes
        self.activation = activation
        self.solver = solver
        self.alpha = alpha
        self.learning_rate_init = learning_rate_init
        self.max_iter = max_iter
        self.early_stopping = early_stopping
        self.validation_fraction = validation_fraction
        self.n_iter_no_change = n_iter_no_change
        self.find_optimal_params = find_optimal_params
        self.n_iter_random_search = n_iter_random_search
        self.random_state = random_state
        
        # –í–Ω—É—Ç—Ä—ñ—à–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏
        self.model: MLPRegressor | None = None
        self.n_features_: int | None = None
        self.n_outputs_: int | None = None
        
        # –î–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ kernel interface
        self._kernel = "neural_network"

    def fit(self, X: np.ndarray, Y: np.ndarray, config_params: dict = None) -> None:
        """
        –ù–∞–≤—á–∞–Ω–Ω—è –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤.
        
        Args:
            X: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏
            Y: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ
            config_params: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó (–≤–∫–ª—é—á–∞—é—á–∏ param_search_space)
        """
        print(f"üß† –ù–∞–≤—á–∞–Ω–Ω—è Neural Network...")
        print(f"   –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞: {self.hidden_layer_sizes}")
        print(f"   –ê–∫—Ç–∏–≤–∞—Ü—ñ—è: {self.activation}, Solver: {self.solver}")
        
        self.n_features_ = X.shape[1]
        self.n_outputs_ = Y.shape[1] if Y.ndim > 1 else 1
        
        if self.find_optimal_params:
            print(f"üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –ø–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤...")
            # –ü–µ—Ä–µ–¥–∞—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–æ –º–µ—Ç–æ–¥—É –ø–æ—à—É–∫—É
            self.model = self._run_random_search(X, Y, config_params)
        else:
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ –∑–∞–¥–∞–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–±–µ–∑ –∑–º—ñ–Ω)
            self.model = MLPRegressor(
                hidden_layer_sizes=self.hidden_layer_sizes,
                activation=self.activation,
                solver=self.solver,
                alpha=self.alpha,
                learning_rate_init=self.learning_rate_init,
                max_iter=self.max_iter,
                early_stopping=self.early_stopping,
                validation_fraction=self.validation_fraction,
                n_iter_no_change=self.n_iter_no_change,
                random_state=self.random_state
            )
            
            print(f"üìö –ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {X.shape[0]} –∑—Ä–∞–∑–∫–∞—Ö...")
            self.model.fit(X, Y)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ü—ñ—ó (–±–µ–∑ –∑–º—ñ–Ω)
        if hasattr(self.model, 'n_iter_'):
            print(f"‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {self.model.n_iter_} –µ–ø–æ—Ö")
            if self.model.n_iter_ >= self.max_iter:
                print(f"‚ö†Ô∏è  –î–æ—Å—è–≥–Ω—É—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö. –ú–æ–∂–ª–∏–≤–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–±—ñ–ª—å—à–∏—Ç–∏ max_iter")
        
        print(f"   –§—ñ–Ω–∞–ª—å–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç: {getattr(self.model, 'loss_', '–Ω–µ–≤—ñ–¥–æ–º–∞'):.6f}")
        
    def predict(self, X: np.ndarray) -> np.ndarray:
        """–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –Ω–∞–≤—á–µ–Ω–æ—ó –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ."""
        if self.model is None:
            raise RuntimeError("–ù–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞ –Ω–µ –Ω–∞–≤—á–µ–Ω–∞. –í–∏–∫–ª–∏—á—Ç–µ fit() —Å–ø–æ—á–∞—Ç–∫—É.")
        
        predictions = self.model.predict(X)
        
        # –ó–∞–±–µ–∑–ø–µ—á—É—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É —Ñ–æ—Ä–º—É –≤–∏—Ö–æ–¥—É
        if predictions.ndim == 1 and self.n_outputs_ > 1:
            predictions = predictions.reshape(-1, self.n_outputs_)
        elif predictions.ndim == 1:
            predictions = predictions.reshape(-1, 1)
            
        return predictions

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        –õ—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—è –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ –≤ —Ç–æ—á—Ü—ñ X0 –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —á–∏—Å–ª–æ–≤–æ–≥–æ –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ—é–≤–∞–Ω–Ω—è.
        
        –ü–æ–≤–µ—Ä—Ç–∞—î:
            W: (n_features, n_outputs) - –º–∞—Ç—Ä–∏—Ü—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ (—è–∫–æ–±—ñ–∞–Ω)
            b: (n_outputs,) - –≤–µ–∫—Ç–æ—Ä –∑–º—ñ—â–µ–Ω–Ω—è –¥–ª—è –ª—ñ–Ω—ñ–π–Ω–æ—ó –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü—ñ—ó
        """
        if self.model is None:
            raise RuntimeError("–ù–µ–π—Ä–æ–Ω–Ω–∞ –º–µ—Ä–µ–∂–∞ –Ω–µ –Ω–∞–≤—á–µ–Ω–∞.")
        
        if X0.ndim == 1:
            X0 = X0.reshape(1, -1)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è —á–∏—Å–ª–æ–≤–æ–≥–æ –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ—é–≤–∞–Ω–Ω—è
        epsilon = 1e-7  # –ö—Ä–æ–∫ –¥–ª—è –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—É
        n_features = X0.shape[1]
        
        # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –≤ —Ç–æ—á—Ü—ñ X0
        y0 = self.predict(X0)[0]  # (n_outputs,)
        n_outputs = len(y0)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–∞—Ç—Ä–∏—Ü—ñ —è–∫–æ–±—ñ–∞–Ω—É
        W = np.zeros((n_features, n_outputs))
        
        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è —á–∞—Å—Ç–∫–æ–≤–∏—Ö –ø–æ—Ö—ñ–¥–Ω–∏—Ö –¥–ª—è –∫–æ–∂–Ω–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
        for i in range(n_features):
            # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–±—É—Ä–µ–Ω—É —Ç–æ—á–∫—É
            X_plus = X0.copy()
            X_minus = X0.copy()
            
            X_plus[0, i] += epsilon
            X_minus[0, i] -= epsilon
            
            # –û–±—á–∏—Å–ª—é—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç –º–µ—Ç–æ–¥–æ–º —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∏—Ö —Ä—ñ–∑–Ω–∏—Ü—å
            y_plus = self.predict(X_plus)[0]
            y_minus = self.predict(X_minus)[0]
            
            # –ß–∞—Å—Ç–∫–æ–≤–∞ –ø–æ—Ö—ñ–¥–Ω–∞ –¥–ª—è i-—ó –∑–º—ñ–Ω–Ω–æ—ó
            grad_i = (y_plus - y_minus) / (2 * epsilon)
            W[i, :] = grad_i
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –∑–º—ñ—â–µ–Ω–Ω—è –¥–ª—è –ª—ñ–Ω—ñ–π–Ω–æ—ó –∞–ø—Ä–æ–∫—Å–∏–º–∞—Ü—ñ—ó
        # y ‚âà y0 + W^T * (x - x0) = (y0 - W^T * x0) + W^T * x
        # –¢–æ–º—É b = y0 - W^T * x0
        b = y0 - X0[0] @ W
        
        # –û–±–º–µ–∂–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        W = np.clip(W, -1e3, 1e3)
        b = np.clip(b, -1e3, 1e3)
        
        return W, b

    def _run_random_search(self, X: np.ndarray, Y: np.ndarray, config_params: dict = None) -> MLPRegressor:
        """
        –í–∏–ø–∞–¥–∫–æ–≤–∏–π –ø–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ.
        –°–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è –±–µ–∑ –Ω–µ–ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π.
        """
        
        print(f"üéØ –í–∏–ø–∞–¥–∫–æ–≤–∏–π –ø–æ—à—É–∫ —Å–µ—Ä–µ–¥ {self.n_iter_random_search} –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π...")
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –ø—Ä–æ—Å—Ç—ñ—Ä –ø–æ—à—É–∫—É
        if config_params and 'param_search_space' in config_params:
            print(f"üìã –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –æ–±–º–µ–∂–µ–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä –ø–æ—à—É–∫—É –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó")
            custom_space = config_params['param_search_space']
            
            param_dist = {}
            
            # –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞
            if 'hidden_layer_sizes' in custom_space:
                param_dist['hidden_layer_sizes'] = custom_space['hidden_layer_sizes']
            else:
                param_dist['hidden_layer_sizes'] = [(50,), (100,), (50, 25), (100, 50)]
                
            # –§—É–Ω–∫—Ü—ñ—ó –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó  
            if 'activation' in custom_space:
                param_dist['activation'] = custom_space['activation']
            else:
                param_dist['activation'] = ['relu', 'tanh']
                
            # –û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∏ - —Ç—ñ–ª—å–∫–∏ –Ω–∞–¥—ñ–π–Ω—ñ
            if 'solver' in custom_space:
                param_dist['solver'] = custom_space['solver']
            else:
                param_dist['solver'] = ['adam']  # –¢—ñ–ª—å–∫–∏ Adam –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
                
            # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
            if 'alpha' in custom_space:
                if isinstance(custom_space['alpha'], list):
                    param_dist['alpha'] = custom_space['alpha']
                else:
                    param_dist['alpha'] = loguniform(1e-5, 1e-1)
            else:
                param_dist['alpha'] = loguniform(1e-5, 1e-1)
                
            # –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è
            if 'learning_rate_init' in custom_space:
                if isinstance(custom_space['learning_rate_init'], list):
                    param_dist['learning_rate_init'] = custom_space['learning_rate_init']
                else:
                    param_dist['learning_rate_init'] = loguniform(1e-4, 1e-1)
            else:
                param_dist['learning_rate_init'] = loguniform(1e-4, 1e-1)
                
        else:
            print(f"üìã –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä –ø–æ—à—É–∫—É")
            # –°–ø—Ä–æ—â–µ–Ω–∏–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π –ø—Ä–æ—Å—Ç—ñ—Ä –ø–æ—à—É–∫—É
            param_dist = {
                'hidden_layer_sizes': [
                    (50,), (100,), (50, 25), (100, 50), (100, 50, 25), (150, 75)
                ],
                'activation': ['relu', 'tanh'],
                'solver': ['adam'],  # –¢—ñ–ª—å–∫–∏ –Ω–∞–¥—ñ–π–Ω–∏–π Adam
                'alpha': loguniform(1e-5, 1e-1),
                'learning_rate_init': loguniform(1e-4, 1e-2)
            }
        
        # –ë–∞–∑–æ–≤–∞ –º–æ–¥–µ–ª—å –∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è Adam
        base_model = MLPRegressor(
            max_iter=2000,                    # –î–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–ª—è –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ Adam
            early_stopping=True,              # Adam –¥–æ–±—Ä–µ –ø—Ä–∞—Ü—é—î –∑ early stopping
            validation_fraction=0.15,         # –¢—Ä–æ—Ö–∏ –±—ñ–ª—å—à–µ –¥–∞–Ω–∏—Ö –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
            n_iter_no_change=25,             # –ë—ñ–ª—å—à–µ —Ç–µ—Ä–ø—ñ–Ω–Ω—è
            random_state=self.random_state
        )
        
        # –ü—Ä–∏–≥–Ω—ñ—á—É—î–º–æ –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è –ø—Ä–æ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å –ø—ñ–¥ —á–∞—Å –ø–æ—à—É–∫—É
        import warnings
        from sklearn.exceptions import ConvergenceWarning
        
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=ConvergenceWarning)
            
            random_search = RandomizedSearchCV(
                base_model,
                param_dist,
                n_iter=self.n_iter_random_search,
                cv=3,
                scoring='neg_mean_squared_error',
                random_state=self.random_state,
                n_jobs=-1,
                verbose=0
            )
            
            print(f"üîç –ó–∞–ø—É—Å–∫ –ø–æ—à—É–∫—É...")
            random_search.fit(X, Y)
        
        print(f"‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:")
        for param, value in random_search.best_params_.items():
            print(f"   ‚Ä¢ {param}: {value}")
        print(f"   ‚Ä¢ –ù–∞–π–∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç CV: {-random_search.best_score_:.6f}")
        
        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å
        best_model = random_search.best_estimator_
        
        # –Ø–∫—â–æ –º–æ–¥–µ–ª—å –≤—Å–µ —â–µ –º–∞—î –ø—Ä–æ–±–ª–µ–º–∏ –∑—ñ –∑–±—ñ–∂–Ω—ñ—Å—Ç—é, –¥–∞—î–º–æ —ó–π —â–µ –æ–¥–∏–Ω —à–∞–Ω—Å
        if hasattr(best_model, 'n_iter_') and best_model.n_iter_ >= best_model.max_iter - 10:
            print(f"üîÑ –ú–æ–¥–µ–ª—å –±–ª–∏–∑—å–∫–∞ –¥–æ –ª—ñ–º—ñ—Ç—É —ñ—Ç–µ—Ä–∞—Ü—ñ–π. –ü–µ—Ä–µ–Ω–∞–≤—á–∞—î–º–æ –∑ –±—ñ–ª—å—à–∏–º max_iter...")
            
            # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ø—ñ—é –∑ –∑–±—ñ–ª—å—à–µ–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            final_model = MLPRegressor(
                hidden_layer_sizes=best_model.hidden_layer_sizes,
                activation=best_model.activation,
                solver=best_model.solver,
                alpha=best_model.alpha,
                learning_rate_init=getattr(best_model, 'learning_rate_init', 0.001),
                max_iter=4000,                # –ü–æ–¥–≤–æ—é—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —ñ—Ç–µ—Ä–∞—Ü—ñ–π
                early_stopping=True,
                validation_fraction=0.15,
                n_iter_no_change=30,         # –©–µ –±—ñ–ª—å—à–µ —Ç–µ—Ä–ø—ñ–Ω–Ω—è
                random_state=self.random_state
            )
            
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=ConvergenceWarning)
                final_model.fit(X, Y)
                
            print(f"‚úÖ –ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {getattr(final_model, 'n_iter_', '–Ω–µ–≤—ñ–¥–æ–º–æ')} —ñ—Ç–µ—Ä–∞—Ü—ñ–π")
            return final_model
        
        return best_model    
    
    def get_model_info(self) -> dict:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –Ω–∞–≤—á–µ–Ω—É –º–æ–¥–µ–ª—å."""
        if self.model is None:
            return {"status": "–Ω–µ –Ω–∞–≤—á–µ–Ω–∞"}
        
        info = {
            "status": "–Ω–∞–≤—á–µ–Ω–∞",
            "architecture": self.hidden_layer_sizes,
            "activation": self.activation,
            "solver": self.solver,
            "n_features": self.n_features_,
            "n_outputs": self.n_outputs_,
            "n_epochs": getattr(self.model, 'n_iter_', '–Ω–µ–≤—ñ–¥–æ–º–æ'),
            "final_loss": getattr(self.model, 'loss_', '–Ω–µ–≤—ñ–¥–æ–º–∞')
        }
        
        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–∞
        if hasattr(self.model, 'coefs_'):
            total_params = sum(w.size for w in self.model.coefs_) + sum(b.size for b in self.model.intercepts_)
            info["total_parameters"] = total_params
            
        return info
    
# ======================================================================
#                              FACADE
# ======================================================================
class KernelModel:
    """
    –Ø–≤–ª—è—î —Å–æ–±–æ—é ¬´—Ç–æ–Ω–∫–∏–π¬ª —Ñ–∞—Å–∞–¥.  
    –ó–∞ `model_type` –≤–∏–±–∏—Ä–∞—î—Ç—å—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è, –∞ –∑–∞–π–≤—ñ kwargs
    –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤—ñ–¥–∫–∏–¥–∞—é—Ç—å—Å—è (—â–æ–± –Ω–µ –≤–∏–Ω–∏–∫–∞–ª–æ TypeError).
    """

    _REGISTRY: Dict[str, Type[_BaseKernelModel]] = {
        "krr": _KRRModel,
        "gpr": _GPRModel,
        "svr": _SVRModel,
        "linear": _LinearModel,
        "nn": _NeuralNetworkModel,      # üÜï –î–û–î–ê–ù–û Neural Network!
        "neural": _NeuralNetworkModel,  # üÜï –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞ –Ω–∞–∑–≤–∞
    }

    def __init__(self, model_type: str = "krr", **kwargs):
        mtype = model_type.lower()
        if mtype not in self._REGISTRY:
            raise ValueError(f"–ù–µ–≤—ñ–¥–æ–º–∞ –º–æ–¥–µ–ª—å '{model_type}'. –î–æ—Å—Ç—É–ø–Ω—ñ: {list(self._REGISTRY)}")

        impl_cls = self._REGISTRY[mtype]

        # --- —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤ –ø—ñ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π __init__ ---
        sig = inspect.signature(impl_cls.__init__)
        impl_kwargs = {k: v for k, v in kwargs.items() if k in sig.parameters}
        self._impl = impl_cls(**impl_kwargs)

    def fit(self, X: np.ndarray, Y: np.ndarray, config_params: dict = None) -> None:
        """
        –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ –ø–µ—Ä–µ–¥–∞—á–µ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤.
        
        Args:
            X: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –æ–∑–Ω–∞–∫–∏
            Y: –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ —Ü—ñ–ª—ñ  
            config_params: –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –¥–ª—è –ø–µ—Ä–µ–¥–∞—á—ñ –¥–æ –≤–Ω—É—Ç—Ä—ñ—à–Ω—å–æ—ó –º–æ–¥–µ–ª—ñ
        """
        return self._impl.fit(X, Y, config_params)

    def predict(self, X: np.ndarray) -> np.ndarray:
        return self._impl.predict(X)

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        return self._impl.linearize(X0)

    def __getattr__(self, item):
        return getattr(self._impl, item)

import time
import json
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error

from tqdm import tqdm  # –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä


import re
import json
from pathlib import Path
from typing import Dict, Any, List, Optional

import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats import ttest_rel


def analyze_experiment_results(
    results_dir: str = "exp_results",
    kernel_focus: Optional[List[str]] = None,   # –Ω–∞–ø—Ä., ["rbf", "linear"]
    metrics_gain: List[str] = ("RMSE_Fe_gain_%", "RMSE_Mass_gain_%", "MSE_gain_%"),
    do_ttests: bool = True,
    save_figs: bool = True,
    show_figs: bool = False,
    export_tables: bool = True,
    style: str = "whitegrid",
    cmap: str = "RdYlGn",
) -> Dict[str, Any]:
    """
    –Ñ–¥–∏–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –∞–Ω–∞–ª—ñ–∑—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –±–∞—Ç—á-–µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤.

    –©–æ —Ä–æ–±–∏—Ç—å:
      - –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î summary.csv —ñ –≤—Å—ñ details_*.csv –∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó results_dir
      - –ë—É–¥—É—î —Ç–µ–ø–ª–æ–≤—ñ –∫–∞—Ä—Ç–∏ –≥–µ–π–Ω—ñ–≤ (–ø–æ lag√óN), –ø—Ä–æ—Ñ—ñ–ª—ñ –≥–µ–π–Ω—ñ–≤ –ø–æ lag –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ N
      - –ë—É–¥—É—î –≥—Ä–∞—Ñ—ñ–∫–∏ —á–∞—Å—É —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è
      - (–æ–ø—Ü—ñ–π–Ω–æ) –í–∏–∫–æ–Ω—É—î –ø–∞—Ä–Ω—ñ t-—Ç–µ—Å—Ç–∏ –ø–æ —Å—ñ–¥–∞—Ö –¥–ª—è RMSE (ARX vs KRR)
      - –ó–±–µ—Ä—ñ–≥–∞—î –≥—Ä–∞—Ñ—ñ–∫–∏ —Ç–∞ —Ç–∞–±–ª–∏—Ü—ñ —É –ø—ñ–¥–ø–∞–ø–∫—É 'analysis' —ñ –ø–æ–≤–µ—Ä—Ç–∞—î –≤—Å–µ —É —Å–ª–æ–≤–Ω–∏–∫—É

    –ü–æ–≤–µ—Ä—Ç–∞—î:
      {
        "df_summary": pd.DataFrame,
        "df_details": pd.DataFrame | None,
        "ttests": pd.DataFrame | None,
        "fig_paths": List[str],
        "table_paths": Dict[str, str],
        "meta": Dict[str, Any],
      }

    –ü—Ä–∏–º—ñ—Ç–∫–∏:
      - –û—á—ñ–∫—É—î—Ç—å—Å—è, —â–æ summary.csv –º–∞—î –∫–æ–ª–æ–Ω–∫–∏:
          ["kernel","N","lag","seeds", "RMSE_Fe_gain_%","RMSE_Mass_gain_%","MSE_gain_%","ARX_time_s","KRR_time_s", ...]
      - –û—á—ñ–∫—É—î—Ç—å—Å—è, —â–æ details_*.csv –º—ñ—Å—Ç—è—Ç—å –ø—Ä–∏–Ω–∞–π–º–Ω—ñ:
          ["seed","N","lag","kernel","MSE_ARX","RMSE_Fe_ARX","RMSE_Mass_ARX","MSE_KRR","RMSE_Fe_KRR","RMSE_Mass_KRR","Train_s_ARX","Train_s_KRR"]
        (N, lag, kernel –¥–æ–¥–∞—é—Ç—å—Å—è –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ –∑ —ñ–º–µ–Ω—ñ —Ñ–∞–π–ª—É)
    """
    results_path = Path(results_dir)
    if not results_path.exists():
        raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞: {results_dir}")

    out_dir = results_path / "analysis"
    out_dir.mkdir(parents=True, exist_ok=True)

    # Headless-—Å–µ–π—Ñ —Ä–µ–∂–∏–º –¥–ª—è —Ñ—ñ–≥—É—Ä
    if not show_figs:
        matplotlib.use("Agg")

    sns.set(style=style)

    # 1) –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è summary
    summary_fp = results_path / "summary.csv"
    if not summary_fp.exists():
        raise FileNotFoundError(f"–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ summary.csv —É {results_dir}")

    df_summary = pd.read_csv(summary_fp)
    # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è
    required_summary_cols = {"kernel", "N", "lag"}
    if not required_summary_cols.issubset(df_summary.columns):
        raise ValueError(f"summary.csv –Ω–µ –º—ñ—Å—Ç–∏—Ç—å –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫: {sorted(required_summary_cols - set(df_summary.columns))}")

    # 2) –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è details (–æ–ø—Ü—ñ–π–Ω–æ)
    details_files = sorted(results_path.glob("details_*.csv"))
    df_details = None
    if details_files:
        rows = []
        for fp in details_files:
            m = re.match(r"details_N(?P<N>\d+)_L(?P<L>\d+)_K(?P<K>\w+)\.csv", fp.name)
            if not m:
                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ —Ñ–∞–π–ª–∏, —â–æ –Ω–µ –ø—ñ–¥–ø–∞–¥–∞—é—Ç—å –ø—ñ–¥ –ø–∞—Ç–µ—Ä–Ω
                continue
            N = int(m.group("N")); L = int(m.group("L")); K = m.group("K")
            d = pd.read_csv(fp)
            d["N"] = N
            d["lag"] = L
            d["kernel"] = K
            rows.append(d)
        if rows:
            df_details = pd.concat(rows, ignore_index=True)

    # –Ø–¥—Ä–∞ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
    if kernel_focus is None:
        kernel_focus = sorted(df_summary["kernel"].unique().tolist())

    fig_paths: List[str] = []
    table_paths: Dict[str, str] = {}

    # 3) –ï–∫—Å–ø–æ—Ä—Ç –±–∞–∑–æ–≤–∏—Ö —Ç–∞–±–ª–∏—Ü—å
    if export_tables:
        tbl_rbf = (df_summary
                   .query("kernel=='rbf'") if "rbf" in df_summary["kernel"].unique() else df_summary.copy())
        tbl_rbf = tbl_rbf.loc[:, [c for c in ["kernel","N","lag","seeds","RMSE_Fe_gain_%","RMSE_Mass_gain_%","MSE_gain_%","KRR_time_s","ARX_time_s"] if c in df_summary.columns]]
        table_paths["table_rbf_gains.csv"] = str((out_dir / "table_rbf_gains.csv").resolve())
        tbl_rbf.to_csv(table_paths["table_rbf_gains.csv"], index=False)

        # –ó–≤–µ–¥–µ–Ω—ñ –ø—ñ–≤–æ–¥-—Ç–∞–±–ª–∏—Ü—ñ –ø–æ –≥–µ–π–Ω–∞—Ö (–¥–ª—è –∫–æ–∂–Ω–æ–≥–æ kernel –æ–∫—Ä–µ–º–æ)
        for k in kernel_focus:
            for metric in metrics_gain:
                if metric not in df_summary.columns:
                    continue
                pivot = (df_summary[df_summary["kernel"] == k]
                         .pivot_table(index="N", columns="lag", values=metric, aggfunc="mean"))
                fp = out_dir / f"pivot_{metric}_kernel-{k}.csv"
                pivot.to_csv(fp)
                table_paths[fp.name] = str(fp.resolve())

    # 4) –ì—Ä–∞—Ñ—ñ–∫–∏ ‚Äî —Ç–µ–ø–ª–æ–≤—ñ –∫–∞—Ä—Ç–∏ –≥–µ–π–Ω—ñ–≤
    def plot_heatmap_gain(df, metric: str, kernel: str, title: Optional[str] = None):
        d = df[(df["kernel"] == kernel)].pivot_table(index="N", columns="lag", values=metric, aggfunc="mean")
        if d.empty:
            return None
        plt.figure(figsize=(6, 4))
        ax = sns.heatmap(d, annot=True, fmt=".1f", cmap=cmap, center=0, cbar_kws={"label": metric})
        plt.title(title or f"{metric} ‚Äî kernel={kernel}")
        plt.ylabel("N"); plt.xlabel("lag")
        plt.tight_layout()
        out_fp = out_dir / f"heatmap_{metric}_kernel-{kernel}.png"
        plt.savefig(out_fp, dpi=200)
        if show_figs:
            plt.show()
        plt.close()
        return str(out_fp.resolve())

    for k in kernel_focus:
        for metric in metrics_gain:
            if metric in df_summary.columns:
                fp = plot_heatmap_gain(df_summary, metric=metric, kernel=k, title=None)
                if fp:
                    fig_paths.append(fp)

    # 5) –ü—Ä–æ—Ñ—ñ–ª—ñ –≥–µ–π–Ω—ñ–≤ –ø–æ lag –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ N
    def plot_line_profile(df, metric: str, kernel: str):
        vals = df[(df["kernel"] == kernel)]
        if vals.empty or metric not in vals.columns:
            return None
        Ns_sorted = sorted(vals["N"].unique().tolist())
        plt.figure(figsize=(7, 4))
        for N in Ns_sorted:
            d = vals[vals["N"] == N].sort_values("lag")
            plt.plot(d["lag"], d[metric], marker="o", label=f"N={N}")
        plt.axhline(0, color="k", lw=1)
        plt.title(f"{metric} vs lag ‚Äî {kernel}")
        plt.xlabel("lag"); plt.ylabel(metric)
        plt.legend()
        plt.tight_layout()
        out_fp = out_dir / f"line_{metric}_kernel-{kernel}.png"
        plt.savefig(out_fp, dpi=200)
        if show_figs:
            plt.show()
        plt.close()
        return str(out_fp.resolve())

    for k in kernel_focus:
        for metric in metrics_gain:
            if metric in df_summary.columns:
                fp = plot_line_profile(df_summary, metric=metric, kernel=k)
                if fp:
                    fig_paths.append(fp)

    # 6) –ß–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è (KRR_time_s –ø–æ lag –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ N —ñ —è–¥—Ä–∞)
    if "KRR_time_s" in df_summary.columns:
        for k in kernel_focus:
            d_k = df_summary[df_summary["kernel"] == k]
            if d_k.empty:
                continue
            plt.figure(figsize=(7, 4))
            for N in sorted(d_k["N"].unique().tolist()):
                d = d_k[d_k["N"] == N].sort_values("lag")
                if "KRR_time_s" in d.columns:
                    plt.plot(d["lag"], d["KRR_time_s"], marker="s", label=f"N={N}")
            plt.title(f"KRR —á–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è vs lag ‚Äî kernel={k}")
            plt.xlabel("lag"); plt.ylabel("—Å–µ–∫—É–Ω–¥–∏")
            plt.legend()
            plt.tight_layout()
            out_fp = out_dir / f"time_KRR_kernel-{k}.png"
            plt.savefig(out_fp, dpi=200)
            if show_figs:
                plt.show()
            plt.close()
            fig_paths.append(str(out_fp.resolve()))

    # 7) –ü–∞—Ä–Ω—ñ t-—Ç–µ—Å—Ç–∏ (–ø–æ —Å—ñ–¥–∞—Ö) ‚Äî —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω—ñ details
    df_sig = None
    if do_ttests and df_details is not None:
        required_detail_cols = {"RMSE_Fe_ARX", "RMSE_Fe_KRR", "RMSE_Mass_ARX", "RMSE_Mass_KRR", "N", "lag", "kernel"}
        if required_detail_cols.issubset(df_details.columns):
            rows = []
            for (N, L, K) in sorted(df_details.groupby(["N", "lag", "kernel"]).groups.keys()):
                g = df_details[(df_details["N"] == N) & (df_details["lag"] == L) & (df_details["kernel"] == K)]
                if len(g) < 2:
                    # –î–≤–æ—Ö —ñ –±—ñ–ª—å—à–µ —Å—ñ–¥—ñ–≤ –ø–æ—Ç—Ä—ñ–±–Ω–æ –¥–ª—è t-—Ç–µ—Å—Ç—É
                    continue
                # t-—Ç–µ—Å—Ç–∏ RMSE Fe —ñ Mass
                t_fe, p_fe = ttest_rel(g["RMSE_Fe_ARX"], g["RMSE_Fe_KRR"])
                t_ms, p_ms = ttest_rel(g["RMSE_Mass_ARX"], g["RMSE_Mass_KRR"])
                rows.append({
                    "N": N, "lag": L, "kernel": K, "n_seeds": len(g),
                    "p_value_fe": p_fe,
                    "p_value_mass": p_ms,
                    "mean_gain_fe_%": 100.0 * (g["RMSE_Fe_ARX"].mean() - g["RMSE_Fe_KRR"].mean()) / (g["RMSE_Fe_ARX"].mean() + 1e-12),
                    "mean_gain_mass_%": 100.0 * (g["RMSE_Mass_ARX"].mean() - g["RMSE_Mass_KRR"].mean()) / (g["RMSE_Mass_ARX"].mean() + 1e-12),
                })
            if rows:
                df_sig = pd.DataFrame(rows).sort_values(["kernel", "N", "lag"]).reset_index(drop=True)
                if export_tables:
                    fp = out_dir / "ttests_by_combo.csv"
                    df_sig.to_csv(fp, index=False)
                    table_paths[fp.name] = str(fp.resolve())

    # 8) –ó–±–µ—Ä–µ–≥—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω—ñ –∞–Ω–∞–ª—ñ–∑—É
    meta = {
        "results_dir": str(results_path.resolve()),
        "analysis_dir": str(out_dir.resolve()),
        "kernels_analyzed": kernel_focus,
        "metrics_gain": list(metrics_gain),
        "has_details": df_details is not None,
        "fig_paths": fig_paths,
        "table_paths": table_paths,
    }
    with open(out_dir / "analysis_meta.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)

    return {
        "df_summary": df_summary,
        "df_details": df_details,
        "ttests": df_sig,
        "fig_paths": fig_paths,
        "table_paths": table_paths,
        "meta": meta,
    }


def run_experiment_suite(
    reference_df: pd.DataFrame,
    lags=(1,2,3,4),
    Ns=(5000, 7000),
    krr_kernels=("linear", "rbf"),
    anomaly_severity="mild",
    use_anomalies=True,
    seeds=(123, 42),
    noise_level="none",
    results_dir="exp_results",
    n_iter_search_rbf=15,
    verbose=False,
):
    """
    –ë–∞—Ç—á-–µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏ –∑–∞ —Å—ñ—Ç–∫–æ—é (N, lag, kernel‚àà{linear, rbf}) + –º—É–ª—å—Ç–∏—Å—ñ–¥–∏ –∑ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä–æ–º (tqdm).
    –ó–±–µ—Ä—ñ–≥–∞—î –¥–µ—Ç–∞–ª—å–Ω—ñ CSV –¥–ª—è –∫–æ–∂–Ω–æ—ó –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó —ñ summary.csv –∑ –∞–≥—Ä–µ–≥–∞—Ç–∞–º–∏.
    """

    Path(results_dir).mkdir(parents=True, exist_ok=True)
    rows_all, detail_records = [], []

    # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∑–∞–≥–∞–ª—å–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —ñ—Ç–µ—Ä–∞—Ü—ñ–π (—Ü–∏–∫–ª = –æ–¥–Ω–∞ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—è –¥–ª—è –æ–¥–Ω–æ–≥–æ seed)
    total_cycles = len(Ns) * len(lags) * len(krr_kernels) * len(seeds)

    # –ó–æ–≤–Ω—ñ—à–Ω—ñ–π –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä –ø–æ –≤—Å—ñ–º —ñ—Ç–µ—Ä–∞—Ü—ñ—è–º
    pbar = tqdm(total=total_cycles, desc="Experiment grid", unit="cycle")

    for N_data in Ns:
        for lag in lags:
            for kernel in krr_kernels:

                metrics = []
                for sd in seeds:
                    t_cycle_start = time.time()

                    # 1) –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
                    simulation_params = {
                        "N_data": int(N_data),
                        "control_pts": max(12, int(0.1*N_data)),
                        "lag": int(lag),
                        "train_size": 0.8,
                        "val_size": 0.1,
                        "test_size": 0.1,
                        "time_step_s": 5,
                        "time_constants_s": {
                            "concentrate_fe_percent": 8.0,
                            "tailings_fe_percent": 10.0,
                            "concentrate_mass_flow": 5.0,
                            "tailings_mass_flow": 7.0
                        },
                        "dead_times_s": {
                            "concentrate_fe_percent": 20.0,
                            "tailings_fe_percent": 25.0,
                            "concentrate_mass_flow": 20.0,
                            "tailings_mass_flow": 25.0
                        },
                        "plant_model_type": "rf",
                        "seed": int(sd),
                        "n_neighbors": 5,
                        "noise_level": noise_level,
                        # –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å (—Ñ—ñ–∫—Å–æ–≤–∞–Ω–∞)
                        "enable_nonlinear": True,
                        "nonlinear_config": {
                            "concentrate_fe_percent": ("pow", 2.0),
                            "concentrate_mass_flow": ("pow", 1.6),
                        },
                        # –ê–Ω–æ–º–∞–ª—ñ—ó
                        "use_anomalies": bool(use_anomalies),
                        "anomaly_severity": anomaly_severity,
                        "anomaly_in_train": False,
                    }

                    # 2) –î–∞–Ω—ñ
                    _, df_sim = create_simulation_data(reference_df, simulation_params)

                    # 3) –õ–∞–≥–∏ + —Å–ø–ª—ñ—Ç
                    X, Y = _create_lagged_matrices_corrected(df_sim, lag)
                    n = X.shape[0]
                    n_train = int(simulation_params["train_size"] * n)
                    n_val = int(simulation_params["val_size"] * n)
                    Xtr, Ytr = X[:n_train], Y[:n_train]
                    Xva, Yva = X[n_train:n_train+n_val], Y[n_train:n_train+n_val]
                    Xte, Yte = X[n_train+n_val:], Y[n_train+n_val:]

                    # 4) –°–∫–µ–π–ª—ñ–Ω–≥
                    xs, ys = StandardScaler(), StandardScaler()
                    Xtr_s, Ytr_s = xs.fit_transform(Xtr), ys.fit_transform(Ytr)
                    Xva_s, Yva_s = xs.transform(Xva), ys.transform(Yva)
                    Xte_s = xs.transform(Xte)

                    # 5) ARX (–ª—ñ–Ω—ñ–π–Ω–∏–π –±–µ–Ω—á–º–∞—Ä–∫)
                    arx = KernelModel(model_type="linear", linear_type="ols", poly_degree=1, include_bias=True)
                    t0 = time.time()
                    try:
                        arx.fit(Xtr_s, Ytr_s, X_val=Xva_s, Y_val=Yva_s)
                    except TypeError:
                        arx.fit(Xtr_s, Ytr_s)
                    arx_t = time.time() - t0
                    Yhat_arx = ys.inverse_transform(arx.predict(Xte_s))
                    mse_arx = mean_squared_error(Yte, Yhat_arx)
                    rmse_fe_arx = np.sqrt(mean_squared_error(Yte[:,0], Yhat_arx[:,0]))
                    rmse_mass_arx = np.sqrt(mean_squared_error(Yte[:,1], Yhat_arx[:,1]))

                    # 6) KRR (linear –∞–±–æ rbf)
                    if kernel not in ("linear", "rbf"):
                        raise ValueError("–ü—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è —Ç—ñ–ª—å–∫–∏ 'linear' —Ç–∞ 'rbf' –¥–ª—è KRR")
                    krr_kwargs = {"model_type": "krr", "kernel": kernel}
                    if kernel == "rbf":
                        krr_kwargs.update({"find_optimal_params": True, "n_iter_random_search": int(n_iter_search_rbf)})
                    else:
                        krr_kwargs.update({"find_optimal_params": False})

                    krr = KernelModel(**krr_kwargs)
                    t0 = time.time()
                    try:
                        krr.fit(Xtr_s, Ytr_s, X_val=Xva_s, Y_val=Yva_s)
                    except TypeError:
                        krr.fit(Xtr_s, Ytr_s)
                    krr_t = time.time() - t0
                    Yhat_krr = ys.inverse_transform(krr.predict(Xte_s))
                    mse_krr = mean_squared_error(Yte, Yhat_krr)
                    rmse_fe_krr = np.sqrt(mean_squared_error(Yte[:,0], Yhat_krr[:,0]))
                    rmse_mass_krr = np.sqrt(mean_squared_error(Yte[:,1], Yhat_krr[:,1]))

                    metrics.append({
                        "seed": sd, "N": N_data, "lag": lag, "kernel": kernel,
                        "MSE_ARX": mse_arx, "RMSE_Fe_ARX": rmse_fe_arx, "RMSE_Mass_ARX": rmse_mass_arx, "Train_s_ARX": arx_t,
                        "MSE_KRR": mse_krr, "RMSE_Fe_KRR": rmse_fe_krr, "RMSE_Mass_KRR": rmse_mass_krr, "Train_s_KRR": krr_t,
                    })

                    # –û–Ω–æ–≤–ª—é—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä + ETA
                    t_cycle = time.time() - t_cycle_start
                    pbar.set_postfix({
                        "N": N_data, "L": lag, "K": kernel, "seed": sd,
                        "cycle_s": f"{t_cycle:.1f}",
                        "arx_s": f"{arx_t:.1f}",
                        "krr_s": f"{krr_t:.1f}",
                    })
                    pbar.update(1)

                # 7) –ê–≥—Ä–µ–≥–∞—Ü—ñ—è –ø–æ —Å—ñ–¥–∞—Ö –¥–ª—è –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó (N, lag, kernel)
                dfm = pd.DataFrame(metrics)
                agg = dfm.agg({
                    "MSE_ARX": ["mean","std"], "RMSE_Fe_ARX": ["mean","std"], "RMSE_Mass_ARX": ["mean","std"], "Train_s_ARX": ["mean","std"],
                    "MSE_KRR": ["mean","std"], "RMSE_Fe_KRR": ["mean","std"], "RMSE_Mass_KRR": ["mean","std"], "Train_s_KRR": ["mean","std"],
                }).T.reset_index()
                agg.columns = ["metric", "mean", "std"]

                summary = {
                    "N": N_data, "lag": lag, "kernel": kernel, "seeds": len(seeds),
                    "MSE_gain_%": (agg.loc[agg.metric=="MSE_ARX","mean"].values[0] - agg.loc[agg.metric=="MSE_KRR","mean"].values[0])
                                   / (agg.loc[agg.metric=="MSE_ARX","mean"].values[0] + 1e-12) * 100,
                    "RMSE_Fe_gain_%": (agg.loc[agg.metric=="RMSE_Fe_ARX","mean"].values[0] - agg.loc[agg.metric=="RMSE_Fe_KRR","mean"].values[0])
                                   / (agg.loc[agg.metric=="RMSE_Fe_ARX","mean"].values[0] + 1e-12) * 100,
                    "RMSE_Mass_gain_%": (agg.loc[agg.metric=="RMSE_Mass_ARX","mean"].values[0] - agg.loc[agg.metric=="RMSE_Mass_KRR","mean"].values[0])
                                   / (agg.loc[agg.metric=="RMSE_Mass_ARX","mean"].values[0] + 1e-12) * 100,
                    "ARX_time_s": agg.loc[agg.metric=="Train_s_ARX","mean"].values[0],
                    "KRR_time_s": agg.loc[agg.metric=="Train_s_KRR","mean"].values[0],
                }
                rows_all.append(summary)

                # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–µ—Ç–∞–ª—å–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫ –ø–æ —Å—ñ–¥–∞—Ö –¥–ª—è —Ü—ñ—î—ó –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó
                detail_path = Path(results_dir) / f"details_N{N_data}_L{lag}_K{kernel}.csv"
                dfm.to_csv(detail_path, index=False)
                detail_records.append({"combo": (N_data, lag, kernel), "path": str(detail_path)})

    pbar.close()

    df_summary = pd.DataFrame(rows_all).sort_values(["kernel","N","lag"]).reset_index(drop=True)
    summary_path = Path(results_dir) / "summary.csv"
    df_summary.to_csv(summary_path, index=False)

    meta = {
        "created_at": pd.Timestamp.now().isoformat(),
        "lags": list(lags), "Ns": list(Ns), "krr_kernels": list(krr_kernels),
        "anomaly_severity": anomaly_severity, "use_anomalies": use_anomalies,
        "seeds": list(seeds), "noise_level": noise_level,
        "detail_files": detail_records, "summary_csv": str(summary_path),
    }
    with open(Path(results_dir) / "meta.json", "w") as f:
        json.dump(meta, f, indent=2)

    if verbose:
        print(f"Summary saved to: {summary_path}")
        print(pd.DataFrame(rows_all))

    return df_summary
def compare_linear_vs_kernel_models(reference_df=None, **kwargs):
    """
    –ü—Ä—è–º–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–∏—Ö (ARX) —Ç–∞ —è–¥–µ—Ä–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó.
    –¢–µ–ø–µ—Ä: –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–æ–≤–∞–Ω–∏—Ö –∞–Ω–æ–º–∞–ª—ñ–π —É val/test —ñ —Å–ø—ñ–ª—å–Ω–∏–π anomaly_cfg.
    """
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import mean_squared_error
    import numpy as np
    from data_gen import StatefulDataGenerator

    print("üéì –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ú–û–î–ï–õ–ï–ô –î–õ–Ø –î–ò–°–ï–†–¢–ê–¶–Ü–á")
    print("=" * 60)
    print("–†–æ–∑–¥—ñ–ª 2.1.1: –õ–æ–≥—ñ—á–Ω–∏–π –ø–µ—Ä–µ—Ö—ñ–¥ –¥–æ —è–¥–µ—Ä–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π")
    print("=" * 60)

    # ----1. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ—Ñ–µ—Ä–µ–Ω—Ç–Ω–∏—Ö –¥–∞–Ω–∏—Ö
    if reference_df is None:
        print("üìä –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Ç–Ω–∏—Ö –¥–∞–Ω–∏—Ö...")
        try:
            reference_df = pd.read_parquet('processed.parquet')
            print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(reference_df)} –∑–∞–ø–∏—Å—ñ–≤")
        except FileNotFoundError:
            print("‚ùå –§–∞–π–ª 'processed.parquet' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

    # ---- –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Å–∏–º—É–ª—è—Ü—ñ—ó
    train_size = kwargs.get('train_size', 0.8)
    val_size   = kwargs.get('val_size', 0.1)
    test_size  = kwargs.get('test_size', 0.1)

    simulation_params = {
        'N_data': kwargs.get('N_data', 7000),
        'control_pts': 700,
        'lag': kwargs.get('lag', 2),
        'train_size': train_size,
        'val_size': val_size,
        'test_size': test_size,
        'time_step_s': 5,
        'time_constants_s': {
            'concentrate_fe_percent': 8.0,
            'tailings_fe_percent': 10.0,
            'concentrate_mass_flow': 5.0,
            'tailings_mass_flow': 7.0
        },
        'dead_times_s': {
            'concentrate_fe_percent': 20.0,
            'tailings_fe_percent': 25.0,
            'concentrate_mass_flow': 20.0,
            'tailings_mass_flow': 25.0
        },
        'plant_model_type': 'rf',
        'seed': kwargs.get('seed', 42),
        'n_neighbors': 5,
        'noise_level': kwargs.get('noise_level', 'none'),

        # –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å
        'enable_nonlinear': True,
        'nonlinear_config': {
            'concentrate_fe_percent': ('pow', 2.0),
            'concentrate_mass_flow': ('pow', 1.5)
        },

        # –ê–Ω–æ–º–∞–ª—ñ—ó
        'use_anomalies': kwargs.get('use_anomalies', True),
        'anomaly_severity': kwargs.get('anomaly_severity', 'mild'),
        'anomaly_in_train': kwargs.get('anomaly_in_train', False),
    }

    print(f"üìà –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–∏–º—É–ª—è—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö (N={simulation_params['N_data']}, L={simulation_params['lag']})...")
    true_gen, df_sim = create_simulation_data(reference_df, simulation_params)

    # ---- –õ–∞–≥–æ–≤–∞–Ω—ñ –º–∞—Ç—Ä–∏—Ü—ñ
    X, Y = _create_lagged_matrices_corrected(df_sim, simulation_params['lag'])
    print(f"   –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å X: {X.shape}, Y: {Y.shape}")

    # ----2. –°–ø–ª—ñ—Ç –Ω–∞ train/val/test
    n = X.shape[0]
    n_train = int(train_size * n)
    n_val   = int(val_size * n)

    X_train, Y_train = X[:n_train], Y[:n_train]
    X_val,   Y_val   = X[n_train:n_train + n_val], Y[n_train:n_train + n_val]
    X_test,  Y_test  = X[n_train + n_val:], Y[n_train + n_val:]

    # ---- –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (fit —Ç—ñ–ª—å–∫–∏ –Ω–∞ train)
    x_scaler = StandardScaler()
    y_scaler = StandardScaler()

    X_train_scaled = x_scaler.fit_transform(X_train)
    Y_train_scaled = y_scaler.fit_transform(Y_train)
    X_val_scaled   = x_scaler.transform(X_val)
    Y_val_scaled   = y_scaler.transform(Y_val)
    X_test_scaled  = x_scaler.transform(X_test)
    Y_test_scaled  = y_scaler.transform(Y_test)

    print(f"   –¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä: {X_train_scaled.shape[0]} –∑—Ä–∞–∑–∫—ñ–≤")
    print(f"   –í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π –Ω–∞–±—ñ—Ä: {X_val_scaled.shape[0]} –∑—Ä–∞–∑–∫—ñ–≤")
    print(f"   –¢–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±—ñ—Ä: {X_test_scaled.shape[0]} –∑—Ä–∞–∑–∫—ñ–≤")

    # ----3. –õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å (ARX)
    print("\nüî¥ –ù–ê–í–ß–ê–ù–ù–Ø –õ–Ü–ù–Ü–ô–ù–û–á –ú–û–î–ï–õ–Ü (ARX)")
    print("-" * 40)
    linear_model = KernelModel(model_type='linear', linear_type='ols', poly_degree=1, include_bias=True)

    import time
    start_time = time.time()
    try:
        # –Ø–∫—â–æ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—ñ–¥—Ç—Ä–∏–º—É—î val-–¥–∞–Ω—ñ –¥–ª—è —Ç—é–Ω—ñ–Ω–≥—É
        linear_model.fit(X_train_scaled, Y_train_scaled, X_val=X_val_scaled, Y_val=Y_val_scaled)
    except TypeError:
        linear_model.fit(X_train_scaled, Y_train_scaled)
    linear_train_time = time.time() - start_time

    Y_pred_linear_scaled = linear_model.predict(X_test_scaled)
    Y_pred_linear = y_scaler.inverse_transform(Y_pred_linear_scaled)

    linear_mse = mean_squared_error(Y_test, Y_pred_linear)
    linear_rmse_fe = np.sqrt(mean_squared_error(Y_test[:, 0], Y_pred_linear[:, 0]))
    linear_rmse_mass = np.sqrt(mean_squared_error(Y_test[:, 1], Y_pred_linear[:, 1]))

    print(f"   ‚è±Ô∏è –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {linear_train_time:.3f} —Å–µ–∫")
    print(f"   üìä MSE: {linear_mse:.6f}")
    print(f"   üìä RMSE Fe: {linear_rmse_fe:.3f}")
    print(f"   üìä RMSE Mass: {linear_rmse_mass:.3f}")

    # ----4. –Ø–¥–µ—Ä–Ω–∞ –º–æ–¥–µ–ª—å (KRR, RBF)
    print("\nüü¢ –ù–ê–í–ß–ê–ù–ù–Ø –Ø–î–ï–†–ù–û–á –ú–û–î–ï–õ–Ü (KRR)")
    print("-" * 40)
    kernel_model = KernelModel(
        model_type='krr',
        kernel='rbf',
        find_optimal_params=kwargs.get('find_optimal_params', True),
        n_iter_random_search=kwargs.get('n_iter_search', 20)
    )

    start_time = time.time()
    try:
        kernel_model.fit(X_train_scaled, Y_train_scaled, X_val=X_val_scaled, Y_val=Y_val_scaled)
    except TypeError:
        kernel_model.fit(X_train_scaled, Y_train_scaled)
    kernel_train_time = time.time() - start_time

    Y_pred_kernel_scaled = kernel_model.predict(X_test_scaled)
    Y_pred_kernel = y_scaler.inverse_transform(Y_pred_kernel_scaled)

    kernel_mse = mean_squared_error(Y_test, Y_pred_kernel)
    kernel_rmse_fe = np.sqrt(mean_squared_error(Y_test[:, 0], Y_pred_kernel[:, 0]))
    kernel_rmse_mass = np.sqrt(mean_squared_error(Y_test[:, 1], Y_pred_kernel[:, 1]))

    print(f"   ‚è±Ô∏è –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {kernel_train_time:.3f} —Å–µ–∫")
    print(f"   üìä MSE: {kernel_mse:.6f}")
    print(f"   üìä RMSE Fe: {kernel_rmse_fe:.3f}")
    print(f"   üìä RMSE Mass: {kernel_rmse_mass:.3f}")

    # ----5. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —ñ –∞–Ω–∞–ª—ñ–∑ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
    improvement_mse = ((linear_mse - kernel_mse) / (linear_mse + 1e-12)) * 100
    improvement_fe = ((linear_rmse_fe - kernel_rmse_fe) / (linear_rmse_fe + 1e-12)) * 100
    improvement_mass = ((linear_rmse_mass - kernel_rmse_mass) / (linear_rmse_mass + 1e-12)) * 100

    print("\nüìä –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –¢–ê –ê–ù–ê–õ–Ü–ó –ù–ï–õ–Ü–ù–Ü–ô–ù–û–°–¢–Ü")
    print("-" * 50)
    print("üéØ –ö–õ–Æ–ß–û–í–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò –î–õ–Ø –î–ò–°–ï–†–¢–ê–¶–Ü–á:")
    print(f"   üí° –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è MSE: {improvement_mse:.1f}%")
    print(f"   üí° –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è RMSE Fe: {improvement_fe:.1f}%")
    print(f"   üí° –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è RMSE Mass: {improvement_mass:.1f}%")

    target_achieved = improvement_mse >= 15
    print(f"   {'‚úÖ' if target_achieved else '‚ùå'} –¶—ñ–ª—å–æ–≤–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω "
          f"{'–î–û–°–Ø–ì–ù–£–¢–û' if target_achieved else '–ù–ï –¥–æ—Å—è–≥–Ω—É—Ç–æ'}")

    nonlinearity_metrics = _analyze_simulation_nonlinearity(df_sim, true_gen)

    print("\nüîç –ê–ù–ê–õ–Ü–ó –ù–ï–õ–Ü–ù–Ü–ô–ù–û–°–¢–Ü –ü–†–û–¶–ï–°–£")
    print("-" * 40)
    for metric_name, value in nonlinearity_metrics.items():
        print(f"   üìà {metric_name}: {value:.3f}")

    # ----6. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    print("\nüìä –ì–ï–ù–ï–†–ê–¶–Ü–Ø –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–ô...")
    figures = _create_comparison_visualizations(
        Y_test, Y_pred_linear, Y_pred_kernel,
        linear_mse, kernel_mse, improvement_mse,
        nonlinearity_metrics, df_sim
    )

    # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
    import json
    results = {
        'timestamp': pd.Timestamp.now().isoformat(),
        'simulation_params': simulation_params,
        'data_info': {
            'samples_total': len(df_sim),
            'samples_train': X_train_scaled.shape[0],
            'samples_val': X_val_scaled.shape[0],
            'samples_test': X_test_scaled.shape[0],
            'lag_used': simulation_params['lag'],
            'features': X_train_scaled.shape[1]
        },
        'linear_model': {
            'type': 'Linear (ARX)',
            'mse': linear_mse,
            'rmse_fe': linear_rmse_fe,
            'rmse_mass': linear_rmse_mass,
            'train_time': linear_train_time
        },
        'kernel_model': {
            'type': 'Kernel Ridge Regression (RBF)',
            'mse': kernel_mse,
            'rmse_fe': kernel_rmse_fe,
            'rmse_mass': kernel_rmse_mass,
            'train_time': kernel_train_time
        },
        'performance_comparison': {
            'mse_improvement_percent': improvement_mse,
            'rmse_fe_improvement_percent': improvement_fe,
            'rmse_mass_improvement_percent': improvement_mass,
            'target_achieved': target_achieved,
            'target_range': (15, 20)
        },
        'nonlinearity_analysis': nonlinearity_metrics
    }

    fname = f'dissertation_comparison_{pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(fname, 'w') as f:
        json.dump(results, f, indent=2, default=str)

    print("\n‚úÖ –ê–ù–ê–õ–Ü–ó –ó–ê–í–ï–†–®–ï–ù–û")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É {fname}")
    print("üñºÔ∏è –ì—Ä–∞—Ñ—ñ–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É PNG —Ñ–∞–π–ª–∏")

    return results

def make_anomaly_config_for_comparison(
    N_data: int,
    train_frac: float = 0.7,
    val_frac: float = 0.15,
    test_frac: float = 0.15,
    seed: int = 42,
    severity: str = "mild",
    include_train: bool = False
) -> dict:
    """
    –ì–µ–Ω–µ—Ä—É—î reproducible anomaly_config –¥–ª—è DataGenerator.generate_anomalies().
    –ó–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: –∞–Ω–æ–º–∞–ª—ñ—ó —Ç—ñ–ª—å–∫–∏ —É val/test.
    severity: 'mild' | 'medium' | 'strong'
    """
    total = train_frac + val_frac + test_frac
    train_frac, val_frac, test_frac = train_frac/total, val_frac/total, test_frac/total

    train_end = int(train_frac * N_data)
    val_end   = train_end + int(val_frac * N_data)
    segments = {"train": (0, train_end), "val": (train_end, val_end), "test": (val_end, N_data)}

    base_durations = {"spike": 1, "drift": 25, "drop": 20, "freeze": 15}
    sev_map = {"mild": (0.08, 0.15), "medium": (0.12, 0.22), "strong": (0.18, 0.30)}
    mag_lo, mag_hi = sev_map.get(severity, sev_map["mild"])

    rng = np.random.default_rng(seed)

    def seg_bounds(name):
        s, e = segments[name]
        return s, max(s, e-1), max(0, e-s)

    def pick_start(seg_name, dur):
        s, e_minus1, length = seg_bounds(seg_name)
        if length <= 0:
            return None
        dur = min(dur, length)
        hi = max(s, e_minus1 - (dur-1))
        return int(rng.integers(low=s, high=hi+1)), dur

    params = [
        "ore_mass_flow", "feed_fe_percent", "solid_feed_percent",
        "concentrate_fe_percent", "tailings_fe_percent",
        "concentrate_mass_flow", "tailings_mass_flow"
    ]
    cfg = {p: [] for p in params}

    def add_anom(param, seg, typ, mag=None, force_positive=False):
        dur = base_durations[typ]
        sd = pick_start(seg, dur)
        if sd is None:
            return
        start, dur = sd
        if typ == "freeze":
            cfg[param].append({"start": start, "duration": dur, "type": typ})
        else:
            m = float(abs(mag) if mag is not None else rng.uniform(mag_lo, mag_hi))
            if typ != "drop" and not force_positive and rng.random() < 0.5:
                m = -m
            if typ == "drop":
                m = abs(m)
            cfg[param].append({"start": start, "duration": dur, "magnitude": m, "type": typ})

    apply_train_here = include_train

    # –ü–ª–∞–Ω –∞–Ω–æ–º–∞–ª—ñ–π (val/test; train –æ–ø—Ü.)
    if apply_train_here: add_anom("ore_mass_flow", "train", "drift")
    add_anom("ore_mass_flow", "val",  "drift")
    add_anom("ore_mass_flow", "test", "spike")

    if apply_train_here: add_anom("solid_feed_percent", "train", "freeze")
    add_anom("solid_feed_percent", "val", "freeze")

    add_anom("feed_fe_percent", "test", "spike")
    add_anom("concentrate_mass_flow", "val", "drop", force_positive=True)
    add_anom("tailings_mass_flow", "test", "drift")
    add_anom("concentrate_fe_percent", "val", "spike")
    add_anom("tailings_fe_percent", "test", "freeze")

    return cfg

def create_simulation_data(reference_df: pd.DataFrame, params: dict):
    """
    –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–∏–º—É–ª—è—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö —á–µ—Ä–µ–∑ StatefulDataGenerator.
    –ê–¥–∞–ø—Ç–æ–≤–∞–Ω–æ: –æ–¥–∏–Ω —ñ —Ç–æ–π —Å–∞–º–∏–π anomaly_cfg –¥–ª—è –±–∞–∑–æ–≤–∏—Ö —ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö.
    """
    from data_gen import StatefulDataGenerator

    true_gen = StatefulDataGenerator(
        reference_df,
        ore_flow_var_pct=3.0,
        time_step_s=params['time_step_s'],
        time_constants_s=params['time_constants_s'],
        dead_times_s=params['dead_times_s'],
        true_model_type=params['plant_model_type'],
        seed=params['seed']
    )

    # 1) –ê–Ω–æ–º–∞–ª—ñ—ó (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º —É–≤—ñ–º–∫–Ω–µ–Ω–æ; —É train ‚Äî –≤–∏–º–∫–Ω–µ–Ω–æ)
    anomaly_cfg = None
    if params.get('use_anomalies', True):
        anomaly_cfg = make_anomaly_config_for_comparison(
            N_data=params['N_data'],
            train_frac=params.get('train_size', 0.8),
            val_frac=params.get('val_size', 0.1),
            test_frac=params.get('test_size', 0.1),
            seed=params['seed'],
            severity=params.get('anomaly_severity', 'mild'),
            include_train=params.get('anomaly_in_train', False),
        )

    # 2) –ë–∞–∑–æ–≤—ñ –¥–∞–Ω—ñ
    df_true_orig = true_gen.generate(
        T=params['N_data'],
        control_pts=params['control_pts'],
        n_neighbors=params['n_neighbors'],
        noise_level=params.get('noise_level', 'none'),
        anomaly_config=anomaly_cfg
    )

    # 3) –ù–µ–ª—ñ–Ω—ñ–π–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç (—Ç—ñ —Å–∞–º—ñ –∞–Ω–æ–º–∞–ª—ñ—ó)
    if params.get('enable_nonlinear', False):
        df_true = true_gen.generate_nonlinear_variant(
            base_df=df_true_orig,
            non_linear_factors=params['nonlinear_config'],
            noise_level='none',            # –©–æ–± –Ω–µ ¬´–Ω–∞–∫–ª–∞–¥–∞—Ç–∏¬ª –¥–æ–¥–∞—Ç–∫–æ–≤–∏–π —à—É–º
            anomaly_config=anomaly_cfg
        )
    else:
        df_true = df_true_orig

    return true_gen, df_true
def _create_lagged_matrices_corrected(df, lag=2):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ª–∞–≥–æ–≤–∏—Ö –º–∞—Ç—Ä–∏—Ü—å –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π"""
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –Ω–∞–∑–≤–∏ –∫–æ–ª–æ–Ω–æ–∫ –∑ StatefulDataGenerator
    input_vars = ['feed_fe_percent', 'ore_mass_flow', 'solid_feed_percent']
    output_vars = ['concentrate_fe', 'concentrate_mass']  # –°–∫–æ—Ä–æ—á–µ–Ω—ñ –Ω–∞–∑–≤–∏ –∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∏—Ö –Ω–∞–∑–≤
    if 'concentrate_fe' not in df.columns and 'concentrate_fe_percent' in df.columns:
        output_vars = ['concentrate_fe_percent', 'concentrate_mass_flow']
    
    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –∫–æ–ª–æ–Ω–æ–∫
    missing_vars = [var for var in input_vars + output_vars if var not in df.columns]
    if missing_vars:
        print(f"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏: {missing_vars}")
        print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ StatefulDataGenerator –º–µ—Ç–æ–¥
        return StatefulDataGenerator.create_lagged_dataset(df, lags=lag)
    
    n = len(df)
    X, Y = [], []
    
    for i in range(lag, n):
        # –õ–∞–≥–æ–≤–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –¥–∏–Ω–∞–º—ñ—á–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
        row = []
        for var in input_vars:
            for j in range(lag + 1):  # –≤—ñ–¥ t –¥–æ t-L
                row.append(df[var].iloc[i - j])
        X.append(row)
        
        # –í–∏—Ö—ñ–¥–Ω—ñ –∑–º—ñ–Ω–Ω—ñ –≤ –º–æ–º–µ–Ω—Ç t
        Y.append([df[var].iloc[i] for var in output_vars])
    
    return np.array(X), np.array(Y)


def _analyze_simulation_nonlinearity(df_sim, true_gen):
    """–ê–Ω–∞–ª—ñ–∑ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ —Å–∏–º—É–ª—è—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
    
    metrics = {}
    
    # 1. –û—Ü—ñ–Ω–∫–∞ S-–ø–æ–¥—ñ–±–Ω–æ—Å—Ç—ñ —á–µ—Ä–µ–∑ –≤–∞—Ä—ñ–∞—Ü—ñ—ó –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
    if 'concentrate_fe' in df_sim.columns:
        fe_values = df_sim['concentrate_fe'].values
        fe_gradients = np.diff(fe_values)
        metrics['fe_gradient_variance'] = np.var(fe_gradients)
        metrics['fe_gradient_skewness'] = pd.Series(fe_gradients).skew()
    
    # 2. –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ –≤–∑–∞—î–º–æ–¥—ñ—ó —á–µ—Ä–µ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
    numeric_cols = df_sim.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) >= 3:
        pearson_corr = df_sim[numeric_cols].corr(method='pearson')
        spearman_corr = df_sim[numeric_cols].corr(method='spearman')
        nonlinearity_indicator = abs(spearman_corr - pearson_corr).mean().mean()
        metrics['correlation_nonlinearity'] = nonlinearity_indicator
    
    # 3. –ï–Ω—Ç—Ä–æ–ø—ñ–π–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ
    if 'solid_feed_percent' in df_sim.columns:
        control_changes = np.abs(np.diff(df_sim['solid_feed_percent']))
        control_entropy = -np.sum((control_changes + 1e-10) * np.log(control_changes + 1e-10))
        metrics['control_complexity'] = control_entropy
    
    # 4. –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª—É
    if 'concentrate_mass' in df_sim.columns:
        mass_values = df_sim['concentrate_mass'].values
        metrics['mass_distribution_kurtosis'] = pd.Series(mass_values).kurtosis()
        metrics['mass_distribution_skewness'] = pd.Series(mass_values).skew()
    
    return metrics


# –ó–∞–ª–∏—à–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –º–µ—Ç–æ–¥–∏ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó —Ç–∞ –≤–∏—Å–Ω–æ–≤–∫—ñ–≤ –±–µ–∑ –∑–º—ñ–Ω
def _create_comparison_visualizations(Y_test, Y_pred_linear, Y_pred_kernel, 
                                    linear_mse, kernel_mse, improvement, 
                                    nonlinearity_metrics, full_df):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–∏—Ö –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π"""
    import matplotlib.pyplot as plt
    import numpy as np
    
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–∏–ª—é
    plt.style.use('default')
    plt.rcParams['figure.figsize'] = (16, 12)
    plt.rcParams['font.size'] = 10
    
    figures = {}
    
    # === –û–°–ù–û–í–ù–ê –§–Ü–ì–£–†–ê: –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ú–û–î–ï–õ–ï–ô ===
    fig1, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig1.suptitle('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ç–∞ —è–¥–µ—Ä–Ω–æ—ó –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó', fontsize=16, fontweight='bold')
    
    # 1.1 Scatter plot –¥–ª—è Fe –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—ó
    ax = axes[0, 0]
    ax.scatter(Y_test[:, 0], Y_pred_linear[:, 0], alpha=0.6, s=20, color='red', label='–õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å')
    ax.scatter(Y_test[:, 0], Y_pred_kernel[:, 0], alpha=0.6, s=20, color='green', label='–Ø–¥–µ—Ä–Ω–∞ –º–æ–¥–µ–ª—å')
    
    # –Ü–¥–µ–∞–ª—å–Ω–∞ –ª—ñ–Ω—ñ—è
    min_val, max_val = Y_test[:, 0].min(), Y_test[:, 0].max()
    ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8, linewidth=2, label='–Ü–¥–µ–∞–ª—å–Ω–∞ –ª—ñ–Ω—ñ—è')
    
    ax.set_xlabel('–†–µ–∞–ª—å–Ω–∞ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—è Fe (%)')
    ax.set_ylabel('–ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–∞ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—è Fe (%)')
    ax.set_title('–ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—ó Fe')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # –î–æ–¥–∞–≤–∞–Ω–Ω—è R¬≤ –Ω–∞ –≥—Ä–∞—Ñ—ñ–∫
    r2_linear = 1 - np.sum((Y_test[:, 0] - Y_pred_linear[:, 0])**2) / np.sum((Y_test[:, 0] - np.mean(Y_test[:, 0]))**2)
    r2_kernel = 1 - np.sum((Y_test[:, 0] - Y_pred_kernel[:, 0])**2) / np.sum((Y_test[:, 0] - np.mean(Y_test[:, 0]))**2)
    ax.text(0.05, 0.95, f'R¬≤ –ª—ñ–Ω—ñ–π–Ω–∞: {r2_linear:.3f}\nR¬≤ —è–¥–µ—Ä–Ω–∞: {r2_kernel:.3f}', 
            transform=ax.transAxes, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    # 1.2 Scatter plot –¥–ª—è –º–∞—Å–æ–≤–æ–≥–æ –ø–æ—Ç–æ–∫—É
    ax = axes[0, 1]
    ax.scatter(Y_test[:, 1], Y_pred_linear[:, 1], alpha=0.6, s=20, color='red', label='–õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å')
    ax.scatter(Y_test[:, 1], Y_pred_kernel[:, 1], alpha=0.6, s=20, color='green', label='–Ø–¥–µ—Ä–Ω–∞ –º–æ–¥–µ–ª—å')
    
    min_val, max_val = Y_test[:, 1].min(), Y_test[:, 1].max()
    ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8, linewidth=2, label='–Ü–¥–µ–∞–ª—å–Ω–∞ –ª—ñ–Ω—ñ—è')
    
    ax.set_xlabel('–†–µ–∞–ª—å–Ω–∏–π –º–∞—Å–æ–≤–∏–π –ø–æ—Ç—ñ–∫ (—Ç/–≥–æ–¥)')
    ax.set_ylabel('–ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–∏–π –º–∞—Å–æ–≤–∏–π –ø–æ—Ç—ñ–∫ (—Ç/–≥–æ–¥)')
    ax.set_title('–ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –º–∞—Å–æ–≤–æ–≥–æ –ø–æ—Ç–æ–∫—É')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 1.3 –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è MSE
    ax = axes[0, 2]
    models = ['–õ—ñ–Ω—ñ–π–Ω–∞\n(ARX)', '–Ø–¥–µ—Ä–Ω–∞\n(KRR)']
    mse_values = [linear_mse, kernel_mse]
    colors = ['red', 'green']
    
    bars = ax.bar(models, mse_values, color=colors, alpha=0.7, width=0.6)
    ax.set_ylabel('MSE')
    ax.set_title(f'–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è MSE\n(–ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è: {improvement:.1f}%)')
    ax.grid(True, alpha=0.3, axis='y')
    
    # –î–æ–¥–∞–≤–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –Ω–∞ —Å—Ç–æ–≤–ø—Ü—ñ
    for bar, value in zip(bars, mse_values):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(mse_values)*0.02,
                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
    
    # –î–æ–¥–∞–≤–∞–Ω–Ω—è —Å—Ç—Ä—ñ–ª–∫–∏ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
    if improvement > 0:
        ax.annotate('', xy=(1, kernel_mse), xytext=(0, linear_mse),
                   arrowprops=dict(arrowstyle='<->', color='blue', lw=2))
        ax.text(0.5, (linear_mse + kernel_mse)/2, f'-{improvement:.1f}%', 
               ha='center', va='center', color='blue', fontweight='bold',
               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))
    
    # 1.4 –ß–∞—Å–æ–≤–∏–π —Ä—è–¥ –ø–æ–º–∏–ª–æ–∫ –¥–ª—è Fe
    ax = axes[1, 0]
    time_steps = range(len(Y_test))
    error_linear_fe = Y_test[:, 0] - Y_pred_linear[:, 0]
    error_kernel_fe = Y_test[:, 0] - Y_pred_kernel[:, 0]
    
    ax.plot(time_steps, error_linear_fe, color='red', alpha=0.7, linewidth=1, label='–ü–æ–º–∏–ª–∫–∞ –ª—ñ–Ω—ñ–π–Ω–æ—ó')
    ax.plot(time_steps, error_kernel_fe, color='green', alpha=0.7, linewidth=1, label='–ü–æ–º–∏–ª–∫–∞ —è–¥–µ—Ä–Ω–æ—ó')
    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    
    ax.set_xlabel('–ö—Ä–æ–∫ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è')
    ax.set_ylabel('–ü–æ–º–∏–ª–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑—É Fe (%)')
    ax.set_title('–î–∏–Ω–∞–º—ñ–∫–∞ –ø–æ–º–∏–ª–æ–∫ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è Fe')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 1.5 –†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–º–∏–ª–æ–∫
    ax = axes[1, 1]
    ax.hist(error_linear_fe, bins=30, alpha=0.6, color='red', label='–õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å', density=True)
    ax.hist(error_kernel_fe, bins=30, alpha=0.6, color='green', label='–Ø–¥–µ—Ä–Ω–∞ –º–æ–¥–µ–ª—å', density=True)
    
    ax.set_xlabel('–ü–æ–º–∏–ª–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑—É Fe (%)')
    ax.set_ylabel('–©—ñ–ª—å–Ω—ñ—Å—Ç—å —Ä–æ–∑–ø–æ–¥—ñ–ª—É')
    ax.set_title('–†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–º–∏–ª–æ–∫ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–º–∏–ª–æ–∫
    ax.text(0.02, 0.98, 
           f'–õ—ñ–Ω—ñ–π–Ω–∞:\n–°–¢–î: {np.std(error_linear_fe):.3f}\n–°–µ—Ä.: {np.mean(error_linear_fe):.3f}\n\n'
           f'–Ø–¥–µ—Ä–Ω–∞:\n–°–¢–î: {np.std(error_kernel_fe):.3f}\n–°–µ—Ä.: {np.mean(error_kernel_fe):.3f}',
           transform=ax.transAxes, verticalalignment='top',
           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    # 1.6 –ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
    ax = axes[1, 2]
    if nonlinearity_metrics:
        metric_names = list(nonlinearity_metrics.keys())
        metric_values = list(nonlinearity_metrics.values())
        
        # –°–∫–æ—Ä–æ—á–µ–Ω–Ω—è –Ω–∞–∑–≤ –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è
        short_names = []
        for name in metric_names:
            if 'gradient' in name:
                short_names.append('–ì—Ä–∞–¥—ñ—î–Ω—Ç\n–≤–∞—Ä—ñ–∞—Ü—ñ—ó')
            elif 'correlation' in name:
                short_names.append('–ö–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞\n–Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å')
            elif 'complexity' in name:
                short_names.append('–°–∫–ª–∞–¥–Ω—ñ—Å—Ç—å\n–∫–µ—Ä—É–≤–∞–Ω–Ω—è')
            elif 'kurtosis' in name:
                short_names.append('–ö—É—Ä—Ç–æ–∑–∏—Å\n—Ä–æ–∑–ø–æ–¥—ñ–ª—É')
            elif 'skewness' in name:
                short_names.append('–ê—Å–∏–º–µ—Ç—Ä—ñ—è\n—Ä–æ–∑–ø–æ–¥—ñ–ª—É')
            else:
                short_names.append(name[:10] + '...' if len(name) > 10 else name)
        
        bars = ax.bar(range(len(metric_values)), metric_values, color='orange', alpha=0.7)
        ax.set_xticks(range(len(short_names)))
        ax.set_xticklabels(short_names, rotation=45, ha='right', fontsize=8)
        ax.set_ylabel('–ó–Ω–∞—á–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫–∏')
        ax.set_title('–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ –ø—Ä–æ—Ü–µ—Å—É')
        ax.grid(True, alpha=0.3, axis='y')
        
        # –î–æ–¥–∞–≤–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –Ω–∞ —Å—Ç–æ–≤–ø—Ü—ñ
        for i, (bar, value) in enumerate(zip(bars, metric_values)):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(metric_values)*0.02,
                   f'{value:.2f}', ha='center', va='bottom', fontsize=8)
    else:
        ax.text(0.5, 0.5, '–ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ\n–Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ñ', 
               ha='center', va='center', transform=ax.transAxes)
        ax.set_title('–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ –ø—Ä–æ—Ü–µ—Å—É')
    
    plt.tight_layout()
    plt.savefig('dissertation_model_comparison.png', dpi=300, bbox_inches='tight')
    figures['main_comparison'] = fig1
    
    # === –î–û–î–ê–¢–ö–û–í–ê –§–Ü–ì–£–†–ê: –î–ï–¢–ê–õ–¨–ù–ò–ô –ê–ù–ê–õ–Ü–ó ===
    fig2, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig2.suptitle('–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª–µ–π', fontsize=14, fontweight='bold')
    
    # 2.1 Box plot –ø–æ–º–∏–ª–æ–∫
    ax = axes[0, 0]
    error_data = [error_linear_fe, error_kernel_fe]
    bp = ax.boxplot(error_data, labels=['–õ—ñ–Ω—ñ–π–Ω–∞', '–Ø–¥–µ—Ä–Ω–∞'], patch_artist=True)
    bp['boxes'][0].set_facecolor('red')
    bp['boxes'][1].set_facecolor('green')
    bp['boxes'][0].set_alpha(0.6)
    bp['boxes'][1].set_alpha(0.6)
    
    ax.set_ylabel('–ü–æ–º–∏–ª–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑—É Fe (%)')
    ax.set_title('–†–æ–∑–ø–æ–¥—ñ–ª –ø–æ–º–∏–ª–æ–∫ (–∫–≤–∞—Ä—Ç–∏–ª—ñ)')
    ax.grid(True, alpha=0.3)
    
    # 2.2 –ö—É–º—É–ª—è—Ç–∏–≤–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –ø–æ–º–∏–ª–æ–∫
    ax = axes[0, 1]
    sorted_linear = np.sort(np.abs(error_linear_fe))
    sorted_kernel = np.sort(np.abs(error_kernel_fe))
    
    y_linear = np.arange(1, len(sorted_linear) + 1) / len(sorted_linear)
    y_kernel = np.arange(1, len(sorted_kernel) + 1) / len(sorted_kernel)
    
    ax.plot(sorted_linear, y_linear, color='red', linewidth=2, label='–õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å')
    ax.plot(sorted_kernel, y_kernel, color='green', linewidth=2, label='–Ø–¥–µ—Ä–Ω–∞ –º–æ–¥–µ–ª—å')
    
    ax.set_xlabel('–ê–±—Å–æ–ª—é—Ç–Ω–∞ –ø–æ–º–∏–ª–∫–∞ Fe (%)')
    ax.set_ylabel('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞ –π–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å')
    ax.set_title('–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª –ø–æ–º–∏–ª–æ–∫')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2.3 –ö–æ—Ä–µ–ª—è—Ü—ñ—è –∑–∞–ª–∏—à–∫—ñ–≤
    ax = axes[1, 0]
    ax.scatter(error_linear_fe, error_kernel_fe, alpha=0.6, s=20, color='purple')
    
    # –õ—ñ–Ω—ñ—è –∫–æ—Ä–µ–ª—è—Ü—ñ—ó
    correlation = np.corrcoef(error_linear_fe, error_kernel_fe)[0, 1]
    ax.plot([error_linear_fe.min(), error_linear_fe.max()], 
           [error_kernel_fe.min(), error_kernel_fe.max()], 'r--', alpha=0.8)
    
    ax.set_xlabel('–ü–æ–º–∏–ª–∫–∞ –ª—ñ–Ω—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ (%)')
    ax.set_ylabel('–ü–æ–º–∏–ª–∫–∞ —è–¥–µ—Ä–Ω–æ—ó –º–æ–¥–µ–ª—ñ (%)')
    ax.set_title(f'–ö–æ—Ä–µ–ª—è—Ü—ñ—è –ø–æ–º–∏–ª–æ–∫ (r = {correlation:.3f})')
    ax.grid(True, alpha=0.3)
    
    # 2.4 –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –∑–∞ –∫–≤–∞—Ä—Ç–∏–ª—è–º–∏
    ax = axes[1, 1]
    quartiles = [25, 50, 75, 90, 95]
    linear_percentiles = np.percentile(np.abs(error_linear_fe), quartiles)
    kernel_percentiles = np.percentile(np.abs(error_kernel_fe), quartiles)
    improvements = ((linear_percentiles - kernel_percentiles) / linear_percentiles) * 100
    
    bars = ax.bar(range(len(quartiles)), improvements, color='blue', alpha=0.7)
    ax.set_xticks(range(len(quartiles)))
    ax.set_xticklabels([f'{q}%' for q in quartiles])
    ax.set_ylabel('–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è (%)')
    ax.set_xlabel('–ö–≤–∞—Ä—Ç–∏–ª—å –ø–æ–º–∏–ª–æ–∫')
    ax.set_title('–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –∑–∞ –∫–≤–∞—Ä—Ç–∏–ª—è–º–∏ –ø–æ–º–∏–ª–æ–∫')
    ax.grid(True, alpha=0.3, axis='y')
    
    # –î–æ–¥–∞–≤–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –Ω–∞ —Å—Ç–æ–≤–ø—Ü—ñ
    for bar, value in zip(bars, improvements):
        color = 'green' if value > 0 else 'red'
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(improvements)*0.02,
               f'{value:.1f}%', ha='center', va='bottom', color=color, fontweight='bold')
    
    plt.tight_layout()
    # plt.savefig('dissertation_detailed_analysis.png', dpi=300, bbox_inches='tight')
    figures['detailed_analysis'] = fig2
    plt.show()
    
    print("üìä –°—Ç–≤–æ—Ä–µ–Ω–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ñ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó:")
    print("   üìà dissertation_model_comparison.png - –æ—Å–Ω–æ–≤–Ω–µ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è")
    print("   üìä dissertation_detailed_analysis.png - –¥–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑")
    
    return figures

if __name__ == '__main__':

    reference_df = pd.read_parquet("processed.parquet")
    
    # df_summary = run_experiment_suite(
    #     reference_df= reference_df,
    #     lags=(1,2,3,4),
    #     Ns=(5000, 7000),
    #     krr_kernels=("linear", "rbf"),
    #     anomaly_severity="mild",
    #     use_anomalies=True,
    #     seeds=(123, 42),
    #     noise_level="none",
    #     results_dir="exp_results",
    #     n_iter_search_rbf=15,
    #     verbose=False,
    # )

    # df_summary = run_experiment_suite(
    #     reference_df= reference_df,
    #     lags=(2,3),
    #     Ns=(5000, 7000),
    #     krr_kernels=("linear", "rbf"),
    #     anomaly_severity="mild",
    #     use_anomalies=True,
    #     seeds=(123, 42),
    #     noise_level="none",
    #     results_dir="exp_results",
    #     n_iter_search_rbf=15,
    #     verbose=False,
    # )

    # print(df_summary)

    # res = analyze_experiment_results(
    #     results_dir="exp_results",
    #     kernel_focus=["rbf","linear"],  # –º–æ–∂–Ω–∞ –∑–∞–ª–∏—à–∏—Ç–∏ None, —â–æ–± –≤–∑—è—Ç–∏ –≤—Å—ñ
    #     do_ttests=True,
    #     save_figs=True,
    #     show_figs=True,                # —É–≤—ñ–º–∫–Ω–∏ True, —è–∫—â–æ —Ö–æ—á–µ—à –ø–æ–∫–∞–∑ —É –Ω–æ—É—Ç–±—É—Ü—ñ
    #     export_tables=True
    # )
    
    # print("–ì—Ä–∞—Ñ—ñ–∫–∏:", *res["fig_paths"], sep="\n - ")
    # print("–¢–∞–±–ª–∏—Ü—ñ:", res["table_paths"])
    # print("Meta:", res["meta"])
    
    # # –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –ø–æ–¥–∏–≤–∏—Ç–∏—Å—å –¢–û–ü –ø–æ –≥–µ–π–Ω—É Fe (rbf):
    # (df := res["df_summary"])
    # print(df[df.kernel=="rbf"].sort_values("RMSE_Fe_gain_%", ascending=False).head(10))    
    
    compare_linear_vs_kernel_models()