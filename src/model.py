# model.py
import inspect
from abc import ABC, abstractmethod
from typing import Dict, Tuple, Type

import numpy as np
from scipy.stats import loguniform
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import (
    ConstantKernel as C,
    Product,
    RBF,
    Sum,
    WhiteKernel,
)
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVR

# ======================================================================
#                        Ğ‘ĞĞ—ĞĞ’Ğ Ğ¡Ğ¢Ğ ĞĞ¢Ğ•Ğ“Ğ†Ğ¯
# ======================================================================
class _BaseKernelModel(ABC):
    def __init__(self):
        self._kernel: str | None = None

    # obĞ¾Ğ²â€™ÑĞ·ĞºĞ¾Ğ²Ñ– API-Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¸
    @abstractmethod
    def fit(self, X: np.ndarray, Y: np.ndarray) -> None: ...

    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray: ...

    @abstractmethod
    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]: ...

    # kernel Ğ· Ğ¼Ğ¾Ğ¶Ğ»Ğ¸Ğ²Ñ–ÑÑ‚Ñ set
    @property
    def kernel(self) -> str | None:  # noqa: D401
        return self._kernel

    @kernel.setter
    def kernel(self, value: str) -> None:
        self._kernel = value.lower() if value else None


# ======================================================================
#                    KRR (Kernel Ridge Regression)
# ======================================================================
class _KRRModel(_BaseKernelModel):
    def __init__(
        self,
        kernel: str = "linear",
        alpha: float = 1.0,
        gamma: float | None = None,
        find_optimal_params: bool = False,
        n_iter_random_search: int = 20,
    ):
        super().__init__()
        self.kernel = kernel
        if self.kernel not in ("linear", "rbf"):
            raise ValueError(
                f"ĞĞµĞ²Ñ–Ğ´Ğ¾Ğ¼Ğµ ÑĞ´Ñ€Ğ¾ '{self.kernel}' Ğ´Ğ»Ñ KRR. ĞŸÑ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑÑ‚ÑŒÑÑ 'linear', 'rbf'."
            )

        self.alpha = alpha
        self.gamma = gamma
        self.find_optimal_params = find_optimal_params
        self.n_iter_random_search = n_iter_random_search

        self.model: KernelRidge | None = None
        self.X_train_: np.ndarray | None = None
        self.dual_coef_: np.ndarray | None = None
        self.coef_: np.ndarray | None = None
        self.intercept_: np.ndarray | None = None

    # ------------------------------------------------------------------
    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        if self.find_optimal_params:
            self.model = self._run_random_search(X, Y)
        else:
            gamma_eff = (
                self.gamma
                if self.gamma is not None
                else (
                    self._median_gamma(X)
                    if self.kernel == "rbf"
                    else None
                )
            )
            self.model = KernelRidge(alpha=self.alpha, kernel=self.kernel, gamma=gamma_eff)
            self.model.fit(X, Y)

        self.X_train_ = X.copy()
        self.dual_coef_ = self.model.dual_coef_
        if self.kernel == "linear":
            self.coef_ = X.T @ self.dual_coef_
            self.intercept_ = np.zeros(Y.shape[1])
        else:
            self.coef_ = None
            self.intercept_ = np.zeros(Y.shape[1])

    def predict(self, X: np.ndarray) -> np.ndarray:
        if self.model is None:
            raise RuntimeError("ĞœĞ¾Ğ´ĞµĞ»ÑŒ KRR Ğ½Ğµ Ğ½Ğ°Ğ²Ñ‡ĞµĞ½Ğ°.")
        return self.model.predict(X)

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        if X0.ndim == 1:
            X0 = X0.reshape(1, -1)

        if self.kernel == "linear":
            return self.coef_, self.intercept_

        gamma_eff = getattr(self.model, "gamma", None) or self._median_gamma(self.X_train_)
        diffs = X0[:, None, :] - self.X_train_[None, :, :]  # shape (1,n,d)
        sq = np.sum(diffs**2, axis=-1)
        K_row = np.exp(-gamma_eff * sq)
        dK_dX = -2 * gamma_eff * diffs * K_row[..., None]  # (1,n,d)

        W = np.einsum("ijk,ji->ki", dK_dX, self.dual_coef_)  # (d,m)
        b = (self.predict(X0) - X0 @ W).flatten()
        return np.clip(W, -1e3, 1e3), np.clip(b, -1e3, 1e3)

    # ------------------------------------------------------------------
    def _run_random_search(self, X, Y) -> KernelRidge:
        base = KernelRidge(kernel=self.kernel)
        if self.kernel == "linear":
            param_dist = {"alpha": loguniform(1e-3, 1e2)}
        else:
            param_dist = {
                "alpha": loguniform(1e-2, 1e2),
                "gamma": loguniform(1e-3, 1e1),
            }

        rs = RandomizedSearchCV(
            base,
            param_dist,
            n_iter=self.n_iter_random_search,
            cv=3,
            scoring="neg_mean_squared_error",
            random_state=42,
            n_jobs=-1,
            verbose=0,
        )
        rs.fit(X, Y)
        return rs.best_estimator_

    @staticmethod
    def _median_gamma(X: np.ndarray) -> float:
        subset = X if X.shape[0] <= 1000 else X[np.random.choice(X.shape[0], 1000, False)]
        d2 = np.sum((subset[:, None] - subset[None]) ** 2, axis=-1)
        med = np.median(d2[np.triu_indices_from(d2, 1)]) or 1.0
        return 1.0 / med


# ======================================================================
#                Gaussian Process Regression
# ======================================================================
class _GPRModel(_BaseKernelModel):
    def __init__(self):
        super().__init__()
        self.models: list[GaussianProcessRegressor] = []

    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        k_base = (
            C(1.0, (1e-3, 1e3))
            * RBF(1.0, (0.1, 20.0))
            + WhiteKernel(0.1, (1e-5, 1e1))
        )
        self.models = []
        for i in range(Y.shape[1]):
            gpr = GaussianProcessRegressor(
                kernel=k_base,
                alpha=0,
                normalize_y=True,
                n_restarts_optimizer=2,
            )
            gpr.fit(X, Y[:, i])
            self.models.append(gpr)

    def predict(self, X: np.ndarray) -> np.ndarray:
        return np.vstack([m.predict(X) for m in self.models]).T

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        if X0.ndim == 1:
            X0 = X0[None, :]
        W_cols, b_cols = [], []
        for gpr in self.models:
            rbf = self._find_rbf(gpr.kernel_)
            if rbf is None:
                raise RuntimeError("RBF-ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ ÑĞ´Ñ€Ğ° Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾.")
            gamma = 1.0 / (2 * rbf.length_scale**2)
            Xtr, alpha = gpr.X_train_, gpr.alpha_

            diffs = X0[:, None] - Xtr[None]
            sq = np.sum(diffs**2, axis=-1)
            K_row = np.exp(-gamma * sq)
            dK = -2 * gamma * diffs * K_row[..., None]

            W = (dK.squeeze(0).T @ alpha).reshape(-1, 1)
            y0 = gpr.predict(X0)
            b = (y0 - X0 @ W).flatten()
            W_cols.append(W)
            b_cols.append(b)
        return np.hstack(W_cols), np.hstack(b_cols)

    # ------------------------------------------------------------------
    @staticmethod
    def _find_rbf(kernel):
        if isinstance(kernel, RBF):
            return kernel
        if isinstance(kernel, (Product, Sum)):
            return _GPRModel._find_rbf(kernel.k1) or _GPRModel._find_rbf(kernel.k2)
        return None


# ======================================================================
#                   ĞĞĞ’Ğ ĞœĞĞ”Ğ•Ğ›Ğ¬ â€“ SVR (Support-Vector Regression)
# ======================================================================
class _SVRModel(_BaseKernelModel):
    def __init__(
        self,
        kernel: str = "rbf",
        C: float = 100.0,        # âœ… Ğ—Ğ±Ñ–Ğ»ÑŒÑˆĞµĞ½Ğ¾ Ğ´Ğ»Ñ ĞºÑ€Ğ°Ñ‰Ğ¾Ñ— Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ñ–
        epsilon: float = 0.01,   # âœ… Ğ—Ğ¼ĞµĞ½ÑˆĞµĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¼ĞµĞ½ÑˆĞ¾Ñ— Ñ‚Ğ¾Ğ»ĞµÑ€Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚Ñ–
        gamma: float | None = None,
        degree: int = 3,
        find_optimal_params: bool = False,
        n_iter_random_search: int = 30,
    ):
        super().__init__()
        self.kernel = kernel.lower()
        if self.kernel not in ("linear", "rbf", "poly"):
            raise ValueError("SVR Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ” Ğ»Ğ¸ÑˆĞµ 'linear', 'rbf', 'poly'.")

        self.C = C
        self.epsilon = epsilon
        self.gamma = gamma
        self.degree = degree
        self.find_optimal_params = find_optimal_params
        self.n_iter_random_search = n_iter_random_search
        self.models: list[SVR] = []

    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        n_targets = Y.shape[1]
        self.models.clear()
        self.X_train_ = X.copy()
    
        for k in range(n_targets):
            y = Y[:, k]
    
            if self.find_optimal_params:
                mdl = self._run_random_search(X, y)
            else:
                # âœ… Ğ’Ğ˜ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ Ğ»Ğ¾Ğ³Ñ–ĞºĞ° gamma
                if self.kernel == "rbf":
                    if self.gamma is not None:
                        gamma_eff = self.gamma
                    else:
                        # âœ… ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞ sklearn Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ° Ğ´Ğ»Ñ "scale"
                        gamma_eff = 1.0 / (X.shape[1] * X.var())
                else:
                    gamma_eff = self.gamma
    
                mdl = SVR(
                    kernel=self.kernel,
                    C=self.C,
                    epsilon=self.epsilon,
                    gamma=gamma_eff,
                    degree=self.degree,
                )
                mdl.fit(X, y)
                
                # âœ… Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾ Ñ‚Ğ¾Ñ‡Ğ½Ğµ gamma Ğ´Ğ»Ñ linearize()
                mdl._actual_gamma = gamma_eff if gamma_eff is not None else 'scale'
    
            self.models.append(mdl)

    def predict(self, X: np.ndarray) -> np.ndarray:
        if not self.models:
            raise RuntimeError("SVRModel Ğ½Ğµ Ğ½Ğ°Ğ²Ñ‡ĞµĞ½Ğ°.")
        preds = [m.predict(X) for m in self.models]
        return np.vstack(preds).T

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        if X0.ndim == 1:
            X0 = X0[None, :]
    
        W_cols, b_cols = [], []
        for mdl in self.models:
            if self.kernel == "linear":
                W = mdl.coef_.reshape(-1, 1)
                b = mdl.intercept_.copy()
                
            elif self.kernel == "rbf":
                # âœ… Ğ’Ğ˜ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ Ğ»Ğ¾Ğ³Ñ–ĞºĞ° gamma Ğ² linearize()
                if hasattr(mdl, '_actual_gamma') and isinstance(mdl._actual_gamma, float):
                    gamma_eff = mdl._actual_gamma
                else:
                    # âœ… ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞ sklearn Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°
                    gamma_eff = 1.0 / (self.X_train_.shape[1] * self.X_train_.var())
                
                sv = mdl.support_vectors_
                coef = mdl.dual_coef_.ravel()
    
                # âœ… Ğ’Ğ˜ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ• Ğ¾Ğ±Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ Ğ³Ñ€Ğ°Ğ´Ñ–Ñ”Ğ½Ñ‚Ñ–Ğ²:
                diffs = X0[:, None, :] - sv[None, :, :]  # (1, n_sv, n_features)
                sq = np.sum(diffs**2, axis=-1)           # (1, n_sv)
                K_row = np.exp(-gamma_eff * sq)          # (1, n_sv)
                dK = -2 * gamma_eff * diffs * K_row[..., None]  # (1, n_sv, n_features)
    
                # âœ… Ğ‘Ğ•Ğ—ĞŸĞ•Ğ§ĞĞ• Ğ²Ğ¸Ñ‚ÑĞ³ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ³Ñ€Ğ°Ğ´Ñ–Ñ”Ğ½Ñ‚Ñ–Ğ²:
                if dK.shape[0] == 1:
                    dK_2d = dK[0]  # (n_sv, n_features) 
                else:
                    dK_2d = dK.squeeze(0)  # Backup
                    
                W = (dK_2d.T @ coef).reshape(-1, 1)  # (n_features, 1)
                y0 = mdl.predict(X0)
                b = (y0 - X0 @ W).flatten()
                
            else:
                raise NotImplementedError(
                    "Ğ›Ñ–Ğ½ĞµĞ°Ñ€Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ SVR Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ”Ñ‚ÑŒÑÑ Ğ»Ğ¸ÑˆĞµ Ğ´Ğ»Ñ 'linear' Ñ‚Ğ° 'rbf'."
                )
    
            W_cols.append(np.clip(W, -1e3, 1e3))
            b_cols.append(np.clip(b, -1e3, 1e3))
    
        W_local = np.hstack(W_cols)
        b_local = np.hstack(b_cols)
        return W_local, b_local

    def _run_random_search(self, X, y) -> SVR:
        """ĞĞ¿Ñ‚Ğ¸Ğ¼Ñ–Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ²ĞµÑ€ÑÑ–Ñ Ğ· ÑˆĞ²Ğ¸Ğ´ÑˆĞ¾Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¾Ñ linear kernel"""
        
        # ğŸš€ Ğ¨Ğ’Ğ˜Ğ”ĞšĞ Ğ›ĞĞ“Ğ†ĞšĞ Ğ”Ğ›Ğ¯ LINEAR KERNEL
        if self.kernel == "linear":
            # Linear kernel Ğ½Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±ÑƒÑ” ÑĞºĞ»Ğ°Ğ´Ğ½Ğ¾Ñ— Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ñ–Ğ·Ğ°Ñ†Ñ–Ñ—
            param_dist = {
                "C": [1.0, 10.0, 100.0],           # Ğ”Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ– Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ
                "epsilon": [0.001, 0.01, 0.1]      # Ğ¢Ğ¸Ğ¿Ğ¾Ğ²Ñ– Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ
            }
            n_iter = 9  # 3x3 = Ğ²ÑÑ– ĞºĞ¾Ğ¼Ğ±Ñ–Ğ½Ğ°Ñ†Ñ–Ñ—
            cv_folds = 2  # ĞœĞµĞ½ÑˆĞµ Ñ„Ğ¾Ğ»Ğ´Ñ–Ğ²
            
            print(f"ğŸš€ SVR Linear: ÑˆĞ²Ğ¸Ğ´ĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ñ–Ğ·Ğ°Ñ†Ñ–Ñ {n_iter} ĞºĞ¾Ğ¼Ğ±Ñ–Ğ½Ğ°Ñ†Ñ–Ğ¹...")
            
        elif self.kernel == "rbf":
            # ĞŸĞ¾Ğ²Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ñ–Ğ·Ğ°Ñ†Ñ–Ñ Ğ´Ğ»Ñ RBF
            param_dist = {
                "C": loguniform(10, 1000),      
                "epsilon": loguniform(1e-3, 0.1),
                "gamma": loguniform(1e-4, 1e-1)
            }
            n_iter = self.n_iter_random_search
            cv_folds = min(3, len(y) // 50)
            
            print(f"ğŸ”§ SVR RBF: Ğ¿Ğ¾Ğ²Ğ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ñ–Ğ·Ğ°Ñ†Ñ–Ñ {n_iter} Ñ–Ñ‚ĞµÑ€Ğ°Ñ†Ñ–Ğ¹...")
            
        elif self.kernel == "poly":
            # ĞĞ±Ğ¼ĞµĞ¶ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ñ–Ğ·Ğ°Ñ†Ñ–Ñ Ğ´Ğ»Ñ poly
            param_dist = {
                "C": loguniform(10, 500),        # ĞœĞµĞ½ÑˆĞ¸Ğ¹ Ğ´Ñ–Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½
                "epsilon": loguniform(1e-3, 0.05),
                "gamma": loguniform(1e-4, 1e-2), # ĞœĞµĞ½ÑˆĞ¸Ğ¹ Ğ´Ñ–Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½
                "degree": [2, 3]                 # Ğ¢Ñ–Ğ»ÑŒĞºĞ¸ 2-3 ÑÑ‚ĞµĞ¿ĞµĞ½Ñ–
            }
            n_iter = min(20, self.n_iter_random_search)
            cv_folds = min(3, len(y) // 50)
            
            print(f"âš™ï¸ SVR Poly: Ğ¾Ğ±Ğ¼ĞµĞ¶ĞµĞ½Ğ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ñ–Ğ·Ğ°Ñ†Ñ–Ñ {n_iter} Ñ–Ñ‚ĞµÑ€Ğ°Ñ†Ñ–Ğ¹...")
        
        else:
            raise ValueError(f"ĞĞµĞ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒĞ²Ğ°Ğ½Ğ¸Ğ¹ kernel: {self.kernel}")
    
        base = SVR(kernel=self.kernel, degree=self.degree)
        
        # ğŸš€ Ğ’Ğ˜ĞšĞĞ Ğ˜Ğ¡Ğ¢ĞĞ’Ğ£Ğ„ĞœĞ GridSearchCV Ğ´Ğ»Ñ linear (ÑˆĞ²Ğ¸Ğ´ÑˆĞµ)
        if self.kernel == "linear":
            from sklearn.model_selection import GridSearchCV
            rs = GridSearchCV(
                base,
                param_dist,  # Ğ¢ÑƒÑ‚ Ñ†Ğµ Ğ±ÑƒĞ´Ğµ dict Ğ· lists
                cv=cv_folds,
                scoring="neg_mean_squared_error",
                n_jobs=-1,
                verbose=1  # ĞŸĞ¾ĞºĞ°Ğ·ÑƒÑ”Ğ¼Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑ
            )
        else:
            # RandomizedSearchCV Ğ´Ğ»Ñ Ñ–Ğ½ÑˆĞ¸Ñ… kernels
            rs = RandomizedSearchCV(
                base,
                param_dist,
                n_iter=n_iter,
                cv=cv_folds,
                scoring="neg_mean_squared_error",
                random_state=42,
                n_jobs=-1,
                verbose=1  # ĞŸĞ¾ĞºĞ°Ğ·ÑƒÑ”Ğ¼Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑ
            )
        
        rs.fit(X, y)
        
        # âœ… Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğµ gamma
        best_model = rs.best_estimator_
        if hasattr(best_model, 'gamma'):
            best_model._actual_gamma = best_model.gamma
        
        print(f"âœ… ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ– Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¸: {rs.best_params_}")
        
        return best_model

# ======================================================================
#                              FACADE
# ======================================================================
class KernelModel:
    """
    Ğ¯Ğ²Ğ»ÑÑ” ÑĞ¾Ğ±Ğ¾Ñ Â«Ñ‚Ğ¾Ğ½ĞºĞ¸Ğ¹Â» Ñ„Ğ°ÑĞ°Ğ´.  
    Ğ—Ğ° `model_type` Ğ²Ğ¸Ğ±Ğ¸Ñ€Ğ°Ñ”Ñ‚ÑŒÑÑ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ñ–Ñ, Ğ° Ğ·Ğ°Ğ¹Ğ²Ñ– kwargs
    Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ²Ñ–Ğ´ĞºĞ¸Ğ´Ğ°ÑÑ‚ÑŒÑÑ (Ñ‰Ğ¾Ğ± Ğ½Ğµ Ğ²Ğ¸Ğ½Ğ¸ĞºĞ°Ğ»Ğ¾ TypeError).
    """

    _REGISTRY: Dict[str, Type[_BaseKernelModel]] = {
        "krr": _KRRModel,
        "gpr": _GPRModel,
        "svr": _SVRModel,
    }

    def __init__(self, model_type: str = "krr", **kwargs):
        mtype = model_type.lower()
        if mtype not in self._REGISTRY:
            raise ValueError(f"ĞĞµĞ²Ñ–Ğ´Ğ¾Ğ¼Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ '{model_type}'. Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ–: {list(self._REGISTRY)}")

        impl_cls = self._REGISTRY[mtype]

        # --- Ñ„Ñ–Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ñ–Ñ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ–Ğ² Ğ¿Ñ–Ğ´ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¸Ğ¹ __init__ ---
        sig = inspect.signature(impl_cls.__init__)
        impl_kwargs = {k: v for k, v in kwargs.items() if k in sig.parameters}
        self._impl = impl_cls(**impl_kwargs)

    # API â€“ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ´ĞµĞ»ĞµĞ³ÑƒÑ”Ğ¼Ğ¾
    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        return self._impl.fit(X, Y)

    def predict(self, X: np.ndarray) -> np.ndarray:
        return self._impl.predict(X)

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        return self._impl.linearize(X0)

    def __getattr__(self, item):
        return getattr(self._impl, item)