# model.py
import inspect
from abc import ABC, abstractmethod
from typing import Dict, Tuple, Type

import numpy as np
from scipy.stats import loguniform
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import (
    ConstantKernel as C,
    Product,
    RBF,
    Sum,
    WhiteKernel,
)
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures

# ======================================================================
#                        –ë–ê–ó–û–í–ê –°–¢–†–ê–¢–ï–ì–Ü–Ø
# ======================================================================
class _BaseKernelModel(ABC):
    def __init__(self):
        self._kernel: str | None = None

    # ob–æ–≤‚Äô—è–∑–∫–æ–≤—ñ API-–º–µ—Ç–æ–¥–∏
    @abstractmethod
    def fit(self, X: np.ndarray, Y: np.ndarray) -> None: ...

    @abstractmethod
    def predict(self, X: np.ndarray) -> np.ndarray: ...

    @abstractmethod
    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]: ...

    # kernel –∑ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—é set
    @property
    def kernel(self) -> str | None:  # noqa: D401
        return self._kernel

    @kernel.setter
    def kernel(self, value: str) -> None:
        self._kernel = value.lower() if value else None


# ======================================================================
#                    KRR (Kernel Ridge Regression)
# ======================================================================
class _KRRModel(_BaseKernelModel):
    def __init__(
        self,
        kernel: str = "linear",
        alpha: float = 1.0,
        gamma: float | None = None,
        find_optimal_params: bool = False,
        n_iter_random_search: int = 20,
    ):
        super().__init__()
        self.kernel = kernel
        if self.kernel not in ("linear", "rbf"):
            raise ValueError(
                f"–ù–µ–≤—ñ–¥–æ–º–µ —è–¥—Ä–æ '{self.kernel}' –¥–ª—è KRR. –ü—ñ–¥—Ç—Ä–∏–º—É—é—Ç—å—Å—è 'linear', 'rbf'."
            )

        self.alpha = alpha
        self.gamma = gamma
        self.find_optimal_params = find_optimal_params
        self.n_iter_random_search = n_iter_random_search

        self.model: KernelRidge | None = None
        self.X_train_: np.ndarray | None = None
        self.dual_coef_: np.ndarray | None = None
        self.coef_: np.ndarray | None = None
        self.intercept_: np.ndarray | None = None

    # ------------------------------------------------------------------
    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        if self.find_optimal_params:
            self.model = self._run_random_search(X, Y)
        else:
            gamma_eff = (
                self.gamma
                if self.gamma is not None
                else (
                    self._median_gamma(X)
                    if self.kernel == "rbf"
                    else None
                )
            )
            self.model = KernelRidge(alpha=self.alpha, kernel=self.kernel, gamma=gamma_eff)
            self.model.fit(X, Y)

        self.X_train_ = X.copy()
        self.dual_coef_ = self.model.dual_coef_
        if self.kernel == "linear":
            self.coef_ = X.T @ self.dual_coef_
            self.intercept_ = np.zeros(Y.shape[1])
        else:
            self.coef_ = None
            self.intercept_ = np.zeros(Y.shape[1])

    def predict(self, X: np.ndarray) -> np.ndarray:
        if self.model is None:
            raise RuntimeError("–ú–æ–¥–µ–ª—å KRR –Ω–µ –Ω–∞–≤—á–µ–Ω–∞.")
        return self.model.predict(X)

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        if X0.ndim == 1:
            X0 = X0.reshape(1, -1)

        if self.kernel == "linear":
            return self.coef_, self.intercept_

        gamma_eff = getattr(self.model, "gamma", None) or self._median_gamma(self.X_train_)
        diffs = X0[:, None, :] - self.X_train_[None, :, :]  # shape (1,n,d)
        sq = np.sum(diffs**2, axis=-1)
        K_row = np.exp(-gamma_eff * sq)
        dK_dX = -2 * gamma_eff * diffs * K_row[..., None]  # (1,n,d)

        W = np.einsum("ijk,ji->ki", dK_dX, self.dual_coef_)  # (d,m)
        b = (self.predict(X0) - X0 @ W).flatten()
        return np.clip(W, -1e3, 1e3), np.clip(b, -1e3, 1e3)

    # ------------------------------------------------------------------
    def _run_random_search(self, X, Y) -> KernelRidge:
        base = KernelRidge(kernel=self.kernel)
        if self.kernel == "linear":
            param_dist = {"alpha": loguniform(1e-3, 1e2)}
        else:
            param_dist = {
                "alpha": loguniform(1e-2, 1e2),
                "gamma": loguniform(1e-3, 1e1),
            }

        rs = RandomizedSearchCV(
            base,
            param_dist,
            n_iter=self.n_iter_random_search,
            cv=3,
            scoring="neg_mean_squared_error",
            random_state=42,
            n_jobs=-1,
            verbose=0,
        )
        rs.fit(X, Y)
        return rs.best_estimator_

    @staticmethod
    def _median_gamma(X: np.ndarray) -> float:
        subset = X if X.shape[0] <= 1000 else X[np.random.choice(X.shape[0], 1000, False)]
        d2 = np.sum((subset[:, None] - subset[None]) ** 2, axis=-1)
        med = np.median(d2[np.triu_indices_from(d2, 1)]) or 1.0
        return 1.0 / med


# ======================================================================
#                Gaussian Process Regression
# ======================================================================
class _GPRModel(_BaseKernelModel):
    def __init__(self):
        super().__init__()
        self.models: list[GaussianProcessRegressor] = []

    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        k_base = (
            C(1.0, (1e-3, 1e3))
            * RBF(1.0, (0.1, 20.0))
            + WhiteKernel(0.1, (1e-5, 1e1))
        )
        self.models = []
        for i in range(Y.shape[1]):
            gpr = GaussianProcessRegressor(
                kernel=k_base,
                alpha=0,
                normalize_y=True,
                n_restarts_optimizer=2,
            )
            gpr.fit(X, Y[:, i])
            self.models.append(gpr)

    def predict(self, X: np.ndarray) -> np.ndarray:
        return np.vstack([m.predict(X) for m in self.models]).T

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        if X0.ndim == 1:
            X0 = X0[None, :]
        W_cols, b_cols = [], []
        for gpr in self.models:
            rbf = self._find_rbf(gpr.kernel_)
            if rbf is None:
                raise RuntimeError("RBF-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç —è–¥—Ä–∞ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
            gamma = 1.0 / (2 * rbf.length_scale**2)
            Xtr, alpha = gpr.X_train_, gpr.alpha_

            diffs = X0[:, None] - Xtr[None]
            sq = np.sum(diffs**2, axis=-1)
            K_row = np.exp(-gamma * sq)
            dK = -2 * gamma * diffs * K_row[..., None]

            W = (dK.squeeze(0).T @ alpha).reshape(-1, 1)
            y0 = gpr.predict(X0)
            b = (y0 - X0 @ W).flatten()
            W_cols.append(W)
            b_cols.append(b)
        return np.hstack(W_cols), np.hstack(b_cols)

    # ------------------------------------------------------------------
    @staticmethod
    def _find_rbf(kernel):
        if isinstance(kernel, RBF):
            return kernel
        if isinstance(kernel, (Product, Sum)):
            return _GPRModel._find_rbf(kernel.k1) or _GPRModel._find_rbf(kernel.k2)
        return None


# ======================================================================
#                   –ù–û–í–ê –ú–û–î–ï–õ–¨ ‚Äì SVR (Support-Vector Regression)
# ======================================================================
class _SVRModel(_BaseKernelModel):
    def __init__(
        self,
        kernel: str = "rbf",
        C: float = 100.0,        # ‚úÖ –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –∫—Ä–∞—â–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ
        epsilon: float = 0.01,   # ‚úÖ –ó–º–µ–Ω—à–µ–Ω–æ –¥–ª—è –º–µ–Ω—à–æ—ó —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—ñ
        gamma: float | None = None,
        degree: int = 3,
        find_optimal_params: bool = False,
        n_iter_random_search: int = 30,
    ):
        super().__init__()
        self.kernel = kernel.lower()
        if self.kernel not in ("linear", "rbf", "poly"):
            raise ValueError("SVR –ø—ñ–¥—Ç—Ä–∏–º—É—î –ª–∏—à–µ 'linear', 'rbf', 'poly'.")

        self.C = C
        self.epsilon = epsilon
        self.gamma = gamma
        self.degree = degree
        self.find_optimal_params = find_optimal_params
        self.n_iter_random_search = n_iter_random_search
        self.models: list[SVR] = []

    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        n_targets = Y.shape[1]
        self.models.clear()
        self.X_train_ = X.copy()
    
        for k in range(n_targets):
            y = Y[:, k]
    
            if self.find_optimal_params:
                mdl = self._run_random_search(X, y)
            else:
                # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ê –ª–æ–≥—ñ–∫–∞ gamma
                if self.kernel == "rbf":
                    if self.gamma is not None:
                        gamma_eff = self.gamma
                    else:
                        # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ê sklearn —Ñ–æ—Ä–º—É–ª–∞ –¥–ª—è "scale"
                        gamma_eff = 1.0 / (X.shape[1] * X.var())
                else:
                    gamma_eff = self.gamma
    
                mdl = SVR(
                    kernel=self.kernel,
                    C=self.C,
                    epsilon=self.epsilon,
                    gamma=gamma_eff,
                    degree=self.degree,
                )
                mdl.fit(X, y)
                
                # ‚úÖ –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç–æ—á–Ω–µ gamma –¥–ª—è linearize()
                mdl._actual_gamma = gamma_eff if gamma_eff is not None else 'scale'
    
            self.models.append(mdl)

    def predict(self, X: np.ndarray) -> np.ndarray:
        if not self.models:
            raise RuntimeError("SVRModel –Ω–µ –Ω–∞–≤—á–µ–Ω–∞.")
        preds = [m.predict(X) for m in self.models]
        return np.vstack(preds).T

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        if X0.ndim == 1:
            X0 = X0[None, :]
    
        W_cols, b_cols = [], []
        for mdl in self.models:
            if self.kernel == "linear":
                W = mdl.coef_.reshape(-1, 1)
                b = mdl.intercept_.copy()
                
            elif self.kernel == "rbf":
                # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ê –ª–æ–≥—ñ–∫–∞ gamma –≤ linearize()
                if hasattr(mdl, '_actual_gamma') and isinstance(mdl._actual_gamma, float):
                    gamma_eff = mdl._actual_gamma
                else:
                    # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ê sklearn —Ñ–æ—Ä–º—É–ª–∞
                    gamma_eff = 1.0 / (self.X_train_.shape[1] * self.X_train_.var())
                
                sv = mdl.support_vectors_
                coef = mdl.dual_coef_.ravel()
    
                # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ï –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤:
                diffs = X0[:, None, :] - sv[None, :, :]  # (1, n_sv, n_features)
                sq = np.sum(diffs**2, axis=-1)           # (1, n_sv)
                K_row = np.exp(-gamma_eff * sq)          # (1, n_sv)
                dK = -2 * gamma_eff * diffs * K_row[..., None]  # (1, n_sv, n_features)
    
                # ‚úÖ –ë–ï–ó–ü–ï–ß–ù–ï –≤–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤:
                if dK.shape[0] == 1:
                    dK_2d = dK[0]  # (n_sv, n_features) 
                else:
                    dK_2d = dK.squeeze(0)  # Backup
                    
                W = (dK_2d.T @ coef).reshape(-1, 1)  # (n_features, 1)
                y0 = mdl.predict(X0)
                b = (y0 - X0 @ W).flatten()
                
            else:
                raise NotImplementedError(
                    "–õ—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—è SVR –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è –ª–∏—à–µ –¥–ª—è 'linear' —Ç–∞ 'rbf'."
                )
    
            W_cols.append(np.clip(W, -1e3, 1e3))
            b_cols.append(np.clip(b, -1e3, 1e3))
    
        W_local = np.hstack(W_cols)
        b_local = np.hstack(b_cols)
        return W_local, b_local

    def _run_random_search(self, X, y) -> SVR:
        """–û–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞ –≤–µ—Ä—Å—ñ—è –∑ —à–≤–∏–¥—à–æ—é –æ–±—Ä–æ–±–∫–æ—é linear kernel"""
        
        # üöÄ –®–í–ò–î–ö–ê –õ–û–ì–Ü–ö–ê –î–õ–Ø LINEAR KERNEL
        if self.kernel == "linear":
            # Linear kernel –Ω–µ –ø–æ—Ç—Ä–µ–±—É—î —Å–∫–ª–∞–¥–Ω–æ—ó –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
            param_dist = {
                "C": [1.0, 10.0, 100.0],           # –î–∏—Å–∫—Ä–µ—Ç–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
                "epsilon": [0.001, 0.01, 0.1]      # –¢–∏–ø–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
            }
            n_iter = 9  # 3x3 = –≤—Å—ñ –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó
            cv_folds = 2  # –ú–µ–Ω—à–µ —Ñ–æ–ª–¥—ñ–≤
            
            print(f"üöÄ SVR Linear: —à–≤–∏–¥–∫–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è {n_iter} –∫–æ–º–±—ñ–Ω–∞—Ü—ñ–π...")
            
        elif self.kernel == "rbf":
            # –ü–æ–≤–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è RBF
            param_dist = {
                "C": loguniform(10, 1000),      
                "epsilon": loguniform(1e-3, 0.1),
                "gamma": loguniform(1e-4, 1e-1)
            }
            n_iter = self.n_iter_random_search
            cv_folds = min(3, len(y) // 50)
            
            print(f"üîß SVR RBF: –ø–æ–≤–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è {n_iter} —ñ—Ç–µ—Ä–∞—Ü—ñ–π...")
            
        elif self.kernel == "poly":
            # –û–±–º–µ–∂–µ–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è poly
            param_dist = {
                "C": loguniform(10, 500),        # –ú–µ–Ω—à–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω
                "epsilon": loguniform(1e-3, 0.05),
                "gamma": loguniform(1e-4, 1e-2), # –ú–µ–Ω—à–∏–π –¥—ñ–∞–ø–∞–∑–æ–Ω
                "degree": [2, 3]                 # –¢—ñ–ª—å–∫–∏ 2-3 —Å—Ç–µ–ø–µ–Ω—ñ
            }
            n_iter = min(20, self.n_iter_random_search)
            cv_folds = min(3, len(y) // 50)
            
            print(f"‚öôÔ∏è SVR Poly: –æ–±–º–µ–∂–µ–Ω–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è {n_iter} —ñ—Ç–µ—Ä–∞—Ü—ñ–π...")
        
        else:
            raise ValueError(f"–ù–µ–ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω–∏–π kernel: {self.kernel}")
    
        base = SVR(kernel=self.kernel, degree=self.degree)
        
        # üöÄ –í–ò–ö–û–†–ò–°–¢–û–í–£–Ñ–ú–û GridSearchCV –¥–ª—è linear (—à–≤–∏–¥—à–µ)
        if self.kernel == "linear":
            from sklearn.model_selection import GridSearchCV
            rs = GridSearchCV(
                base,
                param_dist,  # –¢—É—Ç —Ü–µ –±—É–¥–µ dict –∑ lists
                cv=cv_folds,
                scoring="neg_mean_squared_error",
                n_jobs=-1,
                verbose=1  # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
            )
        else:
            # RandomizedSearchCV –¥–ª—è —ñ–Ω—à–∏—Ö kernels
            rs = RandomizedSearchCV(
                base,
                param_dist,
                n_iter=n_iter,
                cv=cv_folds,
                scoring="neg_mean_squared_error",
                random_state=42,
                n_jobs=-1,
                verbose=1  # –ü–æ–∫–∞–∑—É—î–º–æ –ø—Ä–æ–≥—Ä–µ—Å
            )
        
        rs.fit(X, y)
        
        # ‚úÖ –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–µ gamma
        best_model = rs.best_estimator_
        if hasattr(best_model, 'gamma'):
            best_model._actual_gamma = best_model.gamma
        
        print(f"‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏: {rs.best_params_}")
        
        return best_model

# ======================================================================
#                   LINEAR MODEL (–¥–ª—è L-MPC)
# ======================================================================

class _LinearModel(_BaseKernelModel):
    """
    –õ—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è L-MPC –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é —Ä—ñ–∑–Ω–∏—Ö —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ–π —Ç–∞ –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫
    """
    
    def __init__(
        self,
        linear_type: str = "ols",         # "ols", "ridge", "lasso" 
        alpha: float = 1.0,               # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó –¥–ª—è Ridge/Lasso
        poly_degree: int = 1,             # –°—Ç–µ–ø—ñ–Ω—å –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ (1=–ª—ñ–Ω—ñ–π–Ω–∞)
        include_bias: bool = True,        # –í–∫–ª—é—á–∞—Ç–∏ bias —Ç–µ—Ä–º–∏–Ω
        find_optimal_params: bool = False, # –ü–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        n_iter_random_search: int = 20,
    ):
        super().__init__()
        self.linear_type = linear_type.lower()
        self.alpha = alpha
        self.poly_degree = poly_degree
        self.include_bias = include_bias
        self.find_optimal_params = find_optimal_params
        self.n_iter_random_search = n_iter_random_search
        
        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
        if self.linear_type not in ("ols", "ridge", "lasso"):
            raise ValueError("linear_type –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ 'ols', 'ridge' –∞–±–æ 'lasso'")
        
        if self.poly_degree < 1 or self.poly_degree > 3:
            raise ValueError("poly_degree –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –≤—ñ–¥ 1 –¥–æ 3")
        
        # –í–Ω—É—Ç—Ä—ñ—à–Ω—ñ –∞—Ç—Ä–∏–±—É—Ç–∏
        self.model: LinearRegression | Ridge | Lasso | None = None
        self.poly_features: PolynomialFeatures | None = None
        self.coef_: np.ndarray | None = None
        self.intercept_: np.ndarray | None = None
        
        # –î–ª—è compatibility –∑ kernel –º–æ–¥–µ–ª—è–º–∏
        self._kernel = "linear"  # –ü–æ–∑–Ω–∞—á–∞—î–º–æ —è–∫ –ª—ñ–Ω—ñ–π–Ω—É

    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        """–ù–∞–≤—á–∞–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ"""
        
        # üîß –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if self.poly_degree > 1:
            self.poly_features = PolynomialFeatures(
                degree=self.poly_degree,
                include_bias=False  # bias –¥–æ–¥–∞–º–æ –≤ –º–æ–¥–µ–ª—å
            )
            X_features = self.poly_features.fit_transform(X)
        else:
            self.poly_features = None
            X_features = X
            
        # üéØ –í–∏–±—ñ—Ä —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        if self.find_optimal_params:
            self.model = self._run_random_search(X_features, Y)
        else:
            if self.linear_type == "ols":
                self.model = LinearRegression(fit_intercept=self.include_bias)
            elif self.linear_type == "ridge":
                self.model = Ridge(alpha=self.alpha, fit_intercept=self.include_bias)
            elif self.linear_type == "lasso":
                self.model = Lasso(alpha=self.alpha, fit_intercept=self.include_bias, max_iter=2000)
        
        # üöÄ –ù–∞–≤—á–∞–Ω–Ω—è
        self.model.fit(X_features, Y)
        
        # üìä –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –¥–æ—Å—Ç—É–ø—É
        self.coef_ = self.model.coef_.T if Y.ndim > 1 else self.model.coef_.reshape(-1, 1)
        self.intercept_ = (
            self.model.intercept_ if hasattr(self.model, 'intercept_') 
            else np.zeros(Y.shape[1] if Y.ndim > 1 else 1)
        )
        
        print(f"‚úÖ Linear Model –Ω–∞–≤—á–µ–Ω–∞: {self.linear_type}, poly_degree={self.poly_degree}")
        print(f"   –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ shape: {self.coef_.shape}, Intercept: {self.intercept_.shape}")

    def predict(self, X: np.ndarray) -> np.ndarray:
        """–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ"""
        if self.model is None:
            raise RuntimeError("Linear Model –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
            
        # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if self.poly_features is not None:
            X_features = self.poly_features.transform(X)
        else:
            X_features = X
            
        return self.model.predict(X_features)

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        –õ—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—è –¥–ª—è –ª—ñ–Ω—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ
        
        Returns:
            W: (n_features, n_outputs) - –≥—Ä–∞–¥—ñ—î–Ω—Ç –º–∞—Ç—Ä–∏—Ü—è (—è–∫ —É K-MPC)
            b: (n_outputs,) - –∑–º—ñ—â–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä
        """
        if X0.ndim == 1:
            X0 = X0.reshape(1, -1)
            
        if self.coef_ is None:
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–≤—á–µ–Ω–∞!")
            
        # üéØ –õ–Ü–ù–Ü–ô–ù–ò–ô –í–ò–ü–ê–î–û–ö (poly_degree = 1)
        if self.poly_degree == 1:
            # sklearn LinearRegression: coef_ = (n_features, n_outputs)
            W = self.coef_  # ‚úÖ –ó–∞–ª–∏—à–∞—î–º–æ (n_features, n_outputs) —è–∫ —É K-MPC
            b = self.intercept_  # (n_outputs,)
            return W, b
            
        # üéØ –ü–û–õ–Ü–ù–û–ú–Ü–ê–õ–¨–ù–ò–ô –í–ò–ü–ê–î–û–ö (poly_degree > 1)
        else:
            # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–û: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ include_bias=False
            grad_poly = self._compute_polynomial_gradient(X0)  # (n_samples, n_features, n_poly_features_no_bias)
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ
            n_samples, n_features, n_poly_features = grad_poly.shape
            n_coef_features, n_outputs = self.coef_.shape
            
            if n_poly_features != n_coef_features:
                raise ValueError(f"–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ {n_poly_features} != —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤ {n_coef_features}")
            
            # W = –≥—Ä–∞–¥—ñ—î–Ω—Ç * –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏
            W = np.einsum('ijk,kl->ijl', grad_poly, self.coef_)  # (n_samples, n_features, n_outputs)
            
            # –ë–µ—Ä–µ–º–æ –ø–µ—Ä—à–∏–π —Å–µ–º–ø–ª: (n_features, n_outputs) - —è–∫ —É K-MPC
            W_local = W[0]  # ‚úÖ (n_features, n_outputs)
            
            # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ï –ú–ù–û–ñ–ï–ù–ù–Ø: X @ W (—è–∫ —É K-MPC)
            y0 = self.predict(X0)
            b_local = y0[0] - X0[0] @ W_local
            
            return W_local, b_local

    def _compute_polynomial_gradient(self, X0: np.ndarray) -> np.ndarray:
        """
        –û–±—á–∏—Å–ª—é—î –≥—Ä–∞–¥—ñ—î–Ω—Ç –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫ –≤ —Ç–æ—á—Ü—ñ X0
        
        Returns:
            grad_poly: (n_samples, n_features, n_poly_features) - –≥—Ä–∞–¥—ñ—î–Ω—Ç –º–∞—Ç—Ä–∏—Ü—è
        """
        from sklearn.preprocessing import PolynomialFeatures
        
        n_samples, n_features = X0.shape
        
        # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–û: include_bias=False, —â–æ–± –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ sklearn coef_
        poly = PolynomialFeatures(degree=self.poly_degree, include_bias=False)
        
        # –§—ñ—Ç—É—î–º–æ –Ω–∞ dummy –¥–∞–Ω–∏—Ö —â–æ–± –æ—Ç—Ä–∏–º–∞—Ç–∏ powers_
        dummy_X = np.ones((1, n_features))
        poly.fit(dummy_X)
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –ë–ï–ó bias
        n_poly_features = len(poly.powers_)
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç
        grad_poly = np.zeros((n_samples, n_features, n_poly_features))
        
        # –û–±—á–∏—Å–ª—é—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç –¥–ª—è –∫–æ–∂–Ω–æ—ó –ø–æ–ª—ñ–Ω–æ–º—ñ–∞–ª—å–Ω–æ—ó –æ–∑–Ω–∞–∫–∏
        for i, powers in enumerate(poly.powers_):
            # powers - –º–∞—Å–∏–≤ —Å—Ç–µ–ø–µ–Ω—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ—ó –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ—ó –æ–∑–Ω–∞–∫–∏
            for j in range(n_features):
                if powers[j] > 0:
                    # ‚àÇ(x‚ÇÅ^p‚ÇÅ * x‚ÇÇ^p‚ÇÇ * ... * x‚±º^p‚±º * ...)/‚àÇx‚±º = p‚±º * x‚ÇÅ^p‚ÇÅ * ... * x‚±º^(p‚±º-1) * ...
                    grad_powers = powers.copy()
                    grad_powers[j] -= 1
                    
                    # –û–±—á–∏—Å–ª—é—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞
                    grad_value = powers[j]  # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –≤—ñ–¥ –¥–∏—Ñ–µ—Ä–µ–Ω—Ü—ñ—é–≤–∞–Ω–Ω—è
                    
                    for k in range(n_features):
                        if grad_powers[k] > 0:
                            grad_value *= (X0[:, k] ** grad_powers[k])
                        # –Ø–∫—â–æ grad_powers[k] == 0, —Ç–æ x^0 = 1 (–Ω–µ –º–Ω–æ–∂–∏–º–æ)
                    
                    grad_poly[:, j, i] = grad_value
        
        return grad_poly

    def _run_random_search(self, X, Y):
        """–ü–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤"""
        
        if self.linear_type == "ols":
            # OLS –Ω–µ –º–∞—î –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
            return LinearRegression(fit_intercept=self.include_bias)
            
        elif self.linear_type == "ridge":
            param_dist = {"alpha": loguniform(1e-3, 1e3)}
            base = Ridge(fit_intercept=self.include_bias)
            
        elif self.linear_type == "lasso":
            param_dist = {"alpha": loguniform(1e-4, 1e1)}
            base = Lasso(fit_intercept=self.include_bias, max_iter=2000)
            
        rs = RandomizedSearchCV(
            base,
            param_dist,
            n_iter=self.n_iter_random_search,
            cv=3,
            scoring="neg_mean_squared_error",
            random_state=42,
            n_jobs=-1,
            verbose=1
        )
        
        rs.fit(X, Y)
        print(f"‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ Linear {self.linear_type}: {rs.best_params_}")
        
        return rs.best_estimator_
# ======================================================================
#                              FACADE
# ======================================================================
class KernelModel:
    """
    –Ø–≤–ª—è—î —Å–æ–±–æ—é ¬´—Ç–æ–Ω–∫–∏–π¬ª —Ñ–∞—Å–∞–¥.  
    –ó–∞ `model_type` –≤–∏–±–∏—Ä–∞—î—Ç—å—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—è, –∞ –∑–∞–π–≤—ñ kwargs
    –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤—ñ–¥–∫–∏–¥–∞—é—Ç—å—Å—è (—â–æ–± –Ω–µ –≤–∏–Ω–∏–∫–∞–ª–æ TypeError).
    """

    _REGISTRY: Dict[str, Type[_BaseKernelModel]] = {
        "krr": _KRRModel,
        "gpr": _GPRModel,
        "svr": _SVRModel,
        "linear": _LinearModel,  # üÜï –î–û–î–ê–ù–û L-MPC!

    }

    def __init__(self, model_type: str = "krr", **kwargs):
        mtype = model_type.lower()
        if mtype not in self._REGISTRY:
            raise ValueError(f"–ù–µ–≤—ñ–¥–æ–º–∞ –º–æ–¥–µ–ª—å '{model_type}'. –î–æ—Å—Ç—É–ø–Ω—ñ: {list(self._REGISTRY)}")

        impl_cls = self._REGISTRY[mtype]

        # --- —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤ –ø—ñ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π __init__ ---
        sig = inspect.signature(impl_cls.__init__)
        impl_kwargs = {k: v for k, v in kwargs.items() if k in sig.parameters}
        self._impl = impl_cls(**impl_kwargs)

    # API ‚Äì –ø—Ä–æ—Å—Ç–æ –¥–µ–ª–µ–≥—É—î–º–æ
    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        return self._impl.fit(X, Y)

    def predict(self, X: np.ndarray) -> np.ndarray:
        return self._impl.predict(X)

    def linearize(self, X0: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        return self._impl.linearize(X0)

    def __getattr__(self, item):
        return getattr(self._impl, item)