# sim.py

import numpy as np
import pandas as pd
from typing import Callable, Dict, Any, Tuple
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from collections import deque

from data_gen import StatefulDataGenerator
from model import KernelModel
from objectives import MaxIronMassTrackingObjective
from mpc import MPCController
from utils import (
    run_post_simulation_analysis_enhanced, diagnose_mpc_behavior, diagnose_ekf_detailed
    
)
from ekf import ExtendedKalmanFilter
from anomaly_detector import SignalAnomalyDetector
from maf import MovingAverageFilter
import json
from pathlib import Path
from typing import Optional, Dict


# =============================================================================
# === –ë–õ–û–ö 1: –ü–Ü–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ò–• –¢–ê –°–ö–ê–õ–ï–†–Ü–í ===
# =============================================================================

def prepare_simulation_data(
    reference_df: pd.DataFrame,
    params: Dict[str, Any]
) -> Tuple[StatefulDataGenerator, pd.DataFrame, np.ndarray, np.ndarray]:
    """
    –°—Ç–≤–æ—Ä—é—î –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, —Ñ–æ—Ä–º—É—î —á–∞—Å–æ–≤–∏–π —Ä—è–¥ —ñ–∑ –∞–Ω–æ–º–∞–ª—ñ—è–º–∏,
    –ü–†–û–ú–ò–í–ê–Ñ –π–æ–≥–æ SignalAnomalyDetector-–æ–º, —Ç–∞ –±—É–¥—É—î
    –ª–∞–≥–æ–≤–∞–Ω—ñ –º–∞—Ç—Ä–∏—Ü—ñ X, Y –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è/—Å–∏–º—É–ª—è—Ü—ñ—ó.
    """
    print("–ö—Ä–æ–∫ 1: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–º—É–ª—è—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö...")

    # 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä ¬´plant¬ª
    true_gen = StatefulDataGenerator(
        reference_df,
        ore_flow_var_pct=3.0,
        time_step_s=params['time_step_s'],
        time_constants_s=params['time_constants_s'],
        dead_times_s=params['dead_times_s'],
        true_model_type=params['plant_model_type'],
        seed=params['seed']
    )

    # 2. –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∞–Ω–æ–º–∞–ª—ñ–π
    anomaly_cfg = StatefulDataGenerator.generate_anomaly_config(
        N_data=params['N_data'],
        train_frac=params['train_size'],
        val_frac=params['val_size'],
        test_frac=params['test_size'],
        seed=params['seed']
    )
    # 3. –ì–µ–Ω–µ—Ä—É—î–º–æ –ø–æ–≤–Ω–∏–π —á–∞—Å–æ–≤–∏–π —Ä—è–¥ (–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏)
    df_true_orig = true_gen.generate(
        T=params['N_data'],
        control_pts=params['control_pts'],
        n_neighbors=params['n_neighbors'],
        noise_level=params['noise_level'],
        anomaly_config=anomaly_cfg
    )
    
    if params['enable_nonlinear']:
        # 4. –í–∏–∑–Ω–∞—á–∞—î–º–æ, —è–∫ –º–∏ —Ö–æ—á–µ–º–æ –ø–æ—Å–∏–ª–∏—Ç–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å
        nonlinear_config = params['nonlinear_config']
           
        # 5. –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∑ –ø–æ—Å–∏–ª–µ–Ω–æ—é –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—é
        #    –ú–æ–∂–Ω–∞ –ø–µ—Ä–µ–¥–∞—Ç–∏ —Ç–æ–π —Å–∞–º–∏–π —Ä—ñ–≤–µ–Ω—å —à—É–º—É —Ç–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∞–Ω–æ–º–∞–ª—ñ–π
        df_true = true_gen.generate_nonlinear_variant(
            base_df=df_true_orig,
            non_linear_factors=nonlinear_config,
            noise_level='none',
            anomaly_config=None # –∞–±–æ –ø–µ—Ä–µ–¥–∞—Ç–∏ —Å—é–¥–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∞–Ω–æ–º–∞–ª—ñ–π
        )
    else:
        df_true=df_true_orig
    
    # 6. OFFLINE-–û–ß–ò–©–ï–ù–ù–Ø –≤—Ö—ñ–¥–Ω–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤ –≤—ñ–¥ –∞–Ω–æ–º–∞–ª—ñ–π
    #    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ —Å–∞–º—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è, —â–æ –π –≤ online-—Ü–∏–∫–ª—ñ, –∞–±–æ –º–µ–Ω—à –∂–æ—Ä—Å—Ç–∫—ñ.
    ad_config = params.get('anomaly_params', {})
    ad_feed_fe = SignalAnomalyDetector(**ad_config)
    ad_ore_flow = SignalAnomalyDetector(**ad_config)


    filtered_feed = []
    filtered_ore  = []
    for raw_fe, raw_ore in zip(df_true['feed_fe_percent'], df_true['ore_mass_flow']):
        filtered_feed.append(ad_feed_fe.update(raw_fe))
        filtered_ore.append(ad_ore_flow.update(raw_ore))

    # –ü—ñ–¥–º—ñ–Ω—é—î–º–æ ¬´–±—Ä—É–¥–Ω—ñ¬ª –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞ ¬´–æ—á–∏—â–µ–Ω—ñ¬ª
    df_true = df_true.copy()
    df_true['feed_fe_percent'] = filtered_feed
    df_true['ore_mass_flow']   = filtered_ore

    # 7. –õ–∞–≥–æ–≤–∞–Ω—ñ –≤–∏–±—ñ—Ä–∫–∏ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è/—Å–∏–º—É–ª—è—Ü—ñ—ó
    X, Y_full_np = StatefulDataGenerator.create_lagged_dataset(
        df_true,
        lags=params['lag']
    )
    # –í–∏–±–∏—Ä–∞—î–º–æ –ª–∏—à–µ concentrate_fe —Ç–∞ concentrate_mass –∫–æ–ª–æ–Ω–∫–∏
    Y = Y_full_np[:, [0, 2]]

    return true_gen, df_true, X, Y

def split_and_scale_data(
    X: np.ndarray,
    Y: np.ndarray,
    params: Dict[str, Any]
) -> Tuple[Dict[str, np.ndarray], StandardScaler, StandardScaler]:
    """
    –†–æ–∑–±–∏–≤–∞—î –¥–∞–Ω—ñ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π/–≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π/—Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±–æ—Ä–∏ —Ç–∞ –º–∞—Å—à—Ç–∞–±—É—î —ó—Ö.
    
    Args:
        X: –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ.
        Y: –í–∏—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ.
        params: –°–ª–æ–≤–Ω–∏–∫ –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ä–æ–∑–±–∏—Ç—Ç—è.

    Returns:
        –ö–æ—Ä—Ç–µ–∂ –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–æ–º –¥–∞–Ω–∏—Ö —Ç–∞ –Ω–∞–≤—á–µ–Ω–∏–º–∏ —Å–∫–∞–ª–µ—Ä–∞–º–∏ –¥–ª—è X —Ç–∞ Y.
    """
    n = X.shape[0]
    n_train = int(params['train_size'] * n)
    n_val = int(params['val_size'] * n)

    data_splits = {
        'X_train': X[:n_train], 'Y_train': Y[:n_train],
        'X_val': X[n_train:n_train + n_val], 'Y_val': Y[n_train:n_train + n_val],
        'X_test': X[n_train + n_val:], 'Y_test': Y[n_train + n_val:]
    }

    x_scaler = StandardScaler()
    y_scaler = StandardScaler()

    data_splits['X_train_scaled'] = x_scaler.fit_transform(data_splits['X_train'])
    data_splits['Y_train_scaled'] = y_scaler.fit_transform(data_splits['Y_train'])
    data_splits['X_val_scaled'] = x_scaler.transform(data_splits['X_val'])
    data_splits['Y_val_scaled'] = y_scaler.transform(data_splits['Y_val'])
    data_splits['X_test_scaled'] = x_scaler.transform(data_splits['X_test'])
    data_splits['Y_test_scaled'] = y_scaler.transform(data_splits['Y_test'])
    
    return data_splits, x_scaler, y_scaler


# =============================================================================
# === –ë–õ–û–ö 2: –Ü–ù–Ü–¶–Ü–ê–õ–Ü–ó–ê–¶–Ü–Ø –ö–û–ú–ü–û–ù–ï–ù–¢–Ü–í MPC —Ç–∞ EKF ===
# =============================================================================


def train_and_evaluate_model(
    mpc: MPCController,
    data: Dict[str, np.ndarray],
    y_scaler: StandardScaler
) -> Dict[str, float]:
    """
    –ù–∞–≤—á–∞—î –º–æ–¥–µ–ª—å –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ MPC —Ç–∞ –æ—Ü—ñ–Ω—é—î —ó—ó —è–∫—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö.
    """
    print("–ö—Ä–æ–∫ 3: –ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –ø—Ä–æ—Ü–µ—Å—É...")
    mpc.fit(data['X_train_scaled'], data['Y_train_scaled'])

    y_pred_scaled = mpc.model.predict(data['X_test_scaled'])
    y_pred_orig = y_scaler.inverse_transform(y_pred_scaled)
    
    test_mse = mean_squared_error(data['Y_test'], y_pred_orig)
    print(f"-> –ó–∞–≥–∞–ª—å–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö (MSE): {test_mse:.4f}")
    
    metrics = {'test_mse_total': test_mse}
    output_columns = ['conc_fe', 'conc_mass']
    for i, col in enumerate(output_columns):
        rmse = np.sqrt(mean_squared_error(data['Y_test'][:, i], y_pred_orig[:, i]))
        metrics[f'test_rmse_{col}'] = rmse
        print(f"-> RMSE –¥–ª—è {col}: {rmse:.3f}")
        
    return metrics

def initialize_mpc_controller_enhanced(
    params: Dict[str, Any],
    x_scaler: StandardScaler,
    y_scaler: StandardScaler
) -> MPCController:
    """
    –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î –ø–æ–∫—Ä–∞—â–µ–Ω–∏–π MPC –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä –∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–∏–º trust region.
    """
    print("–ö—Ä–æ–∫ 2: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ MPC –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä–∞...")
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –ø—Ä–æ—Ü–µ—Å—É
    kernel_model = KernelModel(
        model_type=params['model_type'],
        kernel=params['kernel'],
        find_optimal_params=params['find_optimal_params']
    )
    
    # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —É—Å—Ç–∞–≤–æ–∫ —Ç–∞ –æ–±–º–µ–∂–µ–Ω—å
    ref_point_scaled = y_scaler.transform(np.array([[params['ref_fe'], params['ref_mass']]]))[0]
    y_max_scaled = y_scaler.transform(np.array([[params['y_max_fe'], params['y_max_mass']]]))[0]

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó
    objective = MaxIronMassTrackingObjective(
        Œª=params['Œª_obj'], w_fe=params['w_fe'], w_mass=params['w_mass'],
        ref_fe=ref_point_scaled[0], ref_mass=ref_point_scaled[1], K_I=params['K_I']
    )
    
    # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤–∞–≥ –¥–ª—è –º'—è–∫–∏—Ö –æ–±–º–µ–∂–µ–Ω—å
    avg_tracking_weight = (params['w_fe'] + params['w_mass']) / 2.
    rho_y_val = avg_tracking_weight * 1000
    rho_du_val = params['Œª_obj'] * 100

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä–∞ –∑ –Ω–æ–≤–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    mpc = MPCController(
        model=kernel_model, 
        objective=objective, 
        x_scaler=x_scaler, 
        y_scaler=y_scaler,
        n_targets=2, 
        horizon=params['Np'], 
        control_horizon=params['Nc'], 
        lag=params['lag'],
        u_min=params['u_min'], 
        u_max=params['u_max'], 
        delta_u_max=params['delta_u_max'],
        use_disturbance_estimator=params['use_disturbance_estimator'],
        y_max=list(y_max_scaled) if params['use_soft_constraints'] else None,
        rho_y=rho_y_val, 
        rho_delta_u=rho_du_val, 
        rho_trust=params['rho_trust'],
        # === –ù–û–í–Ü –ü–ê–†–ê–ú–ï–¢–†–ò ===
        adaptive_trust_region=params.get('adaptive_trust_region', True),
        initial_trust_radius=params.get('initial_trust_radius', 1.0),
        min_trust_radius=params.get('min_trust_radius', 0.1),
        max_trust_radius=params.get('max_trust_radius', 5.0),
        trust_decay_factor=params.get('trust_decay_factor', 0.8),
        linearization_check_enabled=params.get('linearization_check_enabled', True),
        max_linearization_distance=params.get('max_linearization_distance', 2.0)
    )
    return mpc

def run_simulation_loop_enhanced(
    true_gen: StatefulDataGenerator,
    mpc: MPCController,
    ekf: ExtendedKalmanFilter,
    df_true: pd.DataFrame,
    data: Dict[str, np.ndarray],
    scalers: Tuple[StandardScaler, StandardScaler],
    params: Dict[str, Any],
    progress_callback: Callable | None = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π —Ü–∏–∫–ª —Å–∏–º—É–ª—è—Ü—ñ—ó –∑ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥–æ–º trust region —Ç–∞ —è–∫–æ—Å—Ç—ñ –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó.
    """
    print("–ö—Ä–æ–∫ 5: –ó–∞–ø—É—Å–∫ –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ —Ü–∏–∫–ª—É —Å–∏–º—É–ª—è—Ü—ñ—ó...")
    x_scaler, y_scaler = scalers

    # –ü–æ—á–∞—Ç–∫–æ–≤–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è (—è–∫ —ñ —Ä–∞–Ω—ñ—à–µ)
    n_total = len(df_true) - params['lag'] - 1
    n_train = int(params['train_size'] * n_total)
    n_val   = int(params['val_size'] * n_total)
    test_idx_start = params['lag'] + 1 + n_train + n_val

    hist0_unscaled = df_true[['feed_fe_percent',
                              'ore_mass_flow',
                              'solid_feed_percent']].iloc[
        test_idx_start - (params['lag'] + 1): test_idx_start
    ].values

    mpc.reset_history(hist0_unscaled)
    true_gen.reset_state(hist0_unscaled)

    df_run = df_true.iloc[test_idx_start:]
    d_all  = df_run[['feed_fe_percent', 'ore_mass_flow']].values
    T_sim  = len(df_run) - (params['lag'] + 1)

    # –°–ª—É–∂–±–æ–≤—ñ –∑–º—ñ–Ω–Ω—ñ (—Ä–æ–∑—à–∏—Ä–µ–Ω—ñ)
    records = []
    y_true_hist, x_hat_hist, P_hist, innov_hist, R_hist = [], [], [], [], []
    u_seq_hist = []
    d_hat_hist = []
    
    # ‚úÖ –î–û–î–ê–Ñ–ú–û –ó–ú–Ü–ù–ù–Ü –î–õ–Ø –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ò EKF:
    y_true_seq = []     # –†–µ–∞–ª—å–Ω—ñ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è
    y_pred_seq = []     # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    x_est_seq = []      # –û—Ü—ñ–Ω–∫–∏ —Å—Ç–∞–Ω—É EKF
    innovation_seq = [] # –Ü–Ω–Ω–æ–≤–∞—Ü—ñ—ó EKF
    
    trust_region_stats_hist = []  # –ù–û–í–ò–ô
    linearization_quality_hist = []  # –ù–û–í–ò–ô
    u_prev = float(hist0_unscaled[-1, 2])

    # –§—ñ–ª—å—Ç—Ä–∏ —Ç–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∏ –∞–Ω–æ–º–∞–ª—ñ–π
    window_size = 4
    filt_feed = MovingAverageFilter(window_size)
    filt_ore  = MovingAverageFilter(window_size)

    retrain_cooldown_timer = 0

    # –ë—É—Ñ–µ—Ä–∏ —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
    if params['enable_retraining']:
        print(f"-> –î–∏–Ω–∞–º—ñ—á–Ω–µ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è –£–í–Ü–ú–ö–ù–ï–ù–û. "
              f"–í—ñ–∫–Ω–æ: {params['retrain_window_size']}, "
              f"–ü–µ—Ä—ñ–æ–¥ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏: {params['retrain_period']}")
        retraining_buffer   = deque(maxlen=params['retrain_window_size'])
        initial_train_data  = list(zip(data['X_train_scaled'],
                                       data['Y_train_scaled']))
        retraining_buffer.extend(initial_train_data)
        innovation_monitor  = deque(maxlen=params['retrain_period'])

    # ONLINE-–¥–µ—Ç–µ–∫—Ç–æ—Ä–∏ –∞–Ω–æ–º–∞–ª—ñ–π
    ad_config = params.get('anomaly_params', {})
    ad_feed_fe = SignalAnomalyDetector(**ad_config)
    ad_ore_flow = SignalAnomalyDetector(**ad_config)

    # –ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª —Å–∏–º—É–ª—è—Ü—ñ—ó –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º–∏
    for t in range(T_sim):
        if progress_callback:
            progress_callback(t, T_sim, f"–ö—Ä–æ–∫ —Å–∏–º—É–ª—è—Ü—ñ—ó {t + 1}/{T_sim}")

        # 1. –°–∏—Ä—ñ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è
        feed_fe_raw, ore_flow_raw = d_all[t, :]

        # 2. ONLINE-—Ñ—ñ–ª—å—Ç—Ä—É–≤–∞–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π
        feed_fe_filt_anom = ad_feed_fe.update(feed_fe_raw)
        ore_flow_filt_anom = ad_ore_flow.update(ore_flow_raw)

        # 3. –ì—Ä—É–±–µ –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è
        d_filt = np.array([filt_feed.update(feed_fe_filt_anom),
                           filt_ore.update(ore_flow_filt_anom)])

        # 4. EKF: –ø—Ä–æ–≥–Ω–æ–∑
        ekf.predict(u_prev, d_filt)

        # 5. –û–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó –≤ MPC
        x_est_phys_unscaled = ekf.x_hat[:ekf.n_phys].reshape(params['lag'] + 1, 3)
        mpc.reset_history(x_est_phys_unscaled)
        mpc.d_hat = ekf.x_hat[ekf.n_phys:]

        # –ë–µ—Ä–µ–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω —ñ –ø–µ—Ä–µ–¥–±–∞—á–∞—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π –≤–∏—Ö—ñ–¥
        current_state = x_est_phys_unscaled.flatten().reshape(1, -1)
        current_state_scaled = x_scaler.transform(current_state)
        y_pred_scaled = mpc.model.predict(current_state_scaled)[0]
        y_pred_unscaled = y_scaler.inverse_transform(y_pred_scaled.reshape(1, -1))[0]

        # 6. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è MPC –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º–∏
        d_seq = np.repeat(d_filt.reshape(1, -1), params['Np'], axis=0)
        u_seq = mpc.optimize(d_seq, u_prev)
        u_cur = u_prev if u_seq is None else float(u_seq[0])

        # –î–û–î–ê–ô –î–Ü–ê–ì–ù–û–°–¢–ò–ö–£ –¢–£–¢:
        if t % 10 == 0:  # –ö–æ–∂–Ω—ñ 10 –∫—Ä–æ–∫—ñ–≤
            diagnose_mpc_behavior(mpc, t, u_seq, u_prev, d_seq)
        
        u_cur = u_prev if u_seq is None else float(u_seq[0])

        # 7. –ö—Ä–æ–∫ ¬´—Ä–µ–∞–ª—å–Ω–æ–≥–æ¬ª –ø—Ä–æ—Ü–µ—Å—É
        y_full = true_gen.step(feed_fe_raw, ore_flow_raw, u_cur)

        # 8. EKF: –∫–æ—Ä–µ–∫—Ü—ñ—è
        y_meas_unscaled = y_full[['concentrate_fe_percent',
                                  'concentrate_mass_flow']].values.flatten()
        ekf.update(y_meas_unscaled)

        # ‚úÖ –ó–ë–ò–†–ê–Ñ–ú–û –î–ê–ù–Ü –î–õ–Ø –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ò EKF:
        y_true_seq.append(y_meas_unscaled.copy())
        
        # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ (–∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–µ—Ä–µ–¥ update)
        # if hasattr(ekf, 'y_pred') and ekf.y_pred is not None:
        #     y_pred_seq.append(ekf.y_pred.copy())
        # else:
        #     y_pred_seq.append(np.zeros(2))
        y_pred_seq.append(y_pred_unscaled.copy())
        
        # –û—Ü—ñ–Ω–∫–∞ —Å—Ç–∞–Ω—É –ø—ñ—Å–ª—è update
        x_est_seq.append(ekf.x_hat.copy())
        
        # –Ü–Ω–Ω–æ–≤–∞—Ü—ñ—ó
        if hasattr(ekf, 'last_innovation') and ekf.last_innovation is not None:
            innovation_seq.append(ekf.last_innovation.copy())
        else:
            innovation_seq.append(np.zeros(2))

        # 9. –ó–º–µ–Ω—à—É—î–º–æ cooldown-—Ç–∞–π–º–µ—Ä
        if retrain_cooldown_timer > 0:
            retrain_cooldown_timer -= 1

        # === –ù–û–í–ê –õ–û–ì–Ü–ö–ê: –ó–ë–Ü–† –°–¢–ê–¢–ò–°–¢–ò–ö–ò TRUST REGION ===
        if hasattr(mpc, 'get_trust_region_stats'):
            trust_stats = mpc.get_trust_region_stats()
            trust_region_stats_hist.append(trust_stats)
            
            # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —è–∫—ñ—Å—Ç—å –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó –ø—Ä–∞–≤–∏–ª—å–Ω–æ
            if hasattr(mpc, 'linearization_quality_history') and mpc.linearization_quality_history:
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑ —ñ—Å—Ç–æ—Ä—ñ—ó
                if isinstance(mpc.linearization_quality_history[-1], dict):
                    linearization_quality_hist.append(mpc.linearization_quality_history[-1]['euclidean_distance'])
                else:
                    linearization_quality_hist.append(mpc.linearization_quality_history[-1])

        # 10. –ë—É—Ñ–µ—Ä–∏–∑–∞—Ü—ñ—è —Ç–∞ –º–æ–∂–ª–∏–≤–µ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
        if params['enable_retraining']:
            new_x_unscaled = mpc.x_hist.flatten().reshape(1, -1)
            new_y_unscaled = y_meas_unscaled.reshape(1, -1)

            new_x_scaled = x_scaler.transform(new_x_unscaled)
            new_y_scaled = y_scaler.transform(new_y_unscaled)

            retraining_buffer.append((new_x_scaled[0], new_y_scaled[0]))

            if ekf.last_innovation is not None:
                innov_norm = np.linalg.norm(ekf.last_innovation)
                innovation_monitor.append(innov_norm)

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
            if (t > 0 and
                t % params['retrain_period'] == 0 and
                len(innovation_monitor) == params['retrain_period'] and
                retrain_cooldown_timer == 0):

                avg_innov = float(np.mean(innovation_monitor))

                # === –ü–û–ö–†–ê–©–ï–ù–ê –õ–û–ì–Ü–ö–ê –ü–ï–†–ï–ù–ê–í–ß–ê–ù–ù–Ø ===
                # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: —è–∫—ñ—Å—Ç—å –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó
                should_retrain = avg_innov > params['retrain_innov_threshold']
                
                if (hasattr(mpc, 'linearization_quality_history') and 
                    len(mpc.linearization_quality_history) > 10):
                    
                    # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏—Ç—è–≥—É—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –≤—ñ–¥—Å—Ç–∞–Ω—ñ
                    if isinstance(mpc.linearization_quality_history[-1], dict):
                        recent_distances = [h['euclidean_distance'] for h in mpc.linearization_quality_history[-10:]]
                    else:
                        recent_distances = mpc.linearization_quality_history[-10:]
                    
                    recent_lin_quality = np.mean(recent_distances)
                    lin_threshold = params.get('retrain_linearization_threshold', 1.5)
                    
                    if recent_lin_quality > lin_threshold:
                        print(f"  -> –î–æ–¥–∞—Ç–∫–æ–≤–∏–π —Ç—Ä–∏–≥–µ—Ä: –ø–æ–≥–∞–Ω–∞ —è–∫—ñ—Å—Ç—å –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó ({recent_lin_quality:.3f} > {lin_threshold})")
                        should_retrain = True

                if should_retrain:
                    print(f"\n---> –¢–†–ò–ì–ï–† –ü–ï–†–ï–ù–ê–í–ß–ê–ù–ù–Ø –Ω–∞ –∫—Ä–æ—Ü—ñ {t}! "
                          f"–°–µ—Ä–µ–¥–Ω—è —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—è: {avg_innov:.4f} > "
                          f"{params['retrain_innov_threshold']:.4f}")

                    retrain_data = list(retraining_buffer)
                    X_retrain = np.array([p[0] for p in retrain_data])
                    Y_retrain = np.array([p[1] for p in retrain_data])

                    print(f"--> mpc.fit() –Ω–∞ {len(X_retrain)} —Å–µ–º–ø–ª–∞—Ö ...")
                    mpc.fit(X_retrain, Y_retrain)
                    print("--> –ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
                    
                    # –°–∫–∏–¥–∞—î–º–æ trust region –ø—ñ—Å–ª—è –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
                    if hasattr(mpc, 'reset_trust_region'):
                        mpc.reset_trust_region()
                        print("--> Trust region —Å–∫–∏–Ω—É—Ç–æ.\n")

                    innovation_monitor.clear()
                    retrain_cooldown_timer = params['retrain_period'] * 2

        # 11. –õ–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó / –º–µ—Ç—Ä–∏–∫
        y_true_hist.append(y_meas_unscaled)
        x_hat_hist.append(ekf.x_hat.copy())
        P_hist.append(ekf.P.copy())
        R_hist.append(ekf.R.copy())
        innov_hist.append(
            ekf.last_innovation.copy()
            if ekf.last_innovation is not None
            else np.zeros(ekf.n_dist)
        )

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–ª–∞–Ω—ñ–≤ MPC —Ç–∞ –æ—Ü—ñ–Ω–æ–∫ –∑–±—É—Ä–µ–Ω—å
        if u_seq is not None:
            u_seq_hist.append(u_seq)
        if mpc.d_hat is not None:
            d_hat_orig = y_scaler.inverse_transform(mpc.d_hat.reshape(1, -1))[0]
            d_hat_hist.append(d_hat_orig)

        y_meas = y_full.iloc[0]
        records.append({
            'feed_fe_percent':      y_meas.feed_fe_percent,
            'ore_mass_flow':        y_meas.ore_mass_flow,
            'solid_feed_percent':   u_cur,
            'conc_fe':              y_meas.concentrate_fe_percent,
            'tail_fe':              y_meas.tailings_fe_percent,
            'conc_mass':            y_meas.concentrate_mass_flow,
            'tail_mass':            y_meas.tailings_mass_flow,
            'mass_pull_pct':        y_meas.mass_pull_percent,
            'fe_recovery_percent':  y_meas.fe_recovery_percent,
        })

        u_prev = u_cur

    if progress_callback:
        progress_callback(T_sim, T_sim, "–°–∏–º—É–ª—è—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    # ‚úÖ –î–û–î–ê–Ñ–ú–û –î–Ü–ê–ì–ù–û–°–¢–ò–ö–£ EKF:
    diagnose_ekf_detailed(ekf, y_true_seq, y_pred_seq, x_est_seq, innovation_seq)
        
    # –†–æ–∑—à–∏—Ä–µ–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
    analysis_data = {
        "y_true": np.vstack(y_true_hist),
        "x_hat": np.vstack(x_hat_hist),
        "P": np.stack(P_hist),
        "innov": np.vstack(innov_hist),
        "R": np.stack(R_hist),
        "u_seq": u_seq_hist,
        "d_hat": np.vstack(d_hat_hist) if d_hat_hist else np.array([]),
        "trust_region_stats": trust_region_stats_hist,
        "linearization_quality": linearization_quality_hist,
        # ‚úÖ –î–û–î–ê–Ñ–ú–û –î–ê–ù–Ü –î–õ–Ø –ü–û–î–ê–õ–¨–®–û–ì–û –ê–ù–ê–õ–Ü–ó–£:
        "y_true_seq": y_true_seq,
        "y_pred_seq": y_pred_seq,
        "x_est_seq": x_est_seq,
        "innovation_seq": innovation_seq,
    }

    return pd.DataFrame(records), analysis_data

def initialize_ekf(
    mpc: MPCController,
    scalers: Tuple[StandardScaler, StandardScaler],
    hist0_unscaled: np.ndarray,
    Y_train_scaled: np.ndarray,
    lag: int,
    params: Dict[str, Any]
) -> ExtendedKalmanFilter:
    """
    –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–π —Ñ—ñ–ª—å—Ç—Ä –ö–∞–ª–º–∞–Ω–∞ (EKF).
    """
    print("–ö—Ä–æ–∫ 4: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ñ—ñ–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞ (EKF)...")
       
    x_scaler, y_scaler = scalers
    n_phys, n_dist = (lag + 1) * 3, 2
    
    # x0_aug = np.hstack([hist0_unscaled.flatten(), np.zeros(n_dist)])
    # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –†–æ–∑—É–º–Ω–∞ –ø–æ—á–∞—Ç–∫–æ–≤–∞ –æ—Ü—ñ–Ω–∫–∞ –∑–±—É—Ä–µ–Ω—å
    # –ë–∞–∑—É—é—á–∏—Å—å –Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω—ñ–π –ø–æ–º–∏–ª—Ü—ñ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    initial_disturbances = np.array([0.7, 0.0])  # –ë–ª–∏–∑—å–∫–æ –¥–æ Innovation mean: [0.71, 0.04]
    
    x0_aug = np.hstack([hist0_unscaled.flatten(), initial_disturbances])
    
    # P0 = np.eye(n_phys + n_dist) * params['P0']
    # P0[n_phys:, n_phys:] *= 1 
    P0 = np.eye(n_phys + n_dist) * params['P0'] * 1.5  # –ë—É–ª–æ: * 1.0
    P0[n_phys:, n_phys:] *= 10  # –ó–∞–ª–∏—à–∏—Ç–∏ —è–∫ —î

    Q_phys = np.eye(n_phys) * params['Q_phys']
    Q_dist = np.eye(n_dist) * params['Q_dist'] 
    Q = np.block([[Q_phys, np.zeros((n_phys, n_dist))], [np.zeros((n_dist, n_phys)), Q_dist]])
    
    # R = np.diag(np.var(Y_train_scaled, axis=0)) * params['R']
    R = np.diag(np.var(Y_train_scaled, axis=0)) * params['R'] * 0.5
    
    return ExtendedKalmanFilter(
        mpc.model, x_scaler, y_scaler, x0_aug, P0, Q, R, lag,
        beta_R=params.get('beta_R', 0.1), # .get –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
        q_adaptive_enabled=params.get('q_adaptive_enabled', True),
        q_alpha=params.get('q_alpha', 0.995),
        q_nis_threshold=params.get('q_nis_threshold', 1.8)        
    )

# =============================================================================
# === –ë–õ–û–ö 3: –û–°–ù–û–í–ù–ò–ô –¶–ò–ö–õ –°–ò–ú–£–õ–Ø–¶–Ü–á ===
# =============================================================================


    
# =============================================================================
# === –ì–û–õ–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø-–û–†–ö–ï–°–¢–†–ê–¢–û–† ===
# =============================================================================


def simulate_mpc(
    reference_df: pd.DataFrame,             # DataFrame, —â–æ –º—ñ—Å—Ç–∏—Ç—å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö —Å–∏–º—É–ª—è—Ü—ñ—ó.
    N_data: int = 5000,                     # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ –¥–∞–Ω–∏—Ö, —â–æ –≥–µ–Ω–µ—Ä—É—é—Ç—å—Å—è –¥–ª—è —Å–∏–º—É–ª—è—Ü—ñ—ó.
    control_pts : int = 1000,               # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ (–∫—Ä–æ–∫—ñ–≤) —Å–∏–º—É–ª—è—Ü—ñ—ó, –Ω–∞ —è–∫–∏—Ö –≤—ñ–¥–±—É–≤–∞—î—Ç—å—Å—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è MPC.
    time_step_s : int = 5,                  # –ß–∞—Å–æ–≤–∏–π –∫—Ä–æ–∫ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è
    dead_times_s : dict = 
    {
        'concentrate_fe_percent': 20.0,
        'tailings_fe_percent': 25.0,
        'concentrate_mass_flow': 20.0,
        'tailings_mass_flow': 25.0
    },                                      # –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∞ –∑–∞—Ç—Ä–∏–º–∫–∞ –≤–∏—Ö—ñ–¥–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
    time_constants_s : dict = 
    {
        'concentrate_fe_percent': 8.0,
        'tailings_fe_percent': 10.0,
        'concentrate_mass_flow': 5.0,
        'tailings_mass_flow': 7.0
    },                                      # –Ü–Ω–µ—Ä—Ü—ñ–π–Ω—ñ—Å—Ç—å –≤–∏—Ö—ñ–¥–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
    lag: int = 2,                           # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤ –∑–∞—Ç—Ä–∏–º–∫–∏ (lag) –¥–ª—è –º–æ–¥–µ–ª—ñ, –≤–ø–ª–∏–≤–∞—î –Ω–∞ —Ä–æ–∑–º—ñ—Ä –≤–µ–∫—Ç–æ—Ä–∞ —Å—Ç–∞–Ω—É.
    Np: int = 6,                            # –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è (Prediction Horizon) MPC. –ö—ñ–ª—å–∫—ñ—Å—Ç—å –º–∞–π–±—É—Ç–Ω—ñ—Ö –∫—Ä–æ–∫—ñ–≤, —è–∫—ñ –º–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑—É—î.
    Nc: int = 4,                            # –ì–æ—Ä–∏–∑–æ–Ω—Ç –∫–µ—Ä—É–≤–∞–Ω–Ω—è (Control Horizon) MPC. –ö—ñ–ª—å–∫—ñ—Å—Ç—å –º–∞–π–±—É—Ç–Ω—ñ—Ö –∑–º—ñ–Ω –∫–µ—Ä—É–≤–∞–Ω–Ω—è, —è–∫—ñ MPC —Ä–æ–∑—Ä–∞—Ö–æ–≤—É—î.
    n_neighbors: int = 5,                   # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—É—Å—ñ–¥—ñ–≤ –¥–ª—è KNN —Ä–µ–≥—Ä–µ—Å–æ—Ä–∞, —è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è (–Ω–∞—Ä–∞–∑—ñ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ `KernelModel`).
    seed: int = 0,                          # –ó–µ—Ä–Ω–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö —á–∏—Å–µ–ª, –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ —Å–∏–º—É–ª—è—Ü—ñ—ó.
    noise_level: str = 'none',              # –†—ñ–≤–µ–Ω—å —à—É–º—É, —è–∫–∏–π –¥–æ–¥–∞—î—Ç—å—Å—è –¥–æ –≤–∏–º—ñ—Ä—é–≤–∞–Ω—å 'none', 'low', 'medium', 'high'. –í–∏–∑–Ω–∞—á–∞—î –≤—ñ–¥—Å–æ—Ç–æ–∫ –ø–æ—Ö–∏–±–∫–∏.
    model_type: str = 'krr',                # –¢–∏–ø –º–æ–¥–µ–ª—ñ, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤ MPC: 'krr' (Kernel Ridge Regression), 'gpr' (Gaussian Process Regressor), 'svr' (Support-Vector Regression).
    kernel: str = 'rbf',                    # –¢–∏–ø —è–¥—Ä–∞ –¥–ª—è KernelModel ('linear', 'poly', 'rbf').
    find_optimal_params: bool = True,       # –ß–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ —à—É–∫–∞—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é RandomizedSearchCV.
    Œª_obj: float = 0.1,                     # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –≤–∞–≥–∏ –¥–ª—è —Ç–µ—Ä–º—É –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è –∫–µ—Ä—É–≤–∞–Ω–Ω—è (lambda) –≤ —Ü—ñ–ª—å–æ–≤—ñ–π —Ñ—É–Ω–∫—Ü—ñ—ó MPC.
    K_I: float = 0.01,                      # –Ü–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–∏–π –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –¥–ª—è —ñ–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä–∞ (—è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è). –ù–∞—Ä–∞–∑—ñ –Ω–µ –∑–∞—Å—Ç–æ—Å–æ–≤—É—î—Ç—å—Å—è —è–≤–Ω–æ –≤ MPC.
    w_fe: float = 7.0,                      # –í–∞–≥–∞ –¥–ª—è –ø–æ–º–∏–ª–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—ó –∑–∞–ª—ñ–∑–∞ (Fe) –≤ —Ü—ñ–ª—å–æ–≤—ñ–π —Ñ—É–Ω–∫—Ü—ñ—ó MPC.
    w_mass: float = 1.0,                    # –í–∞–≥–∞ –¥–ª—è –ø–æ–º–∏–ª–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –º–∞—Å–æ–≤–æ—ó –≤–∏—Ç—Ä–∞—Ç–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ç—É –≤ —Ü—ñ–ª—å–æ–≤—ñ–π —Ñ—É–Ω–∫—Ü—ñ—ó MPC.
    ref_fe: float = 53.5,                   # –ë–∞–∂–∞–Ω–µ (—Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–µ) –∑–Ω–∞—á–µ–Ω–Ω—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—ó –∑–∞–ª—ñ–∑–∞ (Fe) –≤ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ç—ñ.
    ref_mass: float = 57.0,                 # –ë–∞–∂–∞–Ω–µ (—Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–µ) –∑–Ω–∞—á–µ–Ω–Ω—è –º–∞—Å–æ–≤–æ—ó –≤–∏—Ç—Ä–∞—Ç–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ç—É.
    train_size: float = 0.7,                # –ß–∞—Å—Ç–∫–∞ –¥–∞–Ω–∏—Ö, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ MPC.
    val_size: float = 0.15,                 # –ß–∞—Å—Ç–∫–∞ –¥–∞–Ω–∏—Ö, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –º–æ–¥–µ–ª—ñ (—è–∫—â–æ `find_optimal_params=True`).
    test_size: float = 0.15,                # –ß–∞—Å—Ç–∫–∞ –¥–∞–Ω–∏—Ö, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ (–∑–∞–∑–≤–∏—á–∞–π –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ –≤ —Ü–∏–∫–ª—ñ MPC).
    u_min: float = 20.0,                    # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –¥–æ–ø—É—Å—Ç–∏–º–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –∫–µ—Ä—É—é—á–æ—ó –∑–º—ñ–Ω–Ω–æ—ó `u` (ore_flow_rate_target).
    u_max: float = 40.0,                    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –¥–æ–ø—É—Å—Ç–∏–º–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è –∫–µ—Ä—É—é—á–æ—ó –∑–º—ñ–Ω–Ω–æ—ó `u` (ore_flow_rate_target).
    delta_u_max: float = 1.0,               # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –¥–æ–ø—É—Å—Ç–∏–º–µ –∞–±—Å–æ–ª—é—Ç–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è –∑–º—ñ–Ω–∏ –∫–µ—Ä—É—é—á–æ—ó –∑–º—ñ–Ω–Ω–æ—ó `u` –º—ñ–∂ –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–∏–º–∏ –∫—Ä–æ–∫–∞–º–∏.
    use_disturbance_estimator: bool = True, # –ß–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –æ—Ü—ñ–Ω—é–≤–∞—á –∑–±—É—Ä–µ–Ω—å (Extended Kalman Filter) –≤ —Ü–∏–∫–ª—ñ MPC.
    y_max_fe: float = 54.5,                 # –í–µ—Ä—Ö–Ω—è –º–µ–∂–∞ –¥–ª—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—ó –∑–∞–ª—ñ–∑–∞ (Fe) –≤ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ç—ñ (–∂–æ—Ä—Å—Ç–∫–µ –∞–±–æ –º'—è–∫–µ –æ–±–º–µ–∂–µ–Ω–Ω—è).
    y_max_mass: float = 58.0,               # –í–µ—Ä—Ö–Ω—è –º–µ–∂–∞ –¥–ª—è –º–∞—Å–æ–≤–æ—ó –≤–∏—Ç—Ä–∞—Ç–∏ –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ç—É (–∂–æ—Ä—Å—Ç–∫–µ –∞–±–æ –º'—è–∫–µ –æ–±–º–µ–∂–µ–Ω–Ω—è).
    rho_trust: float = 0.1,                 # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç —à—Ç—Ä–∞—Ñ—É (rho) –¥–ª—è —Ç–µ—Ä–º—É –¥–æ–≤—ñ—Ä–∏ –≤ —Ü—ñ–ª—å–æ–≤—ñ–π —Ñ—É–Ω–∫—Ü—ñ—ó MPC, —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó.
    max_trust_radius: float = 5.0,
    adaptive_trust_region: bool = True,
    initial_trust_radius: float =  1.0,
    min_trust_radius: float =  0.5,
    trust_decay_factor: float =  0.8,
    linearization_check_enabled: bool = True,
    max_linearization_distance: float =  2.0,
    retrain_linearization_threshold: float =  1.5,
    use_soft_constraints: bool = True,      # –ß–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –º'—è–∫—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è –¥–ª—è –≤–∏—Ö–æ–¥—ñ–≤ (y) —Ç–∞ –∑–º—ñ–Ω–∏ –∫–µ—Ä—É–≤–∞–Ω–Ω—è (delta_u).
    plant_model_type: str = 'rf',           # –¢–∏–ø –º–æ–¥–µ–ª—ñ, —â–æ —ñ–º—ñ—Ç—É—î "—Ä–µ–∞–ª—å–Ω–∏–π –æ–±'—î–∫—Ç" (plant) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö: 'rf' (Random Forest) –∞–±–æ 'nn' (Neural Network).
    enable_retraining: bool = True,         # –í–≤—ñ–º–∫–Ω—É—Ç–∏/–≤–∏–º–∫–Ω—É—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ MPC –ø—ñ–¥ —á–∞—Å —Å–∏–º—É–ª—è—Ü—ñ—ó.
    retrain_period: int = 50,               # –Ø–∫ —á–∞—Å—Ç–æ –ø–µ—Ä–µ–≤—ñ—Ä—è—Ç–∏ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ—Å—Ç—å –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è (–∫–æ–∂–Ω—ñ N –∫—Ä–æ–∫—ñ–≤).
    retrain_window_size: int = 1000,        # –†–æ–∑–º—ñ—Ä –±—É—Ñ–µ—Ä–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –æ—Å—Ç–∞–Ω–Ω—ñ `retrain_window_size` —Ç–æ—á–æ–∫).
    retrain_innov_threshold: float = 0.3,   # –ü–æ—Ä—ñ–≥ –¥–ª—è —Å–µ—Ä–µ–¥–Ω—å–æ—ó –Ω–æ—Ä–º–æ–≤–∞–Ω–æ—ó —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—ó EKF. –Ø–∫—â–æ NIS –ø–µ—Ä–µ–≤–∏—â—É—î —Ü–µ–π –ø–æ—Ä—ñ–≥, —ñ–Ω—ñ—Ü—ñ—é—î—Ç—å—Å—è –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è.
    anomaly_params: dict = {
        'window': 25,
        'spike_z': 4.0,
        'drop_rel': 0.30,
        'freeze_len': 5,
        'enabled': True
    },                                      # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –∞–Ω–æ–º–∞–ª—ñ–π
    nonlinear_config: dict = {
        'concentrate_fe_percent': ('pow', 2),
        'concentrate_mass_flow': ('pow', 1.5)
    },                                      # –ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É–Ω–∞—Ü—ñ—è
    enable_nonlinear: bool =  False,        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
    run_analysis: bool = True,              # –ü–æ–∫–∞–∑–∞—Ç–∏ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —Ä–æ–±–æ—Ç–∏ —Å–∏–º—É–ª—è—Ç–æ—Ä–∞
    P0: float = 1e-2,
    Q_phys: float = 1500,
    Q_dist: float = 1,
    R: float = 0.01,
    q_adaptive_enabled: bool = True,
    q_alpha:float = 0.99,
    q_nis_threshold:float = 1.5,
    progress_callback: Callable[[int, int, str], None] = None # –§—É–Ω–∫—Ü—ñ—è –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –≤–∏–∫–ª–∏–∫—É –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É —Å–∏–º—É–ª—è—Ü—ñ—ó. –ü—Ä–∏–π–º–∞—î –ø–æ—Ç–æ—á–Ω–∏–π –∫—Ä–æ–∫, –∑–∞–≥–∞–ª—å–Ω—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤ —Ç–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.
):
    """
    –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è –≥–æ–ª–æ–≤–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó-–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞.
    """
    # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤ –æ–¥–∏–Ω —Å–ª–æ–≤–Ω–∏–∫
    params = locals()
    # params.update(kwargs)  # –î–æ–¥–∞—î–º–æ –±—É–¥—å-—è–∫—ñ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    
    # 1. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö (–±–µ–∑ –∑–º—ñ–Ω)
    true_gen, df_true, X, Y = prepare_simulation_data(reference_df, params)
    data, x_scaler, y_scaler = split_and_scale_data(X, Y, params)

    # 2. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ MPC
    mpc = initialize_mpc_controller_enhanced(params, x_scaler, y_scaler)
    metrics = train_and_evaluate_model(mpc, data, y_scaler)
    
    # 3. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è EKF (–±–µ–∑ –∑–º—ñ–Ω)
    n_train_pts = len(data['X_train'])
    n_val_pts = len(data['X_val'])
    test_idx_start = params['lag'] + 1 + n_train_pts + n_val_pts
    hist0_unscaled = df_true[['feed_fe_percent', 'ore_mass_flow', 'solid_feed_percent']].iloc[
        test_idx_start - (params['lag'] + 1): test_idx_start
    ].values
    
    ekf = initialize_ekf(mpc, (x_scaler, y_scaler), hist0_unscaled, data['Y_train_scaled'], params['lag'], params)

    # 4. –ó–∞–ø—É—Å–∫ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó —Å–∏–º—É–ª—è—Ü—ñ—ó
    results_df, analysis_data = run_simulation_loop_enhanced(
        true_gen, mpc, ekf, df_true, data, (x_scaler, y_scaler), params, 
        params.get('progress_callback')
    )
    
    # 5. –†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    test_idx_start = params['lag'] + 1 + len(data['X_train']) + len(data['X_val'])
    analysis_data['d_all_test'] = df_true.iloc[test_idx_start:][['feed_fe_percent','ore_mass_flow']].values
    
    if params.get('run_analysis', True):
        run_post_simulation_analysis_enhanced(results_df, analysis_data, params)
    
    return results_df, metrics

# ---- –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç–æ–¥–∏ –¥–ª—è sim.py –∑ —Å–∏—Å—Ç–µ–º–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π

# –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç–æ–¥–∏ –¥–ª—è sim.py –∑ —Å–∏—Å—Ç–µ–º–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π

# –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç–æ–¥–∏ –¥–ª—è sim.py –∑ —Å–∏—Å—Ç–µ–º–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π

import json
import os
from pathlib import Path
from typing import Dict, Any, Optional

def load_mpc_config(config_name: str) -> Dict[str, Any]:
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é MPC –∑ —Ñ–∞–π–ª—É.
    
    Args:
        config_name: –ù–∞–∑–≤–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó (–±–µ–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è .json)
        
    Returns:
        –°–ª–æ–≤–Ω–∏–∫ –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
    """
    config_dir = Path("mpc_configs")
    config_file = config_dir / f"{config_name}.json"
    
    if not config_file.exists():
        raise FileNotFoundError(f"–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è '{config_name}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –≤ {config_dir}")
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é: {config_name}")
        return config
    except json.JSONDecodeError as e:
        raise ValueError(f"–ü–æ–º–∏–ª–∫–∞ —É —Ñ–æ—Ä–º–∞—Ç—ñ JSON –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó '{config_name}': {e}")

def save_mpc_config(config: Dict[str, Any], config_name: str) -> None:
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é MPC —É —Ñ–∞–π–ª.
    
    Args:
        config: –°–ª–æ–≤–Ω–∏–∫ –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
        config_name: –ù–∞–∑–≤–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó (–±–µ–∑ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è .json)
    """
    config_dir = Path("mpc_configs")
    config_dir.mkdir(exist_ok=True)
    
    config_file = config_dir / f"{config_name}.json"
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=4, ensure_ascii=False)
    
    print(f"üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é: {config_name}")

def list_available_configs() -> list:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π.
    
    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π
    """
    config_dir = Path("mpc_configs")
    if not config_dir.exists():
        return []
    
    configs = []
    for config_file in config_dir.glob("*.json"):
        configs.append(config_file.stem)
    
    return sorted(configs)

def create_default_configs():
    """
    –°—Ç–≤–æ—Ä—é—î —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó MPC –∑ –≤–∞–ª—ñ–¥–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
    """
    config_dir = Path("mpc_configs")
    config_dir.mkdir(exist_ok=True)
    
    # –û—Ç—Ä–∏–º—É—î–º–æ –≤–∞–ª—ñ–¥–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    valid_params = get_valid_simulate_mpc_params()
    valid_params.discard('reference_df')  # –í–∏–∫–ª—é—á–∞—î–º–æ reference_df
    
    # –ë–∞–∑–æ–≤–∞ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    conservative_config = {
        "name": "conservative",
        "description": "–ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—ó —Ä–æ–±–æ—Ç–∏",
        
        # –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        "N_data": 2000,
        "control_pts": 200,
        "seed": 42,
        
        # –ú–æ–¥–µ–ª—å
        "model_type": "krr",
        "kernel": "rbf",
        "find_optimal_params": True,
        
        # MPC –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        "Np": 6,
        "Nc": 4,
        "lag": 2,
        "Œª_obj": 0.2,
        
        # –í–∞–≥–∏ —Ç–∞ —É—Å—Ç–∞–≤–∫–∏
        "w_fe": 5.0,
        "w_mass": 1.0,
        "ref_fe": 53.5,
        "ref_mass": 57.0,
        
        # Trust region
        "adaptive_trust_region": True,
        "initial_trust_radius": 1.0,
        "min_trust_radius": 0.5,
        "max_trust_radius": 3.0,
        "trust_decay_factor": 0.9,
        "rho_trust": 0.3,
        
        # EKF –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        "P0": 0.01,
        "Q_phys": 800,
        "Q_dist": 1,
        "R": 0.5,
        "q_adaptive_enabled": True,
        "q_alpha": 0.95,
        "q_nis_threshold": 2.0,
        
        # –ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
        "enable_retraining": True,
        "retrain_period": 50,
        "retrain_innov_threshold": 0.3,
        "retrain_window_size": 1000,
        
        # –û–±–º–µ–∂–µ–Ω–Ω—è
        "use_soft_constraints": True,
        "delta_u_max": 0.8,
        "u_min": 20.0,
        "u_max": 40.0,
        
        # –ü—Ä–æ—Ü–µ—Å
        "plant_model_type": "rf",
        "noise_level": "low",
        "enable_nonlinear": False,
        
        # –ê–Ω–æ–º–∞–ª—ñ—ó
        "anomaly_params": {
            "window": 25,
            "spike_z": 4.0,
            "drop_rel": 0.30,
            "freeze_len": 5,
            "enabled": True
        },
        
        "run_analysis": True
    }
    
    # –ê–≥—Ä–µ—Å–∏–≤–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    aggressive_config = {
        "name": "aggressive",
        "description": "–ê–≥—Ä–µ—Å–∏–≤–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –≤—ñ–¥–≥—É–∫—É",
        
        # –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        "N_data": 3000,
        "control_pts": 300,
        "seed": 42,
        
        # –ú–æ–¥–µ–ª—å
        "model_type": "svr",
        "kernel": "rbf", 
        "find_optimal_params": True,
        
        # MPC –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        "Np": 8,
        "Nc": 6,
        "lag": 2,
        "Œª_obj": 0.05,
        
        # –í–∞–≥–∏ —Ç–∞ —É—Å—Ç–∞–≤–∫–∏
        "w_fe": 10.0,
        "w_mass": 2.0,
        "ref_fe": 54.0,
        "ref_mass": 58.0,
        
        # Trust region
        "adaptive_trust_region": True,
        "initial_trust_radius": 2.0,
        "min_trust_radius": 0.8,
        "max_trust_radius": 5.0,
        "trust_decay_factor": 0.8,
        "rho_trust": 0.1,
        
        # EKF –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        "P0": 0.01,
        "Q_phys": 1200,
        "Q_dist": 1,
        "R": 0.1,
        "q_adaptive_enabled": True,
        "q_alpha": 0.90,
        "q_nis_threshold": 3.0,
        
        # –ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
        "enable_retraining": True,
        "retrain_period": 30,
        "retrain_innov_threshold": 0.2,
        "retrain_window_size": 800,
        
        # –û–±–º–µ–∂–µ–Ω–Ω—è
        "use_soft_constraints": True,
        "delta_u_max": 1.2,
        "u_min": 18.0,
        "u_max": 42.0,
        
        # –ü—Ä–æ—Ü–µ—Å
        "plant_model_type": "rf",
        "noise_level": "medium",
        "enable_nonlinear": True,
        
        # –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å
        "nonlinear_config": {
            "concentrate_fe_percent": ("pow", 2),
            "concentrate_mass_flow": ("pow", 1.5)
        },
        
        # –ê–Ω–æ–º–∞–ª—ñ—ó
        "anomaly_params": {
            "window": 20,
            "spike_z": 3.5,
            "drop_rel": 0.25,
            "freeze_len": 3,
            "enabled": True
        },
        
        "run_analysis": True
    }
    
    # –®–≤–∏–¥–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    fast_test_config = {
        "name": "fast_test",
        "description": "–®–≤–∏–¥–∫–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ç–∞ –≤—ñ–¥–ª–∞–¥–∫–∏",
        
        # –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        "N_data": 1000,
        "control_pts": 100,
        "seed": 42,
        
        # –ú–æ–¥–µ–ª—å
        "model_type": "linear",
        "find_optimal_params": False,
        
        # MPC –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        "Np": 4,
        "Nc": 3,
        "lag": 1,
        "Œª_obj": 0.1,
        
        # –í–∞–≥–∏ —Ç–∞ —É—Å—Ç–∞–≤–∫–∏
        "w_fe": 7.0,
        "w_mass": 1.0,
        "ref_fe": 53.5,
        "ref_mass": 57.0,
        
        # Trust region
        "adaptive_trust_region": False,
        "rho_trust": 0.2,
        
        # EKF –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        "P0": 0.01,
        "Q_phys": 600,
        "Q_dist": 1,
        "R": 1.0,
        "q_adaptive_enabled": False,
        
        # –ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
        "enable_retraining": False,
        
        # –û–±–º–µ–∂–µ–Ω–Ω—è
        "use_soft_constraints": False,
        "delta_u_max": 1.0,
        
        # –ü—Ä–æ—Ü–µ—Å
        "plant_model_type": "rf",
        "noise_level": "none",
        "enable_nonlinear": False,
        
        # –ê–Ω–æ–º–∞–ª—ñ—ó
        "anomaly_params": {
            "enabled": False
        },
        
        "run_analysis": False
    }
    
    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ
    configs = [conservative_config, aggressive_config, fast_test_config]
    
    for config in configs:
        config_name = config["name"]
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω–µ–≤–∞–ª—ñ–¥–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        filtered_config = {}
        invalid_params = []
        
        for key, value in config.items():
            if key in valid_params or key in ["name", "description"]:
                filtered_config[key] = value
            else:
                invalid_params.append(key)
        
        if invalid_params:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –Ω–µ–≤–∞–ª—ñ–¥–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤ {config_name}: {', '.join(invalid_params)}")
        
        save_mpc_config(filtered_config, config_name)
    
    print(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ {len(configs)} —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏—Ö –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π")

def get_valid_simulate_mpc_params() -> set:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î –º–Ω–æ–∂–∏–Ω—É –≤–∞–ª—ñ–¥–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –¥–ª—è —Ñ—É–Ω–∫—Ü—ñ—ó simulate_mpc.
    
    Returns:
        –ú–Ω–æ–∂–∏–Ω–∞ –Ω–∞–∑–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
    """
    import inspect
    
    # –û—Ç—Ä–∏–º—É—î–º–æ –ø—ñ–¥–ø–∏—Å —Ñ—É–Ω–∫—Ü—ñ—ó simulate_mpc
    sig = inspect.signature(simulate_mpc)
    return set(sig.parameters.keys())

def filter_config_for_simulate_mpc(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    –§—ñ–ª—å—Ç—Ä—É—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é, –∑–∞–ª–∏—à–∞—é—á–∏ —Ç—ñ–ª—å–∫–∏ –≤–∞–ª—ñ–¥–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è simulate_mpc.
    
    Args:
        config: –ü–æ–≤–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
        
    Returns:
        –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    """
    valid_params = get_valid_simulate_mpc_params()
    
    # –í–∏–∫–ª—é—á–∞—î–º–æ 'reference_df' —Ç–∞–∫ —è–∫ –≤—ñ–Ω –ø–µ—Ä–µ–¥–∞—î—Ç—å—Å—è –æ–∫—Ä–µ–º–æ
    valid_params.discard('reference_df')
    
    filtered_config = {}
    invalid_params = []
    
    for key, value in config.items():
        if key in valid_params:
            filtered_config[key] = value
        else:
            invalid_params.append(key)
    
    if invalid_params:
        print(f"‚ÑπÔ∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –Ω–µ–≤–∞–ª—ñ–¥–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏: {', '.join(invalid_params)}")
    
    return filtered_config

def simulate_mpc_with_config(
    reference_df: pd.DataFrame,
    config_name: str = "conservative",
    manual_overrides: Optional[Dict[str, Any]] = None,
    progress_callback: Callable = None,
    **kwargs
) -> Tuple[pd.DataFrame, Dict]:
    """
    –ó–∞–ø—É—Å–∫–∞—î —Å–∏–º—É–ª—è—Ü—ñ—é MPC –∑ –±–∞–∑–æ–≤–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é —Ç–∞ —Ä—É—á–Ω–∏–º–∏ –∫–æ—Ä–µ–≥—É–≤–∞–Ω–Ω—è–º–∏.
    
    Args:
        reference_df: –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ñ –¥–∞–Ω—ñ
        config_name: –ù–∞–∑–≤–∞ –±–∞–∑–æ–≤–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º "conservative")
        manual_overrides: –°–ª–æ–≤–Ω–∏–∫ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        progress_callback: –§—É–Ω–∫—Ü—ñ—è –∑–≤–æ—Ä–æ—Ç–Ω–æ–≥–æ –≤–∏–∫–ª–∏–∫—É –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—É
        **kwargs: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –ø–µ—Ä–µ–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è
        
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏, –º–µ—Ç—Ä–∏–∫–∏)
    """
    
    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞–∑–æ–≤—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
    try:
        params = load_mpc_config(config_name)
        print(f"üìã –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –±–∞–∑–æ–≤—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é: {config_name}")
        
        # –ü–æ–∫–∞–∑—É—î–º–æ –∫–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –±–∞–∑–æ–≤–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
        key_params = ['model_type', 'Np', 'Nc', 'ref_fe', 'ref_mass', 'w_fe', 'w_mass', 'Œª_obj']
        print("üìä –ö–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –±–∞–∑–æ–≤–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó:")
        for param in key_params:
            if param in params:
                print(f"   ‚Ä¢ {param}: {params[param]}")
                
    except FileNotFoundError:
        print(f"‚ö†Ô∏è –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è '{config_name}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
        available = list_available_configs()
        if available:
            print(f"–î–æ—Å—Ç—É–ø–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó: {', '.join(available)}")
            print("–°—Ç–≤–æ—Ä—é—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó...")
            create_default_configs()
            params = load_mpc_config("conservative")
        else:
            raise FileNotFoundError(f"–ù–µ –≤–¥–∞—î—Ç—å—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é '{config_name}'")
    
    # 2. –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ —Ä—É—á–Ω—ñ –∫–æ—Ä–µ–≥—É–≤–∞–Ω–Ω—è
    if manual_overrides:
        print(f"\nüîß –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ {len(manual_overrides)} —Ä—É—á–Ω–∏—Ö –∫–æ—Ä–µ–≥—É–≤–∞–Ω—å:")
        for key, value in manual_overrides.items():
            old_value = params.get(key, "–Ω–µ –∑–∞–¥–∞–Ω–æ")
            params[key] = value
            print(f"   ‚Ä¢ {key}: {old_value} ‚Üí {value}")
    
    # 3. –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ kwargs (–Ω–∞–π–Ω–∏–∂—á–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç)
    if kwargs:
        print(f"\n‚öôÔ∏è –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ {len(kwargs)} –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤:")
        for key, value in kwargs.items():
            if key not in manual_overrides:  # –ù–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—É—î–º–æ —Ä—É—á–Ω—ñ –∫–æ—Ä–µ–≥—É–≤–∞–Ω–Ω—è
                old_value = params.get(key, "–Ω–µ –∑–∞–¥–∞–Ω–æ")
                params[key] = value
                print(f"   ‚Ä¢ {key}: {old_value} ‚Üí {value}")
    
    # 4. –î–æ–¥–∞—î–º–æ progress_callback
    if progress_callback:
        params['progress_callback'] = progress_callback
    
    # 5. –ü–æ–∫–∞–∑—É—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
    print(f"\n‚úÖ –§—ñ–Ω–∞–ª—å–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–ª—è –∑–∞–ø—É—Å–∫—É:")
    for param in key_params:
        if param in params:
            print(f"   ‚Ä¢ {param}: {params[param]}")
    
    # 6. –§—ñ–ª—å—Ç—Ä—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è simulate_mpc
    filtered_params = filter_config_for_simulate_mpc(params)
    
    # 7. –ó–∞–ø—É—Å–∫–∞—î–º–æ –æ—Å–Ω–æ–≤–Ω—É —Å–∏–º—É–ª—è—Ü—ñ—é
    return simulate_mpc(reference_df, **filtered_params)

def prompt_manual_adjustments(base_config: Dict[str, Any]) -> Dict[str, Any]:
    """
    –ó–∞–ø–∏—Ç—É—î –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ –ø—Ä–æ —Ä—É—á–Ω—ñ –∫–æ—Ä–µ–≥—É–≤–∞–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó.
    
    Args:
        base_config: –ë–∞–∑–æ–≤–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
        
    Returns:
        –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä—É—á–Ω–∏–º–∏ –∫–æ—Ä–µ–≥—É–≤–∞–Ω–Ω—è–º–∏
    """
    print(f"\nüîß –†–£–ß–ù–ï –ö–û–†–ï–ì–£–í–ê–ù–ù–Ø –ü–ê–†–ê–ú–ï–¢–†–Ü–í")
    print("=" * 50)
    print("–ù–∞—Ç–∏—Å–Ω—ñ—Ç—å Enter —â–æ–± –∑–∞–ª–∏—à–∏—Ç–∏ –∑–Ω–∞—á–µ–Ω–Ω—è –±–µ–∑ –∑–º—ñ–Ω")
    
    adjustments = {}
    
    # –ì—Ä—É–ø—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏
    categories = {
        "üìä –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏": [
            ("N_data", "–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ –¥–∞–Ω–∏—Ö", int),
            ("control_pts", "–ö—Ä–æ–∫—ñ–≤ –∫–µ—Ä—É–≤–∞–Ω–Ω—è", int),
            ("seed", "–ó–µ—Ä–Ω–æ –≤–∏–ø–∞–¥–∫–æ–≤–æ—Å—Ç—ñ", int)
        ],
        "ü§ñ –ú–æ–¥–µ–ª—å": [
            ("model_type", "–¢–∏–ø –º–æ–¥–µ–ª—ñ (krr/svr/linear/gpr)", str),
            ("kernel", "–¢–∏–ø —è–¥—Ä–∞ (rbf/linear/poly)", str),
            ("find_optimal_params", "–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (true/false)", lambda x: x.lower() == 'true')
        ],
        "üéØ MPC –≥–æ—Ä–∏–∑–æ–Ω—Ç–∏": [
            ("Np", "–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è", int),
            ("Nc", "–ì–æ—Ä–∏–∑–æ–Ω—Ç –∫–µ—Ä—É–≤–∞–Ω–Ω—è", int),
            ("lag", "–õ–∞–≥ –º–æ–¥–µ–ª—ñ", int),
            ("Œª_obj", "–ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è", float)
        ],
        "üìç –£—Å—Ç–∞–≤–∫–∏ —Ç–∞ –≤–∞–≥–∏": [
            ("ref_fe", "–£—Å—Ç–∞–≤–∫–∞ Fe %", float),
            ("ref_mass", "–£—Å—Ç–∞–≤–∫–∞ –º–∞—Å–æ–≤–æ–≥–æ –ø–æ—Ç–æ–∫—É", float),
            ("w_fe", "–í–∞–≥–∞ –¥–ª—è Fe", float),
            ("w_mass", "–í–∞–≥–∞ –¥–ª—è –º–∞—Å–æ–≤–æ–≥–æ –ø–æ—Ç–æ–∫—É", float)
        ],
        "üîß Trust Region": [
            ("adaptive_trust_region", "–ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π trust region (true/false)", lambda x: x.lower() == 'true'),
            ("initial_trust_radius", "–ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Ä–∞–¥—ñ—É—Å", float),
            ("min_trust_radius", "–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —Ä–∞–¥—ñ—É—Å", float),
            ("max_trust_radius", "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —Ä–∞–¥—ñ—É—Å", float)
        ],
        "‚öôÔ∏è EKF –ø–∞—Ä–∞–º–µ—Ç—Ä–∏": [
            ("Q_phys", "–®—É–º –ø—Ä–æ—Ü–µ—Å—É (—Ñ—ñ–∑–∏—á–Ω—ñ —Å—Ç–∞–Ω–∏)", float),
            ("R", "–®—É–º –≤–∏–º—ñ—Ä—é–≤–∞–Ω—å", float),
            ("q_alpha", "–ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó Q", float)
        ]
    }
    
    for category_name, params_list in categories.items():
        print(f"\n{category_name}:")
        
        for param_name, description, param_type in params_list:
            current_value = base_config.get(param_name, "–Ω–µ –∑–∞–¥–∞–Ω–æ")
            
            try:
                prompt = f"  {description} (–ø–æ—Ç–æ—á–Ω–µ: {current_value}): "
                user_input = input(prompt).strip()
                
                if user_input:  # –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –≤–≤—ñ–≤ —â–æ—Å—å
                    if param_type == str:
                        adjustments[param_name] = user_input
                    elif param_type in [int, float]:
                        adjustments[param_name] = param_type(user_input)
                    elif callable(param_type):  # –î–ª—è bool —Ç–∞ —ñ–Ω—à–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π
                        adjustments[param_name] = param_type(user_input)
                        
            except (ValueError, TypeError) as e:
                print(f"    ‚ö†Ô∏è –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è –¥–ª—è {param_name}: {e}")
                continue
    
    return adjustments

# –ó–º—ñ–Ω–µ–Ω–∏–π –±–ª–æ–∫ if __name__ == '__main__':
if __name__ == '__main__':
    
    def my_progress(step, total, msg):
        """–ü—Ä–æ—Å—Ç–∏–π callback –¥–ª—è –≤–∏–≤–æ–¥—É –ø—Ä–æ–≥—Ä–µ—Å—É –≤ –∫–æ–Ω—Å–æ–ª—å"""
        if step % 20 == 0 or step == total:  # –ü–æ–∫–∞–∑—É—î–º–æ –∫–æ–∂–Ω—ñ 20 –∫—Ä–æ–∫—ñ–≤
            print(f"[{step}/{total}] {msg}")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ
    try:
        hist_df = pd.read_parquet('processed.parquet')
        print("‚úÖ –î–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
    except FileNotFoundError:
        print("‚ùå –ü–æ–º–∏–ª–∫–∞: —Ñ–∞–π–ª 'processed.parquet' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        exit(1)
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó —è–∫—â–æ —ó—Ö –Ω–µ–º–∞—î
    config_dir = Path("mpc_configs")
    if not config_dir.exists() or len(list_available_configs()) == 0:
        print("üìÅ –°—Ç–≤–æ—Ä—é—î–º–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó...")
        create_default_configs()
    
    # –ü–æ–∫–∞–∑—É—î–º–æ –¥–æ—Å—Ç—É–ø–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
    available_configs = list_available_configs()
    print(f"\nüìã –î–æ—Å—Ç—É–ø–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó: {', '.join(available_configs)}")
    
    # –í–∏–±—ñ—Ä –±–∞–∑–æ–≤–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
    print(f"\n–û–±–µ—Ä—ñ—Ç—å –±–∞–∑–æ–≤—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é:")
    for i, config in enumerate(available_configs, 1):
        print(f"{i}. {config}")
    
    choice = input(f"–í–∞—à –≤–∏–±—ñ—Ä (1-{len(available_configs)}, –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 1): ").strip()
    
    try:
        config_index = int(choice) - 1 if choice else 0
        if 0 <= config_index < len(available_configs):
            selected_config = available_configs[config_index]
        else:
            selected_config = available_configs[0]
    except (ValueError, IndexError):
        selected_config = available_configs[0]
    
    print(f"üéØ –û–±—Ä–∞–Ω–æ –±–∞–∑–æ–≤—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é: {selected_config}")
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –±–∞–∑–æ–≤–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
    base_config = load_mpc_config(selected_config)
    
    # –ü–æ–∫–∞–∑—É—î–º–æ –ø–æ—Ç–æ—á–Ω—ñ –∫–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    key_params = ['model_type', 'Np', 'Nc', 'ref_fe', 'ref_mass', 'w_fe', 'w_mass', 'Œª_obj', 'N_data', 'control_pts']
    print(f"\nüìä –ü–æ—Ç–æ—á–Ω—ñ –∫–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:")
    for param in key_params:
        if param in base_config:
            print(f"   ‚Ä¢ {param}: {base_config[param]}")
    
    # –ó–∞–ø–∏—Ç—É—î–º–æ –ø—Ä–æ —Ä—É—á–Ω—ñ –∫–æ—Ä–µ–≥—É–≤–∞–Ω–Ω—è
    want_adjustments = input(f"\n–•–æ—á–µ—Ç–µ –≤–Ω–µ—Å—Ç–∏ —Ä—É—á–Ω—ñ –∫–æ—Ä–µ–≥—É–≤–∞–Ω–Ω—è? (y/N): ").strip().lower()
    
    manual_overrides = {}
    if want_adjustments in ['y', 'yes', '—Ç–∞–∫', '—Ç']:
        manual_overrides = prompt_manual_adjustments(base_config)
        
        if manual_overrides:
            print(f"\n‚úÖ –ó–∞–ø–ª–∞–Ω–æ–≤–∞–Ω–æ {len(manual_overrides)} –∫–æ—Ä–µ–≥—É–≤–∞–Ω—å")
            
            # –ó–∞–ø–∏—Ç—É—î–º–æ –ø—Ä–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —è–∫ –Ω–æ–≤–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
            save_as_new = input("–ó–±–µ—Ä–µ–≥—Ç–∏ —è–∫ –Ω–æ–≤—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é? (y/N): ").strip().lower()
            if save_as_new in ['y', 'yes', '—Ç–∞–∫', '—Ç']:
                new_config_name = input("–í–≤–µ–¥—ñ—Ç—å –Ω–∞–∑–≤—É –Ω–æ–≤–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó: ").strip()
                if new_config_name:
                    # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é
                    new_config = base_config.copy()
                    new_config.update(manual_overrides)
                    new_config['name'] = new_config_name
                    new_config['description'] = f"–ë–∞–∑—É—î—Ç—å—Å—è –Ω–∞ {selected_config} –∑ —Ä—É—á–Ω–∏–º–∏ –∫–æ—Ä–µ–≥—É–≤–∞–Ω–Ω—è–º–∏"
                    
                    save_mpc_config(new_config, new_config_name)
                    print(f"üíæ –ù–æ–≤—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∑–±–µ—Ä–µ–∂–µ–Ω–æ —è–∫: {new_config_name}")
        else:
            print("‚ÑπÔ∏è –ö–æ—Ä–µ–≥—É–≤–∞–Ω–Ω—è –Ω–µ –≤–Ω–µ—Å–µ–Ω–æ")
    
    # –ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü—ñ—ó
    print(f"\nüöÄ –ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü—ñ—ó...")
    print("=" * 50)
    
    try:
        result = simulate_mpc_with_config(
            hist_df,
            config_name=selected_config,
            manual_overrides=manual_overrides,
            progress_callback=my_progress
        )
        
        if result is None:
            print("‚ùå simulate_mpc_with_config –ø–æ–≤–µ—Ä–Ω—É–≤ None")
            exit(1)
        
        results_df, metrics = result
        
        # –í–∏–≤–æ–¥–∏–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–ò –°–ò–ú–£–õ–Ø–¶–Ü–á:")
        print("=" * 40)
        print(f"üìà –û–±—Ä–æ–±–ª–µ–Ω–æ –∫—Ä–æ–∫—ñ–≤: {len(results_df)}")
        
        # –ö–ª—é—á–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
        key_metrics = ['test_mse_total', 'test_rmse_conc_fe', 'test_rmse_conc_mass']
        for metric in key_metrics:
            if metric in metrics:
                print(f"üìä {metric}: {metrics[metric]:.6f}")
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        output_file = f'mpc_results_{selected_config}_{timestamp}.parquet'
        results_df.to_parquet(output_file)
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {output_file}")
        
        print("\n‚úÖ –°–∏–º—É–ª—è—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ!")
        
    except Exception as e:
        print(f"\n‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —Å–∏–º—É–ª—è—Ü—ñ—ó: {e}")
        import traceback
        traceback.print_exc()