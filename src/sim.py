# sim.py

import numpy as np
import pandas as pd
import inspect
import traceback  

from typing import Callable, Dict, Any, Tuple
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from collections import deque

from data_gen import StatefulDataGenerator
from model import KernelModel
from objectives import MaxIronMassTrackingObjective
from mpc import MPCController
# from conf_manager import MPCConfigManager
from utils import (
    run_post_simulation_analysis_enhanced,  diagnose_mpc_behavior, diagnose_ekf_detailed
)
from ekf import ExtendedKalmanFilter
from anomaly_detector import SignalAnomalyDetector
from maf import MovingAverageFilter
from benchmark import benchmark_model_training, benchmark_mpc_solve_time
from conf_manager import config_manager
from typing import Optional, List

# =============================================================================
# === –ë–õ–û–ö 1: –ü–Ü–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ò–• –¢–ê –°–ö–ê–õ–ï–†–Ü–í ===
# =============================================================================

def prepare_simulation_data(
    reference_df: pd.DataFrame,
    params: Dict[str, Any]
) -> Tuple[StatefulDataGenerator, pd.DataFrame, np.ndarray, np.ndarray]:
    """
    –°—Ç–≤–æ—Ä—é—î –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä, —Ñ–æ—Ä–º—É—î —á–∞—Å–æ–≤–∏–π —Ä—è–¥ —ñ–∑ –∞–Ω–æ–º–∞–ª—ñ—è–º–∏,
    –ü–†–û–ú–ò–í–ê–Ñ –π–æ–≥–æ SignalAnomalyDetector-–æ–º, —Ç–∞ –±—É–¥—É—î
    –ª–∞–≥–æ–≤–∞–Ω—ñ –º–∞—Ç—Ä–∏—Ü—ñ X, Y –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è/—Å–∏–º—É–ª—è—Ü—ñ—ó.
    """
    print("–ö—Ä–æ–∫ 1: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–º—É–ª—è—Ü—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö...")

    # 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä ¬´plant¬ª
    true_gen = StatefulDataGenerator(
        reference_df,
        ore_flow_var_pct=3.0,
        time_step_s=params['time_step_s'],
        time_constants_s=params['time_constants_s'],
        dead_times_s=params['dead_times_s'],
        true_model_type=params['plant_model_type'],
        seed=params['seed']
    )

    # 2. –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∞–Ω–æ–º–∞–ª—ñ–π
    anomaly_cfg = StatefulDataGenerator.generate_anomaly_config(
        N_data=params['N_data'],
        train_frac=params['train_size'],
        val_frac=params['val_size'],
        test_frac=params['test_size'],
        seed=params['seed']
    )
    # 3. –ì–µ–Ω–µ—Ä—É—î–º–æ –ø–æ–≤–Ω–∏–π —á–∞—Å–æ–≤–∏–π —Ä—è–¥ (–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏)
    df_true_orig = true_gen.generate(
        T=params['N_data'],
        control_pts=params['control_pts'],
        n_neighbors=params['n_neighbors'],
        noise_level=params['noise_level'],
        anomaly_config=anomaly_cfg
    )
    
    if params['enable_nonlinear']:
        # 4. –í–∏–∑–Ω–∞—á–∞—î–º–æ, —è–∫ –º–∏ —Ö–æ—á–µ–º–æ –ø–æ—Å–∏–ª–∏—Ç–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å
        nonlinear_config = params['nonlinear_config']
           
        # 5. –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∑ –ø–æ—Å–∏–ª–µ–Ω–æ—é –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—é
        #    –ú–æ–∂–Ω–∞ –ø–µ—Ä–µ–¥–∞—Ç–∏ —Ç–æ–π —Å–∞–º–∏–π —Ä—ñ–≤–µ–Ω—å —à—É–º—É —Ç–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∞–Ω–æ–º–∞–ª—ñ–π
        df_true = true_gen.generate_nonlinear_variant(
            base_df=df_true_orig,
            non_linear_factors=nonlinear_config,
            noise_level='none',
            anomaly_config=None # –∞–±–æ –ø–µ—Ä–µ–¥–∞—Ç–∏ —Å—é–¥–∏ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –∞–Ω–æ–º–∞–ª—ñ–π
        )
    else:
        df_true=df_true_orig
    
    # 6. OFFLINE-–û–ß–ò–©–ï–ù–ù–Ø –≤—Ö—ñ–¥–Ω–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤ –≤—ñ–¥ –∞–Ω–æ–º–∞–ª—ñ–π
    #    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ —Å–∞–º—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è, —â–æ –π –≤ online-—Ü–∏–∫–ª—ñ, –∞–±–æ –º–µ–Ω—à –∂–æ—Ä—Å—Ç–∫—ñ.
    ad_config = params.get('anomaly_params', {})
    ad_feed_fe = SignalAnomalyDetector(**ad_config)
    ad_ore_flow = SignalAnomalyDetector(**ad_config)


    filtered_feed = []
    filtered_ore  = []
    for raw_fe, raw_ore in zip(df_true['feed_fe_percent'], df_true['ore_mass_flow']):
        filtered_feed.append(ad_feed_fe.update(raw_fe))
        filtered_ore.append(ad_ore_flow.update(raw_ore))

    # –ü—ñ–¥–º—ñ–Ω—é—î–º–æ ¬´–±—Ä—É–¥–Ω—ñ¬ª –∫–æ–ª–æ–Ω–∫–∏ –Ω–∞ ¬´–æ—á–∏—â–µ–Ω—ñ¬ª
    df_true = df_true.copy()
    df_true['feed_fe_percent'] = filtered_feed
    df_true['ore_mass_flow']   = filtered_ore

    # 7. –õ–∞–≥–æ–≤–∞–Ω—ñ –≤–∏–±—ñ—Ä–∫–∏ –¥–ª—è —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è/—Å–∏–º—É–ª—è—Ü—ñ—ó
    X, Y_full_np = StatefulDataGenerator.create_lagged_dataset(
        df_true,
        lags=params['lag']
    )
    # –í–∏–±–∏—Ä–∞—î–º–æ –ª–∏—à–µ concentrate_fe —Ç–∞ concentrate_mass –∫–æ–ª–æ–Ω–∫–∏
    Y = Y_full_np[:, [0, 2]]

    return true_gen, df_true, X, Y

def split_and_scale_data(
    X: np.ndarray,
    Y: np.ndarray,
    params: Dict[str, Any]
) -> Tuple[Dict[str, np.ndarray], StandardScaler, StandardScaler]:
    """
    –†–æ–∑–±–∏–≤–∞—î –¥–∞–Ω—ñ –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π/–≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π/—Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±–æ—Ä–∏ —Ç–∞ –º–∞—Å—à—Ç–∞–±—É—î —ó—Ö.
    
    Args:
        X: –í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ.
        Y: –í–∏—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ.
        params: –°–ª–æ–≤–Ω–∏–∫ –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Ä–æ–∑–±–∏—Ç—Ç—è.

    Returns:
        –ö–æ—Ä—Ç–µ–∂ –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–æ–º –¥–∞–Ω–∏—Ö —Ç–∞ –Ω–∞–≤—á–µ–Ω–∏–º–∏ —Å–∫–∞–ª–µ—Ä–∞–º–∏ –¥–ª—è X —Ç–∞ Y.
    """
    n = X.shape[0]
    n_train = int(params['train_size'] * n)
    n_val = int(params['val_size'] * n)

    data_splits = {
        'X_train': X[:n_train], 'Y_train': Y[:n_train],
        'X_val': X[n_train:n_train + n_val], 'Y_val': Y[n_train:n_train + n_val],
        'X_test': X[n_train + n_val:], 'Y_test': Y[n_train + n_val:]
    }

    x_scaler = StandardScaler()
    y_scaler = StandardScaler()

    data_splits['X_train_scaled'] = x_scaler.fit_transform(data_splits['X_train'])
    data_splits['Y_train_scaled'] = y_scaler.fit_transform(data_splits['Y_train'])
    data_splits['X_val_scaled'] = x_scaler.transform(data_splits['X_val'])
    data_splits['Y_val_scaled'] = y_scaler.transform(data_splits['Y_val'])
    data_splits['X_test_scaled'] = x_scaler.transform(data_splits['X_test'])
    data_splits['Y_test_scaled'] = y_scaler.transform(data_splits['Y_test'])
    
    return data_splits, x_scaler, y_scaler


# =============================================================================
# === –ë–õ–û–ö 2: –Ü–ù–Ü–¶–Ü–ê–õ–Ü–ó–ê–¶–Ü–Ø –ö–û–ú–ü–û–ù–ï–ù–¢–Ü–í MPC —Ç–∞ EKF ===
# =============================================================================


def train_and_evaluate_model(
    mpc: MPCController,
    data: Dict[str, np.ndarray],
    y_scaler: StandardScaler
) -> Dict[str, float]:
    """
    –ù–∞–≤—á–∞—î –º–æ–¥–µ–ª—å –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ MPC —Ç–∞ –æ—Ü—ñ–Ω—é—î —ó—ó —è–∫—ñ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö.
    """
    print("–ö—Ä–æ–∫ 3: –ù–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –æ—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –ø—Ä–æ—Ü–µ—Å—É...")
    mpc.fit(data['X_train_scaled'], data['Y_train_scaled'])

    y_pred_scaled = mpc.model.predict(data['X_test_scaled'])
    y_pred_orig = y_scaler.inverse_transform(y_pred_scaled)
    
    test_mse = mean_squared_error(data['Y_test'], y_pred_orig)
    print(f"-> –ó–∞–≥–∞–ª—å–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö (MSE): {test_mse:.4f}")
    
    metrics = {'test_mse_total': test_mse}
    output_columns = ['conc_fe', 'conc_mass']
    for i, col in enumerate(output_columns):
        rmse = np.sqrt(mean_squared_error(data['Y_test'][:, i], y_pred_orig[:, i]))
        metrics[f'test_rmse_{col}'] = rmse
        print(f"-> RMSE –¥–ª—è {col}: {rmse:.3f}")
        
    return metrics

def initialize_mpc_controller_enhanced(
    params: Dict[str, Any],
    x_scaler: StandardScaler,
    y_scaler: StandardScaler
) -> MPCController:
    """
    –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î –ø–æ–∫—Ä–∞—â–µ–Ω–∏–π MPC –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä –∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–∏–º trust region.
    """
    print("–ö—Ä–æ–∫ 2: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ MPC –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä–∞...")
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –ø—Ä–æ—Ü–µ—Å—É
    kernel_model = KernelModel(
        model_type=params['model_type'],
        kernel=params['kernel'],
        find_optimal_params=params['find_optimal_params']
    )
    
    # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è —É—Å—Ç–∞–≤–æ–∫ —Ç–∞ –æ–±–º–µ–∂–µ–Ω—å
    ref_point_scaled = y_scaler.transform(np.array([[params['ref_fe'], params['ref_mass']]]))[0]
    y_max_scaled = y_scaler.transform(np.array([[params['y_max_fe'], params['y_max_mass']]]))[0]

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó
    objective = MaxIronMassTrackingObjective(
        Œª=params['Œª_obj'], w_fe=params['w_fe'], w_mass=params['w_mass'],
        ref_fe=ref_point_scaled[0], ref_mass=ref_point_scaled[1], K_I=params['K_I']
    )
    
    # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤–∞–≥ –¥–ª—è –º'—è–∫–∏—Ö –æ–±–º–µ–∂–µ–Ω—å
    avg_tracking_weight = (params['w_fe'] + params['w_mass']) / 2.
    rho_y_val = avg_tracking_weight * 1000
    rho_du_val = params['Œª_obj'] * 100

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä–∞ –∑ –Ω–æ–≤–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    mpc = MPCController(
        model=kernel_model, 
        objective=objective, 
        x_scaler=x_scaler, 
        y_scaler=y_scaler,
        n_targets=2, 
        horizon=params['Np'], 
        control_horizon=params['Nc'], 
        lag=params['lag'],
        u_min=params['u_min'], 
        u_max=params['u_max'], 
        delta_u_max=params['delta_u_max'],
        use_disturbance_estimator=params['use_disturbance_estimator'],
        y_max=list(y_max_scaled) if params['use_soft_constraints'] else None,
        rho_y=rho_y_val, 
        rho_delta_u=rho_du_val, 
        rho_trust=params['rho_trust'],
        # === –ù–û–í–Ü –ü–ê–†–ê–ú–ï–¢–†–ò ===
        adaptive_trust_region=params.get('adaptive_trust_region', True),
        initial_trust_radius=params.get('initial_trust_radius', 1.0),
        min_trust_radius=params.get('min_trust_radius', 0.1),
        max_trust_radius=params.get('max_trust_radius', 5.0),
        trust_decay_factor=params.get('trust_decay_factor', 0.8),
        linearization_check_enabled=params.get('linearization_check_enabled', True),
        max_linearization_distance=params.get('max_linearization_distance', 2.0)
    )
    return mpc

def run_simulation_loop_enhanced(
    true_gen: StatefulDataGenerator,
    mpc: MPCController,
    ekf: ExtendedKalmanFilter,
    df_true: pd.DataFrame,
    data: Dict[str, np.ndarray],
    scalers: Tuple[StandardScaler, StandardScaler],
    params: Dict[str, Any],
    progress_callback: Callable | None = None,
) -> Tuple[pd.DataFrame, Dict]:
    """
    –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π —Ü–∏–∫–ª —Å–∏–º—É–ª—è—Ü—ñ—ó –∑ –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥–æ–º trust region —Ç–∞ —è–∫–æ—Å—Ç—ñ –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó.
    """
    print("–ö—Ä–æ–∫ 5: –ó–∞–ø—É—Å–∫ –ø–æ–∫—Ä–∞—â–µ–Ω–æ–≥–æ —Ü–∏–∫–ª—É —Å–∏–º—É–ª—è—Ü—ñ—ó...")
    x_scaler, y_scaler = scalers

    # –ü–æ—á–∞—Ç–∫–æ–≤–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è (—è–∫ —ñ —Ä–∞–Ω—ñ—à–µ)
    n_total = len(df_true) - params['lag'] - 1
    n_train = int(params['train_size'] * n_total)
    n_val   = int(params['val_size'] * n_total)
    test_idx_start = params['lag'] + 1 + n_train + n_val

    hist0_unscaled = df_true[['feed_fe_percent',
                              'ore_mass_flow',
                              'solid_feed_percent']].iloc[
        test_idx_start - (params['lag'] + 1): test_idx_start
    ].values

    mpc.reset_history(hist0_unscaled)
    true_gen.reset_state(hist0_unscaled)

    df_run = df_true.iloc[test_idx_start:]
    d_all  = df_run[['feed_fe_percent', 'ore_mass_flow']].values
    T_sim  = len(df_run) - (params['lag'] + 1)

    # –°–ª—É–∂–±–æ–≤—ñ –∑–º—ñ–Ω–Ω—ñ (—Ä–æ–∑—à–∏—Ä–µ–Ω—ñ)
    records = []
    y_true_hist, x_hat_hist, P_hist, innov_hist, R_hist = [], [], [], [], []
    u_seq_hist = []
    d_hat_hist = []
    
    # ‚úÖ –î–û–î–ê–Ñ–ú–û –ó–ú–Ü–ù–ù–Ü –î–õ–Ø –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ò EKF:
    y_true_seq = []     # –†–µ–∞–ª—å–Ω—ñ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è
    y_pred_seq = []     # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    x_est_seq = []      # –û—Ü—ñ–Ω–∫–∏ —Å—Ç–∞–Ω—É EKF
    innovation_seq = [] # –Ü–Ω–Ω–æ–≤–∞—Ü—ñ—ó EKF
    
    trust_region_stats_hist = []  # –ù–û–í–ò–ô
    linearization_quality_hist = []  # –ù–û–í–ò–ô
    u_prev = float(hist0_unscaled[-1, 2])

    # –§—ñ–ª—å—Ç—Ä–∏ —Ç–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∏ –∞–Ω–æ–º–∞–ª—ñ–π
    window_size = 4
    filt_feed = MovingAverageFilter(window_size)
    filt_ore  = MovingAverageFilter(window_size)

    retrain_cooldown_timer = 0

    # –ë—É—Ñ–µ—Ä–∏ —Ç–∞ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
    if params['enable_retraining']:
        print(f"-> –î–∏–Ω–∞–º—ñ—á–Ω–µ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è –£–í–Ü–ú–ö–ù–ï–ù–û. "
              f"–í—ñ–∫–Ω–æ: {params['retrain_window_size']}, "
              f"–ü–µ—Ä—ñ–æ–¥ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏: {params['retrain_period']}")
        retraining_buffer   = deque(maxlen=params['retrain_window_size'])
        initial_train_data  = list(zip(data['X_train_scaled'],
                                       data['Y_train_scaled']))
        retraining_buffer.extend(initial_train_data)
        innovation_monitor  = deque(maxlen=params['retrain_period'])

    # ONLINE-–¥–µ—Ç–µ–∫—Ç–æ—Ä–∏ –∞–Ω–æ–º–∞–ª—ñ–π
    ad_config = params.get('anomaly_params', {})
    ad_feed_fe = SignalAnomalyDetector(**ad_config)
    ad_ore_flow = SignalAnomalyDetector(**ad_config)

    # –ì–æ–ª–æ–≤–Ω–∏–π —Ü–∏–∫–ª —Å–∏–º—É–ª—è—Ü—ñ—ó –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º–∏
    for t in range(T_sim):
        if progress_callback:
            progress_callback(t, T_sim, f"–ö—Ä–æ–∫ —Å–∏–º—É–ª—è—Ü—ñ—ó {t + 1}/{T_sim}")

        # 1. –°–∏—Ä—ñ –≤–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è
        feed_fe_raw, ore_flow_raw = d_all[t, :]

        # 2. ONLINE-—Ñ—ñ–ª—å—Ç—Ä—É–≤–∞–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π
        feed_fe_filt_anom = ad_feed_fe.update(feed_fe_raw)
        ore_flow_filt_anom = ad_ore_flow.update(ore_flow_raw)

        # 3. –ì—Ä—É–±–µ –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è
        d_filt = np.array([filt_feed.update(feed_fe_filt_anom),
                           filt_ore.update(ore_flow_filt_anom)])

        # 4. EKF: –ø—Ä–æ–≥–Ω–æ–∑
        ekf.predict(u_prev, d_filt)

        # 5. –û–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó –≤ MPC
        x_est_phys_unscaled = ekf.x_hat[:ekf.n_phys].reshape(params['lag'] + 1, 3)
        mpc.reset_history(x_est_phys_unscaled)
        mpc.d_hat = ekf.x_hat[ekf.n_phys:]

        # –ë–µ—Ä–µ–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω —ñ –ø–µ—Ä–µ–¥–±–∞—á–∞—î–º–æ –Ω–∞—Å—Ç—É–ø–Ω–∏–π –≤–∏—Ö—ñ–¥
        current_state = x_est_phys_unscaled.flatten().reshape(1, -1)
        current_state_scaled = x_scaler.transform(current_state)
        y_pred_scaled = mpc.model.predict(current_state_scaled)[0]
        y_pred_unscaled = y_scaler.inverse_transform(y_pred_scaled.reshape(1, -1))[0]

        # 6. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è MPC –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º–∏
        d_seq = np.repeat(d_filt.reshape(1, -1), params['Np'], axis=0)
        u_seq = mpc.optimize(d_seq, u_prev)
        u_cur = u_prev if u_seq is None else float(u_seq[0])

        # –î–û–î–ê–ô –î–Ü–ê–ì–ù–û–°–¢–ò–ö–£ –¢–£–¢:
        if t % 10 == 0:  # –ö–æ–∂–Ω—ñ 10 –∫—Ä–æ–∫—ñ–≤
            diagnose_mpc_behavior(mpc, t, u_seq, u_prev, d_seq)
        
        u_cur = u_prev if u_seq is None else float(u_seq[0])

        # 7. –ö—Ä–æ–∫ ¬´—Ä–µ–∞–ª—å–Ω–æ–≥–æ¬ª –ø—Ä–æ—Ü–µ—Å—É
        y_full = true_gen.step(feed_fe_raw, ore_flow_raw, u_cur)

        # 8. EKF: –∫–æ—Ä–µ–∫—Ü—ñ—è
        y_meas_unscaled = y_full[['concentrate_fe_percent',
                                  'concentrate_mass_flow']].values.flatten()
        ekf.update(y_meas_unscaled)

        # ‚úÖ –ó–ë–ò–†–ê–Ñ–ú–û –î–ê–ù–Ü –î–õ–Ø –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ò EKF:
        y_true_seq.append(y_meas_unscaled.copy())
        
        # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ (–∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–µ—Ä–µ–¥ update)
        # if hasattr(ekf, 'y_pred') and ekf.y_pred is not None:
        #     y_pred_seq.append(ekf.y_pred.copy())
        # else:
        #     y_pred_seq.append(np.zeros(2))
        y_pred_seq.append(y_pred_unscaled.copy())
        
        # –û—Ü—ñ–Ω–∫–∞ —Å—Ç–∞–Ω—É –ø—ñ—Å–ª—è update
        x_est_seq.append(ekf.x_hat.copy())
        
        # –Ü–Ω–Ω–æ–≤–∞—Ü—ñ—ó
        if hasattr(ekf, 'last_innovation') and ekf.last_innovation is not None:
            innovation_seq.append(ekf.last_innovation.copy())
        else:
            innovation_seq.append(np.zeros(2))

        # 9. –ó–º–µ–Ω—à—É—î–º–æ cooldown-—Ç–∞–π–º–µ—Ä
        if retrain_cooldown_timer > 0:
            retrain_cooldown_timer -= 1

        # === –ù–û–í–ê –õ–û–ì–Ü–ö–ê: –ó–ë–Ü–† –°–¢–ê–¢–ò–°–¢–ò–ö–ò TRUST REGION ===
        if hasattr(mpc, 'get_trust_region_stats'):
            trust_stats = mpc.get_trust_region_stats()
            trust_region_stats_hist.append(trust_stats)
            
            # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –∑–±–µ—Ä—ñ–≥–∞—î–º–æ —è–∫—ñ—Å—Ç—å –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó –ø—Ä–∞–≤–∏–ª—å–Ω–æ
            if hasattr(mpc, 'linearization_quality_history') and mpc.linearization_quality_history:
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –∑ —ñ—Å—Ç–æ—Ä—ñ—ó
                if isinstance(mpc.linearization_quality_history[-1], dict):
                    linearization_quality_hist.append(mpc.linearization_quality_history[-1]['euclidean_distance'])
                else:
                    linearization_quality_hist.append(mpc.linearization_quality_history[-1])

        # 10. –ë—É—Ñ–µ—Ä–∏–∑–∞—Ü—ñ—è —Ç–∞ –º–æ–∂–ª–∏–≤–µ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
        if params['enable_retraining']:
            new_x_unscaled = mpc.x_hist.flatten().reshape(1, -1)
            new_y_unscaled = y_meas_unscaled.reshape(1, -1)

            new_x_scaled = x_scaler.transform(new_x_unscaled)
            new_y_scaled = y_scaler.transform(new_y_unscaled)

            retraining_buffer.append((new_x_scaled[0], new_y_scaled[0]))

            if ekf.last_innovation is not None:
                innov_norm = np.linalg.norm(ekf.last_innovation)
                innovation_monitor.append(innov_norm)

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
            if (t > 0 and
                t % params['retrain_period'] == 0 and
                len(innovation_monitor) == params['retrain_period'] and
                retrain_cooldown_timer == 0):

                avg_innov = float(np.mean(innovation_monitor))

                # === –ü–û–ö–†–ê–©–ï–ù–ê –õ–û–ì–Ü–ö–ê –ü–ï–†–ï–ù–ê–í–ß–ê–ù–ù–Ø ===
                # –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞: —è–∫—ñ—Å—Ç—å –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó
                should_retrain = avg_innov > params['retrain_innov_threshold']
                
                if (hasattr(mpc, 'linearization_quality_history') and 
                    len(mpc.linearization_quality_history) > 10):
                    
                    # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤–∏—Ç—è–≥—É—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –≤—ñ–¥—Å—Ç–∞–Ω—ñ
                    if isinstance(mpc.linearization_quality_history[-1], dict):
                        recent_distances = [h['euclidean_distance'] for h in mpc.linearization_quality_history[-10:]]
                    else:
                        recent_distances = mpc.linearization_quality_history[-10:]
                    
                    recent_lin_quality = np.mean(recent_distances)
                    lin_threshold = params.get('retrain_linearization_threshold', 1.5)
                    
                    if recent_lin_quality > lin_threshold:
                        print(f"  -> –î–æ–¥–∞—Ç–∫–æ–≤–∏–π —Ç—Ä–∏–≥–µ—Ä: –ø–æ–≥–∞–Ω–∞ —è–∫—ñ—Å—Ç—å –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó ({recent_lin_quality:.3f} > {lin_threshold})")
                        should_retrain = True

                if should_retrain:
                    print(f"\n---> –¢–†–ò–ì–ï–† –ü–ï–†–ï–ù–ê–í–ß–ê–ù–ù–Ø –Ω–∞ –∫—Ä–æ—Ü—ñ {t}! "
                          f"–°–µ—Ä–µ–¥–Ω—è —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—è: {avg_innov:.4f} > "
                          f"{params['retrain_innov_threshold']:.4f}")

                    retrain_data = list(retraining_buffer)
                    X_retrain = np.array([p[0] for p in retrain_data])
                    Y_retrain = np.array([p[1] for p in retrain_data])

                    print(f"--> mpc.fit() –Ω–∞ {len(X_retrain)} —Å–µ–º–ø–ª–∞—Ö ...")
                    mpc.fit(X_retrain, Y_retrain)
                    print("--> –ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
                    
                    # –°–∫–∏–¥–∞—î–º–æ trust region –ø—ñ—Å–ª—è –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
                    if hasattr(mpc, 'reset_trust_region'):
                        mpc.reset_trust_region()
                        print("--> Trust region —Å–∫–∏–Ω—É—Ç–æ.\n")

                    innovation_monitor.clear()
                    retrain_cooldown_timer = params['retrain_period'] * 2

        # 11. –õ–æ–≥—É–≤–∞–Ω–Ω—è –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó / –º–µ—Ç—Ä–∏–∫
        y_true_hist.append(y_meas_unscaled)
        x_hat_hist.append(ekf.x_hat.copy())
        P_hist.append(ekf.P.copy())
        R_hist.append(ekf.R.copy())
        innov_hist.append(
            ekf.last_innovation.copy()
            if ekf.last_innovation is not None
            else np.zeros(ekf.n_dist)
        )

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–ª–∞–Ω—ñ–≤ MPC —Ç–∞ –æ—Ü—ñ–Ω–æ–∫ –∑–±—É—Ä–µ–Ω—å
        if u_seq is not None:
            u_seq_hist.append(u_seq)
        if mpc.d_hat is not None:
            d_hat_orig = y_scaler.inverse_transform(mpc.d_hat.reshape(1, -1))[0]
            d_hat_hist.append(d_hat_orig)

        y_meas = y_full.iloc[0]
        records.append({
            'feed_fe_percent':      y_meas.feed_fe_percent,
            'ore_mass_flow':        y_meas.ore_mass_flow,
            'solid_feed_percent':   u_cur,
            'conc_fe':              y_meas.concentrate_fe_percent,
            'tail_fe':              y_meas.tailings_fe_percent,
            'conc_mass':            y_meas.concentrate_mass_flow,
            'tail_mass':            y_meas.tailings_mass_flow,
            'mass_pull_pct':        y_meas.mass_pull_percent,
            'fe_recovery_percent':  y_meas.fe_recovery_percent,
        })

        u_prev = u_cur

    if progress_callback:
        progress_callback(T_sim, T_sim, "–°–∏–º—É–ª—è—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    # ‚úÖ –î–û–î–ê–Ñ–ú–û –î–Ü–ê–ì–ù–û–°–¢–ò–ö–£ EKF:
    diagnose_ekf_detailed(ekf, y_true_seq, y_pred_seq, x_est_seq, innovation_seq)
        
    # –†–æ–∑—à–∏—Ä–µ–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
    analysis_data = {
        "y_true": np.vstack(y_true_hist),
        "x_hat": np.vstack(x_hat_hist),
        "P": np.stack(P_hist),
        "innov": np.vstack(innov_hist),
        "R": np.stack(R_hist),
        "u_seq": u_seq_hist,
        "d_hat": np.vstack(d_hat_hist) if d_hat_hist else np.array([]),
        "trust_region_stats": trust_region_stats_hist,
        "linearization_quality": linearization_quality_hist,
        # ‚úÖ –î–û–î–ê–Ñ–ú–û –î–ê–ù–Ü –î–õ–Ø –ü–û–î–ê–õ–¨–®–û–ì–û –ê–ù–ê–õ–Ü–ó–£:
        "y_true_seq": y_true_seq,
        "y_pred_seq": y_pred_seq,
        "x_est_seq": x_est_seq,
        "innovation_seq": innovation_seq,
    }

    return pd.DataFrame(records), analysis_data

def initialize_ekf(
    mpc: MPCController,
    scalers: Tuple[StandardScaler, StandardScaler],
    hist0_unscaled: np.ndarray,
    Y_train_scaled: np.ndarray,
    lag: int,
    params: Dict[str, Any]
) -> ExtendedKalmanFilter:
    """
    –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–π —Ñ—ñ–ª—å—Ç—Ä –ö–∞–ª–º–∞–Ω–∞ (EKF).
    """
    print("–ö—Ä–æ–∫ 4: –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ñ—ñ–ª—å—Ç—Ä–∞ –ö–∞–ª–º–∞–Ω–∞ (EKF)...")
       
    x_scaler, y_scaler = scalers
    n_phys, n_dist = (lag + 1) * 3, 2
    
    # x0_aug = np.hstack([hist0_unscaled.flatten(), np.zeros(n_dist)])
    # ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –†–æ–∑—É–º–Ω–∞ –ø–æ—á–∞—Ç–∫–æ–≤–∞ –æ—Ü—ñ–Ω–∫–∞ –∑–±—É—Ä–µ–Ω—å
    # –ë–∞–∑—É—é—á–∏—Å—å –Ω–∞ —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω—ñ–π –ø–æ–º–∏–ª—Ü—ñ –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    initial_disturbances = np.array([0.7, 0.0])  # –ë–ª–∏–∑—å–∫–æ –¥–æ Innovation mean: [0.71, 0.04]
    
    x0_aug = np.hstack([hist0_unscaled.flatten(), initial_disturbances])
    
    # P0 = np.eye(n_phys + n_dist) * params['P0']
    # P0[n_phys:, n_phys:] *= 1 
    P0 = np.eye(n_phys + n_dist) * params['P0'] * 1.5  # –ë—É–ª–æ: * 1.0
    P0[n_phys:, n_phys:] *= 10  # –ó–∞–ª–∏—à–∏—Ç–∏ —è–∫ —î

    Q_phys = np.eye(n_phys) * params['Q_phys']
    Q_dist = np.eye(n_dist) * params['Q_dist'] 
    Q = np.block([[Q_phys, np.zeros((n_phys, n_dist))], [np.zeros((n_dist, n_phys)), Q_dist]])
    
    # R = np.diag(np.var(Y_train_scaled, axis=0)) * params['R']
    R = np.diag(np.var(Y_train_scaled, axis=0)) * params['R'] * 0.5
    
    return ExtendedKalmanFilter(
        mpc.model, x_scaler, y_scaler, x0_aug, P0, Q, R, lag,
        beta_R=params.get('beta_R', 0.1), # .get –¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ
        q_adaptive_enabled=params.get('q_adaptive_enabled', True),
        q_alpha=params.get('q_alpha', 0.995),
        q_nis_threshold=params.get('q_nis_threshold', 1.8)        
    )

def collect_performance_metrics(
    mpc: MPCController,
    data: Dict[str, np.ndarray],
    model_config: Dict
) -> Dict[str, float]:
    """–ó–±–∏—Ä–∞—î –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –¥–ª—è —Å—Ç–∞—Ç—Ç—ñ"""
    
    print("üìä –ó–±–∏—Ä–∞—é –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ...")
    
    # 1. –ë–µ–Ω—á–º–∞—Ä–∫ –Ω–∞–≤—á–∞–Ω–Ω—è —Ç–∞ –ø—Ä–æ–≥–Ω–æ–∑—É
    model_configs = [model_config]  # –ü–æ—Ç–æ—á–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    training_metrics = benchmark_model_training(
        data['X_train_scaled'], 
        data['Y_train_scaled'], 
        model_configs
    )
    
    # 2. –ë–µ–Ω—á–º–∞—Ä–∫ MPC
    mpc_metrics = benchmark_mpc_solve_time(mpc, n_iterations=50)
    
    # 3. –ó–∞–≥–∞–ª—å–Ω–∏–π —á–∞—Å —Ü–∏–∫–ª—É (–ø—Ä–∏–±–ª–∏–∑–Ω–æ)
    total_cycle_time = (
        training_metrics[f"{model_config['model_type']}-{model_config.get('kernel', 'default')}_predict_time"] +
        training_metrics[f"{model_config['model_type']}-{model_config.get('kernel', 'default')}_linearize_time"] +
        mpc_metrics["mpc_solve_mean"]
    )
    
    # 4. –û–±'—î–¥–Ω—É—î–º–æ –≤—Å–µ
    all_metrics = {**training_metrics, **mpc_metrics}
    all_metrics["total_cycle_time"] = total_cycle_time
    
    return all_metrics
    
# ========================================  
# –û–°–ù–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø –°–ò–ú–£–õ–Ø–¶–Ü–á (–ë–ï–ó –¶–ò–ö–õ–Ü–ß–ù–û–ì–û –í–ò–ö–õ–ò–ö–£)  
# ========================================  

def simulate_mpc_core(  
    reference_df: pd.DataFrame,             # DataFrame –∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏  
    N_data: int = 5000,                     # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫ –¥–∞–Ω–∏—Ö  
    control_pts: int = 1000,                # –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫—Ä–æ–∫—ñ–≤ MPC  
    time_step_s: int = 5,                   # –ß–∞—Å–æ–≤–∏–π –∫—Ä–æ–∫  
    dead_times_s: dict = {  
        'concentrate_fe_percent': 20.0,  
        'tailings_fe_percent': 25.0,  
        'concentrate_mass_flow': 20.0,  
        'tailings_mass_flow': 25.0  
    },                                      # –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∞ –∑–∞—Ç—Ä–∏–º–∫–∞  
    time_constants_s: dict = {  
        'concentrate_fe_percent': 8.0,  
        'tailings_fe_percent': 10.0,  
        'concentrate_mass_flow': 5.0,  
        'tailings_mass_flow': 7.0  
    },                                      # –Ü–Ω–µ—Ä—Ü—ñ–π–Ω—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤  
    lag: int = 2,                           # –ö—Ä–æ–∫–∏ –∑–∞—Ç—Ä–∏–º–∫–∏ –º–æ–¥–µ–ª—ñ  
    Np: int = 6,                            # –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è MPC  
    Nc: int = 4,                            # –ì–æ—Ä–∏–∑–æ–Ω—Ç –∫–µ—Ä—É–≤–∞–Ω–Ω—è MPC  
    n_neighbors: int = 5,                   # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—É—Å—ñ–¥—ñ–≤ –¥–ª—è KNN  
    seed: int = 0,                          # –ó–µ—Ä–Ω–æ –¥–ª—è RNG  
    noise_level: str = 'none',              # –†—ñ–≤–µ–Ω—å —à—É–º—É  
    model_type: str = 'krr',                # –¢–∏–ø –º–æ–¥–µ–ª—ñ MPC  
    kernel: str = 'rbf',                    # –¢–∏–ø —è–¥—Ä–∞  
    linear_type: str = 'ridge',             # ols, ridge, lasso  
    poly_degree: int = 2,                   # –°—Ç–µ–ø—ñ–Ω—å –ø–æ–ª—ñ–Ω–æ–º—É  
    alpha: float = 1.0,                     # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è  
    find_optimal_params: bool = True,       # –ü–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤  
    Œª_obj: float = 0.1,                     # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è  
    K_I: float = 0.01,                      # –Ü–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–∏–π –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç  
    w_fe: float = 7.0,                      # –í–∞–≥–∞ –¥–ª—è Fe  
    w_mass: float = 1.0,                    # –í–∞–≥–∞ –¥–ª—è –º–∞—Å–æ–≤–æ—ó –≤–∏—Ç—Ä–∞—Ç–∏  
    ref_fe: float = 53.5,                   # –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è Fe  
    ref_mass: float = 57.0,                 # –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è –º–∞—Å–∏  
    train_size: float = 0.7,                # –†–æ–∑–º—ñ—Ä –Ω–∞–≤—á–∞–ª—å–Ω–æ—ó –≤–∏–±—ñ—Ä–∫–∏  
    val_size: float = 0.15,                 # –†–æ–∑–º—ñ—Ä –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ—ó –≤–∏–±—ñ—Ä–∫–∏  
    test_size: float = 0.15,                # –†–æ–∑–º—ñ—Ä —Ç–µ—Å—Ç–æ–≤–æ—ó –≤–∏–±—ñ—Ä–∫–∏  
    u_min: float = 20.0,                    # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –∫–µ—Ä—É–≤–∞–Ω–Ω—è  
    u_max: float = 40.0,                    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –∫–µ—Ä—É–≤–∞–Ω–Ω—è  
    delta_u_max: float = 1.0,               # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∑–º—ñ–Ω–∞ –∫–µ—Ä—É–≤–∞–Ω–Ω—è  
    use_disturbance_estimator: bool = True, # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è EKF  
    y_max_fe: float = 54.5,                 # –í–µ—Ä—Ö–Ω—è –º–µ–∂–∞ Fe  
    y_max_mass: float = 58.0,               # –í–µ—Ä—Ö–Ω—è –º–µ–∂–∞ –º–∞—Å–∏  
    rho_trust: float = 0.1,                 # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –¥–æ–≤—ñ—Ä–∏  
    max_trust_radius: float = 5.0,          # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —Ä–∞–¥—ñ—É—Å –¥–æ–≤—ñ—Ä–∏  
    adaptive_trust_region: bool = True,     # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞ –æ–±–ª–∞—Å—Ç—å –¥–æ–≤—ñ—Ä–∏  
    initial_trust_radius: float = 1.0,      # –ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Ä–∞–¥—ñ—É—Å –¥–æ–≤—ñ—Ä–∏  
    min_trust_radius: float = 0.5,          # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —Ä–∞–¥—ñ—É—Å –¥–æ–≤—ñ—Ä–∏  
    trust_decay_factor: float = 0.8,        # –§–∞–∫—Ç–æ—Ä –∑–º–µ–Ω—à–µ–Ω–Ω—è –¥–æ–≤—ñ—Ä–∏  
    linearization_check_enabled: bool = True,           # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó  
    max_linearization_distance: float = 2.0,            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –≤—ñ–¥—Å—Ç–∞–Ω—å –ª—ñ–Ω–µ–∞—Ä–∏–∑–∞—Ü—ñ—ó  
    retrain_linearization_threshold: float = 1.5,       # –ü–æ—Ä—ñ–≥ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è  
    use_soft_constraints: bool = True,      # –ú'—è–∫—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è  
    plant_model_type: str = 'rf',           # –¢–∏–ø –º–æ–¥–µ–ª—ñ –æ–±'—î–∫—Ç–∞  
    enable_retraining: bool = True,         # –ü–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ  
    retrain_period: int = 50,               # –ü–µ—Ä—ñ–æ–¥–∏—á–Ω—ñ—Å—Ç—å –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è  
    retrain_window_size: int = 1000,        # –†–æ–∑–º—ñ—Ä –≤—ñ–∫–Ω–∞ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è  
    retrain_innov_threshold: float = 0.3,   # –ü–æ—Ä—ñ–≥ —ñ–Ω–Ω–æ–≤–∞—Ü—ñ—ó EKF  
    anomaly_params: dict = {  
        'window': 25,  
        'spike_z': 4.0,  
        'drop_rel': 0.30,  
        'freeze_len': 5,  
        'enabled': True  
    },                                      # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –∞–Ω–æ–º–∞–ª—ñ–π  
    nonlinear_config: dict = {  
        'concentrate_fe_percent': ('pow', 2),  
        'concentrate_mass_flow': ('pow', 1.5)  
    },                                      # –ù–µ–ª—ñ–Ω—ñ–π–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è  
    enable_nonlinear: bool = False,         # –ù–µ–ª—ñ–Ω—ñ–π–Ω–∏–π —Ä–µ–∂–∏–º  
    run_analysis: bool = True,              # –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤  
    P0: float = 1e-2,                      # –ü–æ—á–∞—Ç–∫–æ–≤–∞ –∫–æ–≤–∞—Ä—ñ–∞—Ü—ñ—è EKF  
    Q_phys: float = 1500,                  # –ö–æ–≤–∞—Ä—ñ–∞—Ü—ñ—è –ø—Ä–æ—Ü–µ—Å—É  
    Q_dist: float = 1,                     # –ö–æ–≤–∞—Ä—ñ–∞—Ü—ñ—è –∑–±—É—Ä–µ–Ω—å  
    R: float = 0.01,                       # –ö–æ–≤–∞—Ä—ñ–∞—Ü—ñ—è –≤–∏–º—ñ—Ä—é–≤–∞–Ω—å  
    q_adaptive_enabled: bool = True,        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞ Q –º–∞—Ç—Ä–∏—Ü—è  
    q_alpha: float = 0.99,                 # –§–∞–∫—Ç–æ—Ä –∑–≥–ª–∞–¥–∂—É–≤–∞–Ω–Ω—è Q  
    q_nis_threshold: float = 1.5,          # –ü–æ—Ä—ñ–≥ NIS  
    progress_callback: Callable[[int, int, str], None] = None  # Callback –ø—Ä–æ–≥—Ä–µ—Å—É  
) -> Tuple[pd.DataFrame, Dict]:  
    """  
    –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è —Å–∏–º—É–ª—è—Ü—ñ—ó MPC –¥–ª—è –º–∞–≥–Ω—ñ—Ç–Ω–æ—ó —Å–µ–ø–∞—Ä–∞—Ü—ñ—ó  
    
    Returns:  
        Tuple[pd.DataFrame, Dict]: (results_df, metrics)  
    """  
    
    # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤ —Å–ª–æ–≤–Ω–∏–∫ (–∑–∞–º—ñ—Å—Ç—å locals() —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø—Ä–æ–±–ª–µ–º)  
    params = {  
        'N_data': N_data, 'control_pts': control_pts, 'time_step_s': time_step_s,  
        'dead_times_s': dead_times_s, 'time_constants_s': time_constants_s,  
        'lag': lag, 'Np': Np, 'Nc': Nc, 'n_neighbors': n_neighbors,  
        'seed': seed, 'noise_level': noise_level, 'model_type': model_type,  
        'kernel': kernel, 'linear_type': linear_type, 'poly_degree': poly_degree,  
        'alpha': alpha, 'find_optimal_params': find_optimal_params,  
        'Œª_obj': Œª_obj, 'K_I': K_I, 'w_fe': w_fe, 'w_mass': w_mass,  
        'ref_fe': ref_fe, 'ref_mass': ref_mass, 'train_size': train_size,  
        'val_size': val_size, 'test_size': test_size, 'u_min': u_min,  
        'u_max': u_max, 'delta_u_max': delta_u_max,  
        'use_disturbance_estimator': use_disturbance_estimator,  
        'y_max_fe': y_max_fe, 'y_max_mass': y_max_mass,  
        'rho_trust': rho_trust, 'max_trust_radius': max_trust_radius,  
        'adaptive_trust_region': adaptive_trust_region,  
        'initial_trust_radius': initial_trust_radius,  
        'min_trust_radius': min_trust_radius,  
        'trust_decay_factor': trust_decay_factor,  
        'linearization_check_enabled': linearization_check_enabled,  
        'max_linearization_distance': max_linearization_distance,  
        'retrain_linearization_threshold': retrain_linearization_threshold,  
        'use_soft_constraints': use_soft_constraints,  
        'plant_model_type': plant_model_type,   
        'enable_retraining': enable_retraining,  
        'retrain_period': retrain_period,   
        'retrain_window_size': retrain_window_size,  
        'retrain_innov_threshold': retrain_innov_threshold,  
        'anomaly_params': anomaly_params,   
        'nonlinear_config': nonlinear_config,  
        'enable_nonlinear': enable_nonlinear,   
        'run_analysis': run_analysis,  
        'P0': P0, 'Q_phys': Q_phys, 'Q_dist': Q_dist, 'R': R,  
        'q_adaptive_enabled': q_adaptive_enabled,   
        'q_alpha': q_alpha,  
        'q_nis_threshold': q_nis_threshold,   
        'progress_callback': progress_callback  
    }  
    
    try:  
        # 1. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö  
        true_gen, df_true, X, Y = prepare_simulation_data(reference_df, params)  
        data, x_scaler, y_scaler = split_and_scale_data(X, Y, params)  

        # 2. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è MPC  
        mpc = initialize_mpc_controller_enhanced(params, x_scaler, y_scaler)  
        metrics = train_and_evaluate_model(mpc, data, y_scaler)  

        # 3. –ë–µ–Ω—á–º–∞—Ä–∫ –º–µ—Ç—Ä–∏–∫–∏  
        perf_metrics = collect_performance_metrics(mpc, data, {  
            'model_type': params['model_type'],  
            'kernel': params.get('kernel', 'default'),  
            'linear_type': params.get('linear_type', 'default'),  
            'poly_degree': params.get('poly_degree', 1),  
            'find_optimal_params': params.get('find_optimal_params', False)  
        })  
        
        metrics.update(perf_metrics)  
        
        # 4. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è EKF  
        n_train_pts = len(data['X_train'])  
        n_val_pts = len(data['X_val'])  
        test_idx_start = params['lag'] + 1 + n_train_pts + n_val_pts  
        hist0_unscaled = df_true[['feed_fe_percent', 'ore_mass_flow', 'solid_feed_percent']].iloc[  
            test_idx_start - (params['lag'] + 1): test_idx_start  
        ].values  
        
        ekf = initialize_ekf(mpc, (x_scaler, y_scaler), hist0_unscaled, data['Y_train_scaled'], params['lag'], params)  

        # 5. –ó–∞–ø—É—Å–∫ —Å–∏–º—É–ª—è—Ü—ñ—ó  
        results_df, analysis_data = run_simulation_loop_enhanced(  
            true_gen, mpc, ekf, df_true, data, (x_scaler, y_scaler), params,   
            params.get('progress_callback')  
        )  
        
        # 6. –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤  
        test_idx_start = params['lag'] + 1 + len(data['X_train']) + len(data['X_val'])  
        analysis_data['d_all_test'] = df_true.iloc[test_idx_start:][['feed_fe_percent','ore_mass_flow']].values  
        
        if params.get('run_analysis', True):  
            run_post_simulation_analysis_enhanced(results_df, analysis_data, params)  
        
        return results_df, metrics  
        
    except Exception as e:  
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≤ simulate_mpc_core: {e}")  
        traceback.print_exc()  
        raise  


# ========================================
# WRAPPER –§–£–ù–ö–¶–Ü–Ø –ó –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø–ú–ò
# ========================================

def simulate_mpc_with_config(
    hist_df: pd.DataFrame, 
    config: Optional[str] = None,
    config_overrides: Optional[Dict[str, Any]] = None,
    progress_callback: Optional[Callable] = None,
    **kwargs
) -> Tuple[pd.DataFrame, Dict]:
    """
    Wrapper —Ñ—É–Ω–∫—Ü—ñ—è –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π + –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    """
    
    # 1. –ó–±–∏—Ä–∞—î–º–æ –ø–æ–≤–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é (—è–∫ —ñ —Ä–∞–Ω—ñ—à–µ)
    if config:
        print(f"üìã –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ø—Ä–æ—Ñ—ñ–ª—å –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó: '{config}'")
        try:
            params = config_manager.load_config(config)
            print(f"   ‚úÖ –ü—Ä–æ—Ñ—ñ–ª—å '{config}' –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
        except Exception as e:
            print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            params = {}
    else:
        params = {}
    
    if config_overrides:
        print(f"üîß –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ {len(config_overrides)} override –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤")
        params.update(config_overrides)
    
    if kwargs:
        print(f"‚öôÔ∏è –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ {len(kwargs)} –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤")
        params.update(kwargs)
    
    # üÜï –ó–ë–ï–†–Ü–ì–ê–Ñ–ú–û –ü–û–í–ù–£ –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Æ –î–õ–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í
    full_config_info = {
        'config_source': config if config else 'default',
        'config_overrides': config_overrides.copy() if config_overrides else {},
        'kwargs_applied': kwargs.copy(),
        'final_params': params.copy(),  # üìã –ü–æ–≤–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è —â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∞—Å—è
        'timestamp': pd.Timestamp.now().isoformat(),
        'total_params_count': len(params)
    }
    
    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –¥–ª—è simulate_mpc_core
    core_signature = inspect.signature(simulate_mpc_core)
    valid_params = set(core_signature.parameters.keys())
    sim_params = {k: v for k, v in params.items() if k in valid_params}
    
    if progress_callback:
        sim_params['progress_callback'] = progress_callback
    
    print(f"üöÄ –ü–µ—Ä–µ–¥–∞—î–º–æ {len(sim_params)} –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –≤ simulate_mpc_core")
    
    try:
        results, metrics = simulate_mpc_core(hist_df, **sim_params)
        
        # üÜï –î–û–î–ê–Ñ–ú–û –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Æ –î–û –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í DataFrame
        print("üíæ –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –¥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤...")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–ª–æ–Ω–∫–∏ –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é (–∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ñ –¥–ª—è –≤—Å—ñ—Ö —Ä—è–¥–∫—ñ–≤)
        results['config_source'] = full_config_info['config_source']
        results['config_timestamp'] = full_config_info['timestamp']
        
        # –ö–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —è–∫ –æ–∫—Ä–µ–º—ñ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –ª–µ–≥–∫–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
        key_params = ['model_type', 'kernel', 'linear_type', 'Np', 'Nc', 
                     'w_fe', 'w_mass', 'ref_fe', 'ref_mass', 'Œª_obj']
        
        for param in key_params:
            if param in full_config_info['final_params']:
                results[f'cfg_{param}'] = full_config_info['final_params'][param]
        
        # üÜï –î–û–î–ê–Ñ–ú–û –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Æ –î–û –ú–ï–¢–†–ò–ö
        metrics['config_info'] = full_config_info
        metrics['config_summary'] = {
            'source': full_config_info['config_source'],
            'model_type': full_config_info['final_params'].get('model_type', 'unknown'),
            'kernel': full_config_info['final_params'].get('kernel', 'unknown'),
            'horizons': f"Np={full_config_info['final_params'].get('Np', '?')}, Nc={full_config_info['final_params'].get('Nc', '?')}",
            'weights': f"w_fe={full_config_info['final_params'].get('w_fe', '?')}, w_mass={full_config_info['final_params'].get('w_mass', '?')}"
        }
        
        print("‚úÖ –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –¥–æ–¥–∞–Ω–æ –¥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
        return results, metrics
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å–∏–º—É–ª—è—Ü—ñ—ó: {e}")
        traceback.print_exc()
        raise


# üÜï –§–£–ù–ö–¶–Ü–Ø –î–õ–Ø –ê–ù–ê–õ–Ü–ó–£ –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–á –ó –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í
def analyze_results_config(results_df: pd.DataFrame, metrics: Dict = None) -> None:
    """
    –ê–Ω–∞–ª—ñ–∑—É—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é —Å–∏–º—É–ª—è—Ü—ñ—ó –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    
    Args:
        results_df: DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å–∏–º—É–ª—è—Ü—ñ—ó
        metrics: –°–ª–æ–≤–Ω–∏–∫ –º–µ—Ç—Ä–∏–∫ (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
    """
    
    print("="*60)
    print("üìã –ê–ù–ê–õ–Ü–ó –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–á –°–ò–ú–£–õ–Ø–¶–Ü–á")
    print("="*60)
    
    # –ë–∞–∑–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑ DataFrame
    if 'config_source' in results_df.columns:
        config_source = results_df['config_source'].iloc[0]
        print(f"üéØ –î–∂–µ—Ä–µ–ª–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó: {config_source}")
    
    if 'config_timestamp' in results_df.columns:
        timestamp = results_df['config_timestamp'].iloc[0]
        print(f"üïí –ß–∞—Å —Å–∏–º—É–ª—è—Ü—ñ—ó: {timestamp}")
    
    # –ö–ª—é—á–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ –∫–æ–ª–æ–Ω–æ–∫
    config_columns = [col for col in results_df.columns if col.startswith('cfg_')]
    if config_columns:
        print(f"\nüîß –ö–õ–Æ–ß–û–í–Ü –ü–ê–†–ê–ú–ï–¢–†–ò ({len(config_columns)}):")
        for col in sorted(config_columns):
            param_name = col.replace('cfg_', '')
            value = results_df[col].iloc[0]
            print(f"   {param_name}: {value}")
    
    # –î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑ metrics
    if metrics and 'config_info' in metrics:
        config_info = metrics['config_info']
        
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–á:")
        print(f"   –ó–∞–≥–∞–ª–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {config_info['total_params_count']}")
        print(f"   Override –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {len(config_info['config_overrides'])}")
        print(f"   Kwargs –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {len(config_info['kwargs_applied'])}")
        
        if config_info['config_overrides']:
            print(f"\nüîÑ –ü–ï–†–ï–í–ò–ó–ù–ê–ß–ï–ù–Ü –ü–ê–†–ê–ú–ï–¢–†–ò:")
            for key, value in config_info['config_overrides'].items():
                print(f"   {key}: {value}")
        
        if config_info['kwargs_applied']:
            print(f"\n‚öôÔ∏è –î–û–î–ê–¢–ö–û–í–Ü –ü–ê–†–ê–ú–ï–¢–†–ò (kwargs):")
            for key, value in config_info['kwargs_applied'].items():
                print(f"   {key}: {value}")
    
    # –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    numeric_cols = results_df.select_dtypes(include=[np.number]).columns
    non_config_cols = [col for col in numeric_cols if not col.startswith('cfg_')]
    
    if non_config_cols:
        print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–ò –°–ò–ú–£–õ–Ø–¶–Ü–á:")
        print(f"   –ö—Ä–æ–∫—ñ–≤ —Å–∏–º—É–ª—è—Ü—ñ—ó: {len(results_df)}")
        if 'y_fe_pred' in results_df.columns and 'y_mass_pred' in results_df.columns:
            fe_mean = results_df['y_fe_pred'].mean()
            mass_mean = results_df['y_mass_pred'].mean()
            print(f"   –°–µ—Ä–µ–¥–Ω—î Fe: {fe_mean:.2f}")
            print(f"   –°–µ—Ä–µ–¥–Ω—è –º–∞—Å–∞: {mass_mean:.2f}")
    
    print("="*60)


# üÜï –§–£–ù–ö–¶–Ü–Ø –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–ô
def compare_simulation_configs(*results_and_metrics_pairs) -> pd.DataFrame:
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó —Ä—ñ–∑–Ω–∏—Ö —Å–∏–º—É–ª—è—Ü—ñ–π
    
    Args:
        *results_and_metrics_pairs: –ü–∞—Ä–∏ (results_df, metrics) –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    
    Returns:
        DataFrame –∑ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è–º –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π
    """
    
    comparison_data = []
    
    for i, (results_df, metrics) in enumerate(results_and_metrics_pairs):
        row = {'simulation': f'Run_{i+1}'}
        
        # –ë–∞–∑–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
        if 'config_source' in results_df.columns:
            row['config_source'] = results_df['config_source'].iloc[0]
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ –∫–æ–ª–æ–Ω–æ–∫
        config_cols = [col for col in results_df.columns if col.startswith('cfg_')]
        for col in config_cols:
            param_name = col.replace('cfg_', '')
            row[param_name] = results_df[col].iloc[0]
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ
        if isinstance(metrics, dict):
            for metric_key in ['rmse_fe', 'rmse_mass', 'r2_fe', 'r2_mass']:
                if metric_key in metrics:
                    row[metric_key] = metrics[metric_key]
        
        comparison_data.append(row)
    
    comparison_df = pd.DataFrame(comparison_data)
    
    print("üìä –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–ô:")
    print(comparison_df.to_string(index=False))
    
    return comparison_df


# ========================================
# –ê–õ–ò–ê–° –î–õ–Ø –ó–í–û–†–û–¢–ù–û–á –°–£–ú–Ü–°–ù–û–°–¢–Ü
# ========================================

# –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
simulate_mpc = simulate_mpc_with_config

print("‚úÖ simulate_mpc_with_config –≥–æ—Ç–æ–≤–∏–π –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è!")
print("üîß –§—É–Ω–∫—Ü—ñ—è config_manager.load_config() –±—É–¥–µ –≤–∏–∫–ª–∏–∫–∞—Ç–∏—Å—è –ø—Ä–∏ –≤–∫–∞–∑–∞–Ω–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ 'config'")


# ========================================
# –ü–†–ò–ö–õ–ê–î –í–ò–ö–û–†–ò–°–¢–ê–ù–ù–Ø –í __main__
# ========================================

# if __name__ == '__main__':
    
#     def my_progress(step, total, msg):
#         print(f"[{step}/{total}] {msg}")

#     # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
#     try:
#         hist_df = pd.read_parquet('processed.parquet')
#         print(f"‚úÖ –î–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {hist_df.shape}")
#     except FileNotFoundError:
#         try:
#             hist_df = pd.read_parquet('/content/KModel/src/processed.parquet')
#             print(f"‚úÖ –î–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ /content/: {hist_df.shape}")
#         except Exception as e:
#             print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")
#             hist_df = None

#     if hist_df is not None:
#         print("\nüéØ –¢–ï–°–¢–£–Ñ–ú–û –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–á:")
        
#         # üî• –¢–£–¢ –í–Ü–î–ë–£–í–ê–Ñ–¢–¨–°–Ø –í–ò–ö–õ–ò–ö config_manager.load_config():
#         try:
#             # –°–ø–æ—á–∞—Ç–∫—É –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –¥–æ—Å—Ç—É–ø–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
#             if hasattr(config_manager, 'list_configs'):
#                 available_configs = config_manager.list_configs()
#                 print(f"üìÅ –î–æ—Å—Ç—É–ø–Ω—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó: {available_configs}")
            
#             # –ó–∞–ø—É—Å–∫–∞—î–º–æ —Å–∏–º—É–ª—è—Ü—ñ—é –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é
#             results, metrics = simulate_mpc(
#                 hist_df, 
#                 config='oleksandr_original',  # üéØ –¢–£–¢ –í–ò–ö–õ–ò–ö–ê–Ñ–¢–¨–°–Ø config_manager.load_config('oleksandr_original')
#                 config_overrides={
#                     'run_analysis':False
#                 },
#                 progress_callback=my_progress
#             )
            
#             print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Å–∏–º—É–ª—è—Ü—ñ—ó:")
#             if isinstance(metrics, dict):
#                 for key in ['rmse_fe', 'rmse_mass', 'config_used']:
#                     if key in metrics:
#                         print(f"   {key}: {metrics[key]}")
            
#             # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
#             results.to_parquet('mpc_simulation_results.parquet')
#             print("üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ")

#             loaded_results = pd.read_parquet('mpc_simulation_results.parquet')
#             analyze_results_config(loaded_results, metrics)

#         except Exception as e:
#             print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è: {e}")
#             traceback.print_exc()
#     else:
#         print("üí• –î–∞–Ω—ñ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ")

# print("\nüéâ –ú–æ–¥—É–ª—å sim.py –≥–æ—Ç–æ–≤–∏–π!")

# ---- –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢
''' ==================================='''

import numpy as np  
import pandas as pd  
import time  
import json  
import os  
from datetime import datetime

def run_model_comparison_experiment(  
    hist_df: pd.DataFrame,  
    base_config: str = 'oleksandr_original',  
    n_repeats: int = 5,  
    results_dir: str = 'model_comparison_experiment',  
    save_individual: bool = True  
) -> Dict[str, Dict]:  
    """  
    –ó–∞–ø—É—Å–∫ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è–º–∏  
    
    Returns:  
        Dict –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—Å—ñ—Ö –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤  
    """  
    
    # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É  
    models_config = {  
        'KRR_RBF': {  
            'model_type': 'krr',  
            'kernel': 'rbf',  
            'find_optimal_params': True  
        },  
        'SVR_RBF': {  
            'model_type': 'svr',   
            'kernel': 'rbf',  
            'find_optimal_params': True  
        },  
        'GPR_RBF': {  
            'model_type': 'gpr',  
            'kernel': 'rbf',   
            'find_optimal_params': True  
        },  
        'LINEAR_RIDGE': {  
            'model_type': 'linear',  
            'linear_type': 'ridge',  
            'find_optimal_params': True  
        }  
    }  
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é  
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  
    full_results_dir = f"{results_dir}_{timestamp}"  
    os.makedirs(full_results_dir, exist_ok=True)  
    
    print("üî¨ –ü–û–ß–ê–¢–û–ö –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–£ –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ú–û–î–ï–õ–ï–ô")  
    print("="*70)  
    print(f"üìä –ú–æ–¥–µ–ª–µ–π: {len(models_config)}")  
    print(f"üîÑ –ü–æ–≤—Ç–æ—Ä–µ–Ω—å: {n_repeats}")  
    print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤: {full_results_dir}")  
    print(f"‚ö° –ó–∞–≥–∞–ª–æ–º –∑–∞–ø—É—Å–∫—ñ–≤: {len(models_config) * n_repeats}")  
    print("="*70)  
    
    all_results = {}  
    experiment_summary = []  
    
    total_runs = len(models_config) * n_repeats  
    current_run = 0  
    
    for model_name, model_params in models_config.items():  
        print(f"\nüß™ –ú–û–î–ï–õ–¨: {model_name}")  
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–∏: {model_params}")  
        
        model_results = {  
            'runs': {},  
            'summary_stats': {},  
            'model_config': model_params  
        }  
        
        run_metrics = []  
        
        for repeat in range(n_repeats):  
            current_run += 1  
            run_id = f"{model_name}_run_{repeat+1}"  
            
            print(f"\n   üéØ –ó–∞–ø—É—Å–∫ {repeat+1}/{n_repeats} ({current_run}/{total_runs})")  
            
            try:  
                # –ó–º—ñ–Ω—é—î–º–æ seed –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä—É  
                run_start = time.time()  
                
                results_df, metrics = simulate_mpc(  
                    hist_df,  
                    config=base_config,  
                    config_overrides=model_params,  
                    seed=repeat * 42,  # –†—ñ–∑–Ω—ñ seeds –¥–ª—è –ø–æ–≤—Ç–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ  
                    run_analysis=False  
                )  
                
                run_time = time.time() - run_start  
                
                # –î–æ–¥–∞—î–º–æ –º–µ—Ç–∞—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é  
                results_df['experiment_model'] = model_name  
                results_df['experiment_run'] = repeat + 1  
                results_df['experiment_run_id'] = run_id  
                
                metrics['experiment_info'] = {  
                    'model_name': model_name,  
                    'run_number': repeat + 1,  
                    'run_id': run_id,  
                    'seed_used': repeat * 42,  
                    'run_time_seconds': run_time  
                }  
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏  
                if save_individual:  
                    results_df.to_parquet(f"{full_results_dir}/{run_id}_results.parquet")  
                    
                    with open(f"{full_results_dir}/{run_id}_metrics.json", 'w') as f:  
                        json_metrics = {}  
                        for key, value in metrics.items():  
                            try:  
                                json.dumps(value)  # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ JSON serializable  
                                json_metrics[key] = value  
                            except:  
                                json_metrics[key] = str(value)  
                        json.dump(json_metrics, f, indent=2)  
                
                # –ó–±–∏—Ä–∞—î–º–æ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏  
                run_summary = {  
                    'run_id': run_id,  
                    'model': model_name,  
                    'run_number': repeat + 1,  
                    'run_time': run_time  
                }  
                
                # –î–æ–¥–∞—î–º–æ –∫–ª—é—á–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏  
                key_metrics = ['rmse_fe', 'rmse_mass', 'mae_fe', 'mae_mass', 'r2_fe', 'r2_mass']  
                for metric in key_metrics:  
                    if metric in metrics:  
                        run_summary[metric] = metrics[metric]  
                
                run_metrics.append(run_summary)  
                model_results['runs'][run_id] = (results_df, metrics)  
                experiment_summary.append(run_summary)  
                
                print(f"      ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {run_time:.1f}—Å")  
                if 'rmse_fe' in metrics:  
                    print(f"      üìä RMSE: Fe={metrics['rmse_fe']:.4f}, Mass={metrics.get('rmse_mass', 'N/A'):.4f}")  
                
            except Exception as e:  
                print(f"      ‚ùå –ü–û–ú–ò–õ–ö–ê: {e}")  
                run_summary = {  
                    'run_id': run_id,  
                    'model': model_name,  
                    'run_number': repeat + 1,  
                    'error': str(e)  
                }  
                experiment_summary.append(run_summary)  
                continue  
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª—ñ  
        if run_metrics:  
            metrics_df = pd.DataFrame(run_metrics)  
            
            model_stats = {}  
            for metric in ['rmse_fe', 'rmse_mass', 'r2_fe', 'r2_mass', 'run_time']:  
                if metric in metrics_df.columns:  
                    model_stats[f'{metric}_mean'] = metrics_df[metric].mean()  
                    model_stats[f'{metric}_std'] = metrics_df[metric].std()  
                    model_stats[f'{metric}_min'] = metrics_df[metric].min()  
                    model_stats[f'{metric}_max'] = metrics_df[metric].max()  
            
            model_results['summary_stats'] = model_stats  
            
            print(f"   üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê {model_name}:")  
            print(f"      RMSE Fe: {model_stats.get('rmse_fe_mean', 0):.4f} ¬± {model_stats.get('rmse_fe_std', 0):.4f}")  
            print(f"      RMSE Mass: {model_stats.get('rmse_mass_mean', 0):.4f} ¬± {model_stats.get('rmse_mass_std', 0):.4f}")  
            print(f"      R¬≤ Fe: {model_stats.get('r2_fe_mean', 0):.4f} ¬± {model_stats.get('r2_fe_std', 0):.4f}")  
            print(f"      –ß–∞—Å: {model_stats.get('run_time_mean', 0):.1f}—Å ¬± {model_stats.get('run_time_std', 0):.1f}—Å")  
        
        all_results[model_name] = model_results  
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–∞–≥–∞–ª—å–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É  
    summary_df = pd.DataFrame(experiment_summary)  
    summary_df.to_csv(f"{full_results_dir}/experiment_summary.csv", index=False)  
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω—É —Ç–∞–±–ª–∏—Ü—é  
    comparison_table = create_model_comparison_table(all_results)  
    comparison_table.to_csv(f"{full_results_dir}/model_comparison.csv", index=False)  
    
    print("\n" + "="*70)  
    print("üèÅ –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù–û")  
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {full_results_dir}")  
    print("="*70)  
    
    return all_results, comparison_table  


def create_model_comparison_table(all_results: Dict) -> pd.DataFrame:  
    """–°—Ç–≤–æ—Ä—é—î —Ç–∞–±–ª–∏—Ü—é –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π"""  
    
    comparison_data = []  
    
    for model_name, model_data in all_results.items():  
        if 'summary_stats' in model_data and model_data['summary_stats']:  
            stats = model_data['summary_stats']  
            
            row = {  
                'Model': model_name,  
                'RMSE_Fe_Mean': stats.get('rmse_fe_mean', np.nan),  
                'RMSE_Fe_Std': stats.get('rmse_fe_std', np.nan),  
                'RMSE_Mass_Mean': stats.get('rmse_mass_mean', np.nan),  
                'RMSE_Mass_Std': stats.get('rmse_mass_std', np.nan),  
                'R2_Fe_Mean': stats.get('r2_fe_mean', np.nan),  
                'R2_Fe_Std': stats.get('r2_fe_std', np.nan),  
                'R2_Mass_Mean': stats.get('r2_mass_mean', np.nan),  
                'R2_Mass_Std': stats.get('r2_mass_std', np.nan),  
                'Runtime_Mean': stats.get('run_time_mean', np.nan),  
                'Runtime_Std': stats.get('run_time_std', np.nan)  
            }  
            comparison_data.append(row)  
    
    comparison_df = pd.DataFrame(comparison_data)  
    
    # –°–æ—Ä—Ç—É—î–º–æ –ø–æ RMSE Fe (–Ω–∞–π–∫—Ä–∞—â–∏–π –∑–≤–µ—Ä—Ö—É)  
    if 'RMSE_Fe_Mean' in comparison_df.columns:  
        comparison_df = comparison_df.sort_values('RMSE_Fe_Mean')  
    
    return comparison_df  


def analyze_experiment_results(results_dir: str) -> None:  
    """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É"""  
    
    print("üìä –ê–ù–ê–õ–Ü–ó –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–£")  
    print("="*70)  
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω—É —Ç–∞–±–ª–∏—Ü—é  
    comparison_file = f"{results_dir}/model_comparison.csv"  
    if os.path.exists(comparison_file):  
        comparison_df = pd.read_csv(comparison_file)  
        
        print("üèÜ –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô (–ø–æ RMSE Fe):")  
        print(comparison_df[['Model', 'RMSE_Fe_Mean', 'RMSE_Fe_Std', 'R2_Fe_Mean']].round(4))  
        
        print("\nüìà –°–¢–ê–¢–ò–°–¢–ò–ß–ù–ê –ó–ù–ê–ß–£–©–Ü–°–¢–¨:")  
        # –ü—Ä–æ—Å—Ç–∏–π –∞–Ω–∞–ª—ñ–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ—ó –∑–Ω–∞—á—É—â–æ—Å—Ç—ñ  
        best_model = comparison_df.iloc[0]  
        print(f"ü•á –ù–∞–π–∫—Ä–∞—â–∏–π: {best_model['Model']}")  
        print(f"   RMSE Fe: {best_model['RMSE_Fe_Mean']:.4f} ¬± {best_model['RMSE_Fe_Std']:.4f}")  
        print(f"   R¬≤ Fe: {best_model['R2_Fe_Mean']:.4f} ¬± {best_model['R2_Fe_Std']:.4f}")  
        
        if len(comparison_df) > 1:  
            second_model = comparison_df.iloc[1]  
            rmse_diff = second_model['RMSE_Fe_Mean'] - best_model['RMSE_Fe_Mean']  
            combined_std = np.sqrt(best_model['RMSE_Fe_Std']**2 + second_model['RMSE_Fe_Std']**2)  
            
            print(f"\nü•à –î—Ä—É–≥–∏–π: {second_model['Model']}")  
            print(f"   –†—ñ–∑–Ω–∏—Ü—è RMSE: +{rmse_diff:.4f}")  
            print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∞ –∑–Ω–∞—á—É—â—ñ—Å—Ç—å: {'–¢–∞–∫' if rmse_diff > 2*combined_std else '–°—É–º–Ω—ñ–≤–Ω–æ'}")  
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–µ—Ç–∞–ª—å–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É  
    summary_file = f"{results_dir}/experiment_summary.csv"  
    if os.path.exists(summary_file):  
        summary_df = pd.read_csv(summary_file)  
        
        print(f"\nüìã –î–ï–¢–ê–õ–Ü –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–£:")  
        print(f"   –ó–∞–≥–∞–ª–æ–º –∑–∞–ø—É—Å–∫—ñ–≤: {len(summary_df)}")  
        print(f"   –£—Å–ø—ñ—à–Ω–∏—Ö: {len(summary_df[~summary_df.get('error', pd.Series()).notna()])}")  
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª—è—Ö  
        if 'model' in summary_df.columns:  
            model_stats = summary_df.groupby('model').agg({  
                'rmse_fe': ['count', 'mean', 'std'],  
                'run_time': ['mean', 'std']  
            }).round(4)  
            print(f"\nüìä –î–ï–¢–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ú–û–î–ï–õ–Ø–•:")  
            print(model_stats)  


# üöÄ –ó–ê–ü–£–°–ö –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–£  
def main_experiment():  
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É"""  
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ  
    try:  
        hist_df = pd.read_parquet('processed.parquet')  
        print(f"‚úÖ –î–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {hist_df.shape}")  
    except FileNotFoundError:  
        try:  
            hist_df = pd.read_parquet('/content/KModel/src/processed.parquet')  
            print(f"‚úÖ –î–∞–Ω—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –∑ /content/: {hist_df.shape}")  
        except Exception as e:  
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö: {e}")  
            return  
    
    # Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—É (–ø–æ–∫–∞–∑—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –∫–ª—é—á–æ–≤—ñ –º–æ–º–µ–Ω—Ç–∏)  
    def progress_callback(step, total, msg):  
        if step % 100 == 0 or step == total or '–∑–∞–≤–µ—Ä—à–µ–Ω' in msg.lower():  
            print(f"      [{step}/{total}] {msg}")  
    
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç  
    print("üöÄ –ü–û–ß–ò–ù–ê–Ñ–ú–û –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢...")  
    start_time = time.time()  
    
    all_results, comparison_table = run_model_comparison_experiment(  
        hist_df,  
        base_config='oleksandr_original',  
        n_repeats=5,  
        results_dir='model_comparison_experiment'  
    )  
    
    total_time = time.time() - start_time  
    
    print(f"\n‚è±Ô∏è –ó–ê–ì–ê–õ–¨–ù–ò–ô –ß–ê–° –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–£: {total_time/60:.1f} —Ö–≤")  
    print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–ò:")  
    print(comparison_table.round(4))  
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤  
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤  
    results_dirs = [d for d in os.listdir('.') if d.startswith('model_comparison_experiment_')]  
    if results_dirs:  
        latest_dir = max(results_dirs)  # –û—Å—Ç–∞–Ω–Ω—ñ–π –∑–∞ –∞–ª—Ñ–∞–≤—ñ—Ç–æ–º (–Ω–∞–π–Ω–æ–≤—ñ—à–∏–π timestamp)  
        print(f"\nüîç –î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤...")  
        analyze_experiment_results(latest_dir)  
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤  
        create_experiment_visualizations(latest_dir)  
    
    return all_results, comparison_table  


def create_experiment_visualizations(results_dir: str) -> None:  
    """–°—Ç–≤–æ—Ä—é—î –≥—Ä–∞—Ñ—ñ–∫–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É"""  
    
    try:  
        import matplotlib.pyplot as plt  
        import seaborn as sns  
        
        print("\nüìà –°–¢–í–û–†–ï–ù–ù–Ø –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–ô...")  
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ  
        comparison_df = pd.read_csv(f"{results_dir}/model_comparison.csv")  
        summary_df = pd.read_csv(f"{results_dir}/experiment_summary.csv")  
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—Ç–∏–ª—é  
        plt.style.use('default')  
        sns.set_palette("husl")  
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ñ—ñ–≥—É—Ä—É –∑ –ø—ñ–¥–≥—Ä–∞—Ñ—ñ–∫–∞–º–∏  
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))  
        fig.suptitle('üî¨ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π MPC', fontsize=16, fontweight='bold')  
        
        # 1. Bar plot RMSE Fe  
        ax1 = axes[0, 0]  
        bars1 = ax1.bar(comparison_df['Model'], comparison_df['RMSE_Fe_Mean'],   
                       yerr=comparison_df['RMSE_Fe_Std'], capsize=5, alpha=0.8)  
        ax1.set_title('üéØ RMSE Fe (–Ω–∏–∂—á–µ = –∫—Ä–∞—â–µ)', fontweight='bold')  
        ax1.set_ylabel('RMSE Fe')  
        ax1.tick_params(axis='x', rotation=45)  
        ax1.grid(True, alpha=0.3)  
        
        # –ü—ñ–¥—Å–≤—ñ—á—É—î–º–æ –Ω–∞–π–∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç  
        min_idx = comparison_df['RMSE_Fe_Mean'].idxmin()  
        bars1[min_idx].set_color('gold')  
        bars1[min_idx].set_edgecolor('orange')  
        bars1[min_idx].set_linewidth(2)  
        
        # 2. Bar plot RMSE Mass  
        ax2 = axes[0, 1]  
        bars2 = ax2.bar(comparison_df['Model'], comparison_df['RMSE_Mass_Mean'],  
                       yerr=comparison_df['RMSE_Mass_Std'], capsize=5, alpha=0.8)  
        ax2.set_title('‚öñÔ∏è RMSE Mass (–Ω–∏–∂—á–µ = –∫—Ä–∞—â–µ)', fontweight='bold')  
        ax2.set_ylabel('RMSE Mass')  
        ax2.tick_params(axis='x', rotation=45)  
        ax2.grid(True, alpha=0.3)  
        
        # 3. R¬≤ Fe  
        ax3 = axes[0, 2]  
        bars3 = ax3.bar(comparison_df['Model'], comparison_df['R2_Fe_Mean'],  
                       yerr=comparison_df['R2_Fe_Std'], capsize=5, alpha=0.8)  
        ax3.set_title('üìà R¬≤ Fe (–≤–∏—â–µ = –∫—Ä–∞—â–µ)', fontweight='bold')  
        ax3.set_ylabel('R¬≤ Fe')  
        ax3.tick_params(axis='x', rotation=45)  
        ax3.grid(True, alpha=0.3)  
        ax3.set_ylim(0, 1)  
        
        # –ü—ñ–¥—Å–≤—ñ—á—É—î–º–æ –Ω–∞–π–∫—Ä–∞—â–∏–π R¬≤  
        max_r2_idx = comparison_df['R2_Fe_Mean'].idxmax()  
        bars3[max_r2_idx].set_color('gold')  
        bars3[max_r2_idx].set_edgecolor('orange')  
        bars3[max_r2_idx].set_linewidth(2)  
        
        # 4. Box plot RMSE –ø–æ –º–æ–¥–µ–ª—è—Ö (—è–∫—â–æ —î –¥–µ—Ç–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ)  
        ax4 = axes[1, 0]  
        if 'rmse_fe' in summary_df.columns and 'model' in summary_df.columns:  
            summary_df.boxplot(column='rmse_fe', by='model', ax=ax4)  
            ax4.set_title('üì¶ –†–æ–∑–ø–æ–¥—ñ–ª RMSE Fe –ø–æ –º–æ–¥–µ–ª—è—Ö', fontweight='bold')  
            ax4.set_xlabel('–ú–æ–¥–µ–ª—å')  
            ax4.set_ylabel('RMSE Fe')  
        else:  
            ax4.text(0.5, 0.5, '–î–µ—Ç–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ\n–Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ', ha='center', va='center',   
                    transform=ax4.transAxes, fontsize=12)  
            ax4.set_title('üì¶ –†–æ–∑–ø–æ–¥—ñ–ª RMSE Fe')  
        
        # 5. –ß–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è  
        ax5 = axes[1, 1]  
        bars5 = ax5.bar(comparison_df['Model'], comparison_df['Runtime_Mean'],  
                       yerr=comparison_df['Runtime_Std'], capsize=5, alpha=0.8, color='lightcoral')  
        ax5.set_title('‚è±Ô∏è –ß–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è (—Å–µ–∫)', fontweight='bold')  
        ax5.set_ylabel('–ß–∞—Å (—Å–µ–∫)')  
        ax5.tick_params(axis='x', rotation=45)  
        ax5.grid(True, alpha=0.3)  
        
        # 6. Scatter RMSE vs R¬≤  
        ax6 = axes[1, 2]  
        scatter = ax6.scatter(comparison_df['RMSE_Fe_Mean'], comparison_df['R2_Fe_Mean'],  
                             s=100, alpha=0.7, c=range(len(comparison_df)), cmap='viridis')  
        ax6.set_xlabel('RMSE Fe')  
        ax6.set_ylabel('R¬≤ Fe')  
        ax6.set_title('üéØ RMSE vs R¬≤ (–ª—ñ–≤–æ-–≤–µ—Ä—Ö = –∫—Ä–∞—â–µ)', fontweight='bold')  
        ax6.grid(True, alpha=0.3)  
        
        # –î–æ–¥–∞—î–º–æ –ø—ñ–¥–ø–∏—Å–∏ —Ç–æ—á–æ–∫  
        for i, model in enumerate(comparison_df['Model']):  
            ax6.annotate(model,   
                        (comparison_df.iloc[i]['RMSE_Fe_Mean'], comparison_df.iloc[i]['R2_Fe_Mean']),  
                        xytext=(5, 5), textcoords='offset points', fontsize=8)  
        
        plt.tight_layout()  
        plt.savefig(f"{results_dir}/experiment_analysis.png", dpi=300, bbox_inches='tight')  
        plt.savefig(f"{results_dir}/experiment_analysis.pdf", bbox_inches='tight')  
        
        print(f"   ‚úÖ –ì—Ä–∞—Ñ—ñ–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {results_dir}/experiment_analysis.png")  
        
        # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –≥—Ä–∞—Ñ—ñ–∫: –¥–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ  
        if 'rmse_fe' in summary_df.columns:  
            create_detailed_model_analysis(summary_df, results_dir)  
        
        plt.show()  
        
    except ImportError:  
        print("   ‚ö†Ô∏è matplotlib/seaborn –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó")  
    except Exception as e:  
        print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤: {e}")  


def create_detailed_model_analysis(summary_df: pd.DataFrame, results_dir: str) -> None:  
    """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ –º–æ–¥–µ–ª—è—Ö"""  
    
    try:  
        import matplotlib.pyplot as plt  
        import seaborn as sns  
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))  
        fig.suptitle('üîç –î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª–µ–π', fontsize=14, fontweight='bold')  
        
        # 1. Violin plot RMSE Fe  
        ax1 = axes[0, 0]  
        if len(summary_df['model'].unique()) > 1:  
            sns.violinplot(data=summary_df, x='model', y='rmse_fe', ax=ax1)  
            ax1.set_title('üéª –†–æ–∑–ø–æ–¥—ñ–ª RMSE Fe')  
            ax1.tick_params(axis='x', rotation=45)  
        
        # 2. Violin plot R¬≤ Fe  
        ax2 = axes[0, 1]  
        if 'r2_fe' in summary_df.columns:  
            sns.violinplot(data=summary_df, x='model', y='r2_fe', ax=ax2)  
            ax2.set_title('üéª –†–æ–∑–ø–æ–¥—ñ–ª R¬≤ Fe')  
            ax2.tick_params(axis='x', rotation=45)  
        
        # 3. –ö–æ—Ä–µ–ª—è—Ü—ñ—è –º–µ—Ç—Ä–∏–∫  
        ax3 = axes[1, 0]  
        if 'r2_fe' in summary_df.columns:  
            ax3.scatter(summary_df['rmse_fe'], summary_df['r2_fe'], alpha=0.6)  
            ax3.set_xlabel('RMSE Fe')  
            ax3.set_ylabel('R¬≤ Fe')  
            ax3.set_title('üìä –ö–æ—Ä–µ–ª—è—Ü—ñ—è RMSE vs R¬≤')  
            ax3.grid(True, alpha=0.3)  
        
        # 4. –°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å –ø–æ –∑–∞–ø—É—Å–∫–∞–º  
        ax4 = axes[1, 1]  
        if 'run_number' in summary_df.columns:  
            for model in summary_df['model'].unique():  
                model_data = summary_df[summary_df['model'] == model]  
                ax4.plot(model_data['run_number'], model_data['rmse_fe'],   
                        'o-', label=model, alpha=0.7)  
            ax4.set_xlabel('–ù–æ–º–µ—Ä –∑–∞–ø—É—Å–∫—É')  
            ax4.set_ylabel('RMSE Fe')  
            ax4.set_title('üìà –°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å –ø–æ –∑–∞–ø—É—Å–∫–∞–º')  
            ax4.legend()  
            ax4.grid(True, alpha=0.3)  
        
        plt.tight_layout()  
        plt.savefig(f"{results_dir}/detailed_analysis.png", dpi=300, bbox_inches='tight')  
        print(f"   ‚úÖ –î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {results_dir}/detailed_analysis.png")  
        
    except Exception as e:  
        print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É: {e}")  


def statistical_significance_test(results_dir: str) -> None:  
    """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏–π —Ç–µ—Å—Ç –∑–Ω–∞—á—É—â–æ—Å—Ç—ñ —Ä—ñ–∑–Ω–∏—Ü—å –º—ñ–∂ –º–æ–¥–µ–ª—è–º–∏"""  
    
    try:  
        from scipy import stats  
        
        print("\nüßÆ –°–¢–ê–¢–ò–°–¢–ò–ß–ù–ò–ô –ê–ù–ê–õ–Ü–ó –ó–ù–ê–ß–£–©–û–°–¢–Ü")  
        print("="*50)  
        
        summary_df = pd.read_csv(f"{results_dir}/experiment_summary.csv")  
        
        if 'rmse_fe' not in summary_df.columns or 'model' not in summary_df.columns:  
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ–≥–æ —Ç–µ—Å—Ç—É")  
            return  
        
        models = summary_df['model'].unique()  
        if len(models) < 2:  
            print("‚ùå –ü–æ—Ç—Ä—ñ–±–Ω–æ –ø—Ä–∏–Ω–∞–π–º–Ω—ñ 2 –º–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è")  
            return  
        
        # –ü–∞—Ä–Ω—ñ t-—Ç–µ—Å—Ç–∏ –º—ñ–∂ –º–æ–¥–µ–ª—è–º–∏  
        print("üî¨ –ü–ê–†–ù–Ü T-–¢–ï–°–¢–ò (RMSE Fe):")  
        results_matrix = []  
        
        for i, model1 in enumerate(models):  
            row = []  
            for j, model2 in enumerate(models):  
                if i == j:  
                    row.append("-")  
                else:  
                    data1 = summary_df[summary_df['model'] == model1]['rmse_fe']  
                    data2 = summary_df[summary_df['model'] == model2]['rmse_fe']  
                    
                    if len(data1) >= 2 and len(data2) >= 2:  
                        t_stat, p_value = stats.ttest_ind(data1, data2)  
                        significance = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"  
                        row.append(f"{p_value:.4f} {significance}")  
                    else:  
                        row.append("N/A")  
            results_matrix.append(row)  
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞–±–ª–∏—Ü—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤  
        results_df = pd.DataFrame(results_matrix, index=models, columns=models)  
        print(results_df)  
        
        print("\nüìä –ü–æ–∑–Ω–∞—á–µ–Ω–Ω—è: *** p<0.001, ** p<0.01, * p<0.05, ns p‚â•0.05")  
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑  
        results_df.to_csv(f"{results_dir}/statistical_analysis.csv")  
        
    except ImportError:  
        print("‚ö†Ô∏è scipy –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É")  
    except Exception as e:  
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É: {e}")  


# # üéØ –ì–û–õ–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø –ó–ê–ü–£–°–ö–£  
# if __name__ == '__main__':  
#     print("üöÄ –ó–ê–ü–£–°–ö –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–£ –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ú–û–î–ï–õ–ï–ô MPC")  
#     print("="*70)  
    
#     # –ó–∞–ø—É—Å–∫–∞—î–º–æ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç  
#     try:  
#         all_results, comparison_table = main_experiment()  
        
#         # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏  
#         results_dirs = [d for d in os.listdir('.') if d.startswith('model_comparison_experiment_')]  
#         if results_dirs:  
#             latest_dir = max(results_dirs)  
            
#             print(f"\nüìä –§–Ü–ù–ê–õ–¨–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò:")  
#             print("="*70)  
            
#             # –ü–æ–∫–∞–∑—É—î–º–æ –ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω—É —Ç–∞–±–ª–∏—Ü—é  
#             print("üèÜ –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô:")  
#             print(comparison_table[['Model', 'RMSE_Fe_Mean', 'RMSE_Fe_Std', 'R2_Fe_Mean', 'Runtime_Mean']].round(4))  
            
#             # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑  
#             statistical_significance_test(latest_dir)  
            
#             # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó  
#             print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á:")  
#             best_model = comparison_table.iloc[0]['Model']  
#             best_rmse = comparison_table.iloc[0]['RMSE_Fe_Mean']  
#             best_r2 = comparison_table.iloc[0]['R2_Fe_Mean']  
            
#             print(f"ü•á –ù–∞–π–∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_model}")  
#             print(f"   RMSE Fe: {best_rmse:.4f}")  
#             print(f"   R¬≤ Fe: {best_r2:.4f}")  
            
#             if len(comparison_table) > 1:  
#                 second_best = comparison_table.iloc[1]  
#                 improvement = ((second_best['RMSE_Fe_Mean'] - best_rmse) / second_best['RMSE_Fe_Mean'] * 100)  
#                 print(f"   –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤—ñ–¥–Ω–æ—Å–Ω–æ –¥—Ä—É–≥–æ–≥–æ –º—ñ—Å—Ü—è: {improvement:.2f}%")  
            
#             print(f"\nüìÅ –í—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {latest_dir}")  
#             print(f"üìä –§–∞–π–ª–∏:")  
#             print(f"   - model_comparison.csv (–ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∞ —Ç–∞–±–ª–∏—Ü—è)")  
#             print(f"   - experiment_summary.csv (–¥–µ—Ç–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏)")  
#             print(f"   - experiment_analysis.png (–≥—Ä–∞—Ñ—ñ–∫–∏)")  
#             print(f"   - statistical_analysis.csv (—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑)")  
#             print(f"   - individual *_results.parquet (—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–æ–∂–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫—É)")  
            
#     except Exception as e:  
#         print(f"‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê: {e}")
#         import traceback
#         traceback.print_exc()
        
#     print("\nüéâ –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ó–ê–í–ï–†–®–ï–ù–û!")
#     print("="*70)


# üîß –î–û–î–ê–¢–ö–û–í–Ü –£–¢–ò–õ–Ü–¢–ò –î–õ–Ø –ê–ù–ê–õ–Ü–ó–£ –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í

def load_and_compare_experiments(experiment_dirs: List[str]) -> pd.DataFrame:
    """–ü–æ—Ä—ñ–≤–Ω—é—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫—ñ–ª—å–∫–æ—Ö –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤"""
    
    print("üîç –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–Ü–í")
    print("="*50)
    
    all_comparisons = []
    
    for exp_dir in experiment_dirs:
        try:
            comparison_file = f"{exp_dir}/model_comparison.csv"
            if os.path.exists(comparison_file):
                df = pd.read_csv(comparison_file)
                df['Experiment'] = exp_dir
                all_comparisons.append(df)
                print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ: {exp_dir}")
            else:
                print(f"‚ùå –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {comparison_file}")
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {exp_dir}: {e}")
    
    if all_comparisons:
        combined_df = pd.concat(all_comparisons, ignore_index=True)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–≤–µ–¥–µ–Ω—É —Ç–∞–±–ª–∏—Ü—é
        pivot_table = combined_df.pivot_table(
            index='Model',
            columns='Experiment', 
            values='RMSE_Fe_Mean',
            aggfunc='mean'
        )
        
        print(f"\nüìä –ó–í–ï–î–ï–ù–ê –¢–ê–ë–õ–ò–¶–Ø RMSE Fe:")
        print(pivot_table.round(4))
        
        return combined_df
    else:
        print("‚ùå –ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è")
        return pd.DataFrame()


def generate_experiment_report(results_dir: str, output_file: str = None) -> str:
    """–ì–µ–Ω–µ—Ä—É—î –¥–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç –ø—Ä–æ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç"""
    
    if output_file is None:
        output_file = f"{results_dir}/experiment_report.md"
    
    try:
        comparison_df = pd.read_csv(f"{results_dir}/model_comparison.csv")
        summary_df = pd.read_csv(f"{results_dir}/experiment_summary.csv")
        
        report = f"""# üî¨ –ó–≤—ñ—Ç –ø—Ä–æ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π MPC

## üìä –ó–∞–≥–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
- **–î–∞—Ç–∞ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- **–ö—ñ–ª—å–∫—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π:** {len(comparison_df)}
- **–ü–æ–≤—Ç–æ—Ä–µ–Ω—å –Ω–∞ –º–æ–¥–µ–ª—å:** {summary_df['model'].value_counts().iloc[0] if len(summary_df) > 0 else 'N/A'}
- **–ó–∞–≥–∞–ª–æ–º –∑–∞–ø—É—Å–∫—ñ–≤:** {len(summary_df)}

## üèÜ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏

### –†–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π (–ø–æ RMSE Fe):
"""
        
        for idx, row in comparison_df.iterrows():
            rank = idx + 1
            medal = "ü•á" if rank == 1 else "ü•à" if rank == 2 else "ü•â" if rank == 3 else f"{rank}."
            
            report += f"""
{medal} **{row['Model']}**
- RMSE Fe: {row['RMSE_Fe_Mean']:.4f} ¬± {row['RMSE_Fe_Std']:.4f}
- RMSE Mass: {row['RMSE_Mass_Mean']:.4f} ¬± {row['RMSE_Mass_Std']:.4f}
- R¬≤ Fe: {row['R2_Fe_Mean']:.4f} ¬± {row['R2_Fe_Std']:.4f}
- R¬≤ Mass: {row['R2_Mass_Mean']:.4f} ¬± {row['R2_Mass_Std']:.4f}
- –ß–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {row['Runtime_Mean']:.1f}—Å ¬± {row['Runtime_Std']:.1f}—Å
"""
        
        # –ê–Ω–∞–ª—ñ–∑ —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        report += f"""
## üìà –ê–Ω–∞–ª—ñ–∑ —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ

### –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –≤–∞—Ä—ñ–∞—Ü—ñ—ó (CV = std/mean):
"""
        
        for idx, row in comparison_df.iterrows():
            cv_rmse = (row['RMSE_Fe_Std'] / row['RMSE_Fe_Mean']) * 100
            cv_r2 = (row['R2_Fe_Std'] / row['R2_Fe_Mean']) * 100 if row['R2_Fe_Mean'] > 0 else float('inf')
            
            stability = "–î—É–∂–µ —Å—Ç–∞–±—ñ–ª—å–Ω–∞" if cv_rmse < 5 else "–°—Ç–∞–±—ñ–ª—å–Ω–∞" if cv_rmse < 10 else "–ù–µ—Å—Ç–∞–±—ñ–ª—å–Ω–∞"
            
            report += f"- **{row['Model']}**: CV_RMSE = {cv_rmse:.2f}% ({stability})\n"
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        best_model = comparison_df.iloc[0]
        report += f"""
## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

### üéØ –ù–∞–π–∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_model['Model']}
- –ù–∞–π–Ω–∏–∂—á–∏–π RMSE Fe: {best_model['RMSE_Fe_Mean']:.4f}
- –í–∏—Å–æ–∫–∏–π R¬≤: {best_model['R2_Fe_Mean']:.4f}
- {'–®–≤–∏–¥–∫–∏–π' if best_model['Runtime_Mean'] < 60 else '–ü–æ–≤—ñ–ª—å–Ω–∏–π'} —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {best_model['Runtime_Mean']:.1f}—Å

### üìä –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ–Ω—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏:
"""
        
        if len(comparison_df) > 1:
            for idx in range(1, min(4, len(comparison_df))):  # –¢–æ–ø-3 –ø—ñ—Å–ª—è –Ω–∞–π–∫—Ä–∞—â–æ—ó
                model = comparison_df.iloc[idx]
                improvement = ((model['RMSE_Fe_Mean'] - best_model['RMSE_Fe_Mean']) / model['RMSE_Fe_Mean'] * 100)
                report += f"- –ö—Ä–∞—â–µ –∑–∞ {model['Model']} –Ω–∞ {improvement:.2f}%\n"
        
        # –¢–µ—Ö–Ω—ñ—á–Ω—ñ –¥–µ—Ç–∞–ª—ñ
        report += f"""
## üîß –¢–µ—Ö–Ω—ñ—á–Ω—ñ –¥–µ—Ç–∞–ª—ñ

### –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É:
- –ë–∞–∑–æ–≤–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è: `oleksandr_original`
- Seeds –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ: 0, 42, 84, 126, 168 (–¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ)
- –ê–Ω–∞–ª—ñ–∑ –≤—ñ–¥–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ

### –§–∞–π–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤:
- `model_comparison.csv` - –ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∞ —Ç–∞–±–ª–∏—Ü—è
- `experiment_summary.csv` - –¥–µ—Ç–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤—Å—ñ—Ö –∑–∞–ø—É—Å–∫—ñ–≤
- `experiment_analysis.png` - –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
- `statistical_analysis.csv` - —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∑–Ω–∞—á—É—â–æ—Å—Ç—ñ
- `*_results.parquet` - —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ–∫—Ä–µ–º–∏—Ö —Å–∏–º—É–ª—è—Ü—ñ–π
- `*_metrics.json` - –º–µ—Ç—Ä–∏–∫–∏ –æ–∫—Ä–µ–º–∏—Ö —Å–∏–º—É–ª—è—Ü—ñ–π

## üìù –í–∏—Å–Ω–æ–≤–∫–∏

1. **–ù–∞–π–∫—Ä–∞—â–∏–π –≤–∏–±—ñ—Ä:** {best_model['Model']} –ø–æ–∫–∞–∑—É—î –Ω–∞–π–∫—Ä–∞—â—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç—ñ
2. **–°—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å:** –í—Å—ñ –º–æ–¥–µ–ª—ñ –ø–æ–∫–∞–∑—É—é—Ç—å {'—Å—Ç–∞–±—ñ–ª—å–Ω—ñ' if comparison_df['RMSE_Fe_Std'].max() < 0.1 else '–≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ'} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
3. **–®–≤–∏–¥–∫–æ–¥—ñ—è:** {'GPR —Ç–∞ SVR' if any('GPR' in m or 'SVR' in m for m in comparison_df['Model']) else '–õ—ñ–Ω—ñ–π–Ω—ñ –º–æ–¥–µ–ª—ñ'} –ø–æ—Ç—Ä–µ–±—É—é—Ç—å –±—ñ–ª—å—à–µ —á–∞—Å—É –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
4. **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ {best_model['Model']} –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω —Å–∏—Å—Ç–µ–º–∏

---
*–ó–≤—ñ—Ç –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–≤—ñ—Ç
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"üìÑ –ó–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {output_file}")
        return report
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑–≤—ñ—Ç—É: {e}")
        return ""

# üéØ –®–í–ò–î–ö–ò–ô –ó–ê–ü–£–°–ö (–î–õ–Ø –¢–ï–°–¢–£–í–ê–ù–ù–Ø)
def quick_test_experiment():
    """–®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É –∑ –º–µ–Ω—à–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö"""
    
    print("‚ö° –®–í–ò–î–ö–ò–ô –¢–ï–°–¢ –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–£")
    
    try:
        hist_df = pd.read_parquet('processed.parquet')
    except:
        try:
            hist_df = pd.read_parquet('/content/KModel/src/processed.parquet')
        except Exception as e:
            print(f"‚ùå –ù–µ –º–æ–∂—É –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ: {e}")
            return
    
    # –¢–µ—Å—Ç—É—î–º–æ —Ç—ñ–ª—å–∫–∏ 2 –º–æ–¥–µ–ª—ñ –ø–æ 2 —Ä–∞–∑–∏ –∑ –º–µ–Ω—à–∏–º–∏ –¥–∞–Ω–∏–º–∏
    models_config = {
        'KRR_RBF_TEST': {
            'model_type': 'krr',
            'kernel': 'rbf',
            'N_data': 2000,
            'control_pts': 200,
            'find_optimal_params': True
        },
        'LINEAR_TEST': {
            'model_type': 'linear',
            'linear_type': 'ridge',
            'N_data': 2000,
            'control_pts': 200,
            'find_optimal_params': True
        }
    }
    
    results = {}
    for model_name, params in models_config.items():
        print(f"\nüß™ –¢–µ—Å—Ç—É—î–º–æ {model_name}...")
        
        try:
            results_df, metrics = simulate_mpc(
                hist_df,
                config='oleksandr_original',
                config_overrides=params,
                run_analysis=False
            )
            
            results[model_name] = {
                'rmse_fe': metrics.get('rmse_fe', 0),
                'rmse_mass': metrics.get('rmse_mass', 0),
                'r2_fe': metrics.get('r2_fe', 0)
            }
            
            print(f"   ‚úÖ RMSE Fe: {metrics.get('rmse_fe', 0):.4f}")
            
        except Exception as e:
            print(f"   ‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
    
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–ò –®–í–ò–î–ö–û–ì–û –¢–ï–°–¢–£:")
    for model, metrics in results.items():
        print(f"   {model}: RMSE_Fe={metrics['rmse_fe']:.4f}, R¬≤={metrics['r2_fe']:.4f}")
    
    return results

# ---- –ê–ù–ê–õ–Ü–ó –¢–ê –¢–ï–°–¢–£–í–ê–ù–ù–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í

# üîç –®–í–ò–î–ö–ê –ü–ï–†–ï–í–Ü–†–ö–ê –ü–û–í–ï–†–ù–ï–ù–ù–Ø –ú–ï–¢–†–ò–ö
def quick_metrics_check():
    """–®–≤–∏–¥–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —á–∏ –º–µ—Ç—Ä–∏–∫–∏ –≤–∑–∞–≥–∞–ª—ñ –ø–æ–≤–µ—Ä—Ç–∞—é—Ç—å—Å—è –∑ simulate_mpc"""
    
    try:
        hist_df = pd.read_parquet('processed.parquet')
    except:
        hist_df = pd.read_parquet('/content/KModel/src/processed.parquet')
    
    print("üîç –®–í–ò–î–ö–ê –ü–ï–†–ï–í–Ü–†–ö–ê –ú–ï–¢–†–ò–ö")
    print("="*40)
    
    # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    config = {
        'model_type': 'krr',
        'kernel': 'rbf',
        'N_data': 500,
        'control_pts': 20,
        'find_optimal_params': False
    }
    
    # –í–∏–∫–ª–∏–∫–∞—î–º–æ simulate_mpc
    results_df, metrics = simulate_mpc(
        hist_df,
        config='oleksandr_original',
        config_overrides=config,
        run_analysis=False
    )
    
    print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢ –í–ò–ö–õ–ò–ö–£:")
    print(f"   results_df: {type(results_df)} shape={getattr(results_df, 'shape', 'N/A')}")
    print(f"   metrics: {type(metrics)}")
    
    if isinstance(metrics, dict):
        print(f"   metrics –º–∞—î {len(metrics)} –∫–ª—é—á—ñ–≤")
        
        # –ü–æ–∫–∞–∑—É—î–º–æ –≤—Å—ñ –∫–ª—é—á—ñ —ñ –∑–Ω–∞—á–µ–Ω–Ω—è
        print(f"\nüìã –í–°–Ü –ú–ï–¢–†–ò–ö–ò:")
        for key, value in metrics.items():
            print(f"   {key}: {value} ({type(value)})")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —î RMSE
        rmse_keys = [k for k in metrics.keys() if 'rmse' in k.lower()]
        if rmse_keys:
            print(f"\nüéØ –ó–ù–ê–ô–î–ï–ù–Ü RMSE –ö–õ–Æ–ß–Ü:")
            for key in rmse_keys:
                print(f"   {key} = {metrics[key]}")
                return metrics[key]  # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø–µ—Ä—à–µ RMSE –∑–Ω–∞—á–µ–Ω–Ω—è
        else:
            print(f"\n‚ùå RMSE –ö–õ–Æ–ß–Ü –ù–ï –ó–ù–ê–ô–î–ï–ù–Ü!")
            print(f"–î–æ—Å—Ç—É–ø–Ω—ñ –∫–ª—é—á—ñ: {list(metrics.keys())}")
    
    else:
        print(f"   ‚ùå metrics –Ω–µ —î —Å–ª–æ–≤–Ω–∏–∫–æ–º!")
    
    return metrics

# –°–ü–û–ß–ê–¢–ö–£ –ó–ê–ü–£–°–ö–ê–ô –¶–ï
if __name__ == '__main__':
    # quick_test_experiment()
    quick_metrics_check()