# linear_models_comparison_service.py

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import json
import time
from typing import Dict, Any, List, Optional, Tuple
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import cross_val_score
from pathlib import Path
from datetime import datetime
import seaborn as sns
from scipy import stats
from statsmodels.stats.diagnostic import het_breuschpagan, acorr_ljungbox

from data_gen import StatefulDataGenerator
from model import KernelModel


class LinearModelsComparisonService:
    """
    –°–µ—Ä–≤—ñ—Å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
    –∑ –≥–Ω—É—á–∫–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é –¥–ª—è –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –æ–±–º–µ–∂–µ–Ω—å ARX –ø—ñ–¥—Ö–æ–¥—É.
    
    –û—Å–Ω–æ–≤–Ω—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ:
    - –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π (OLS, Ridge, Lasso, ARMAX)
    - –ê–Ω–∞–ª—ñ–∑ –≤–ø–ª–∏–≤—É –ª–∞–≥–æ–≤–æ—ó —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
    - –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤ —Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω—ñ —Ç–µ—Å—Ç–∏
    - –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ –¥–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ —Ç–∞ —à—É–º—É
    - –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤—ñ—Ç—ñ–≤ —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó
    """
    
    def __init__(self, reference_df: Optional[pd.DataFrame] = None, 
                 output_dir: Optional[str] = None):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–µ—Ä–≤—ñ—Å—É –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π.
        
        Args:
            reference_df: –†–µ—Ñ–µ—Ä–µ–Ω—Ç–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è —Å–∏–º—É–ª—è—Ü—ñ—ó
            output_dir: –ë–∞–∑–æ–≤–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        """
        self.reference_df = reference_df
        self.output_dir = Path(output_dir) if output_dir else Path("linear_comparison_results")
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ–π
        self.dirs = {
            'main': self.output_dir,
            'plots': self.output_dir / 'plots',
            'data': self.output_dir / 'data', 
            'reports': self.output_dir / 'reports',
            'latex': self.output_dir / 'latex',
            'diagnostics': self.output_dir / 'diagnostics'
        }
        
        for dir_path in self.dirs.values():
            dir_path.mkdir(parents=True, exist_ok=True)
        
        self.results = {}
        self.models = {}
        self.scaler_x = StandardScaler()
        self.scaler_y = StandardScaler()
        
    def run_comprehensive_comparison(self, 
                                   model_configs: List[Dict[str, Any]],
                                   global_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π.
        
        Args:
            model_configs: –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π –¥–ª—è –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ
                –ü—Ä–∏–∫–ª–∞–¥: [
                    {'name': 'ARX_OLS', 'linear_type': 'ols', 'poly_degree': 1},
                    {'name': 'ARX_Ridge', 'linear_type': 'ridge', 'alpha': 0.1},
                    {'name': 'ARX_Lasso', 'linear_type': 'lasso', 'alpha': 0.01}
                ]
            global_config: –ì–ª–æ–±–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ (–¥–∞–Ω—ñ, —Ä–æ–∑–±–∏—Ç—Ç—è, —Å–∏–º—É–ª—è—Ü—ñ—è)
                
        Returns:
            Dict –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        """
        print("üöÄ –ó–ê–ü–£–°–ö –ö–û–ú–ü–õ–ï–ö–°–ù–û–ì–û –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –õ–Ü–ù–Ü–ô–ù–ò–• –ú–û–î–ï–õ–ï–ô")
        print("=" * 60)
        
        # 1. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
        data_results = self._prepare_data(global_config)
        X_train, Y_train, X_val, Y_val, X_test, Y_test, X_test_unscaled, Y_test_scaled = data_results
        
        # 2. –ù–∞–≤—á–∞–Ω–Ω—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π
        self.models = {}
        training_results = {}
        
        for config in model_configs:
            model_name = config['name']
            print(f"\nüìö –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {model_name}")
            print("-" * 40)
            
            model_results = self._train_single_model(
                config, X_train, Y_train, X_val, Y_val
            )
            
            self.models[model_name] = model_results['model']
            training_results[model_name] = model_results['metrics']
        
        # 3. –û—Ü—ñ–Ω–∫–∞ –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        evaluation_results = self._evaluate_all_models(
            X_test, Y_test, training_results
        )
        
        # 4. –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤
        diagnostics_results = self._run_residual_diagnostics(
            X_test, Y_test
        )
        
        # 5. –ê–Ω–∞–ª—ñ–∑ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ
        robustness_results = self._run_robustness_analysis(
            global_config, model_configs
        )
        
        # 6. –ó–±—ñ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        self.results = {
            'models_config': model_configs,
            'global_config': global_config,
            'training_results': training_results,
            'evaluation_results': evaluation_results,
            'diagnostics_results': diagnostics_results,
            'robustness_results': robustness_results,
            'data_info': {
                'train_size': X_train.shape[0],
                'val_size': X_val.shape[0] if X_val is not None else 0,
                'test_size': X_test.shape[0],
                'n_features': X_train.shape[1]
            }
        }
        
        # 7. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤—ñ—Ç—ñ–≤ —Ç–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π
        self._generate_comprehensive_report()
        
        return self.results
    
    def _prepare_data(self, global_config: Dict[str, Any]) -> Tuple[np.ndarray, ...]:
        """
        –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô –º–µ—Ç–æ–¥ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–∏—Ö –∑–≥—ñ–¥–Ω–æ –∑ –≥–ª–æ–±–∞–ª—å–Ω–æ—é –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é.
        
        –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø:
        - –ê–∫—Ç–∏–≤–æ–≤–∞–Ω–æ –∞–Ω–æ–º–∞–ª—ñ—ó –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º (use_anomalies=True)
        - –î–æ–¥–∞–Ω–æ —à—É–º –¥–æ –¥–∞–Ω–∏—Ö (noise_level='mild')
        - –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ –ø–µ—Ä–µ–¥–∞—á—É –∞–Ω–æ–º–∞–ª—ñ–π –≤ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç
        - –£–∑–≥–æ–¥–∂–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ 'N_data' vs 'T'
        """
        
        if global_config.get('use_simulation', True):
            print("üìÑ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö –∑ –ê–ù–û–ú–ê–õ–Ü–Ø–ú–ò —Ç–∞ –®–£–ú–û–ú...")
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –¥–∞–Ω–∏—Ö (–ø—Ä–∞–≤–∏–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤)
            data_gen = StatefulDataGenerator(
                reference_df=self.reference_df,
                ore_flow_var_pct=3.0,
                time_step_s=global_config.get('time_step_s', 5),
                time_constants_s={
                    'concentrate_fe_percent': 300,
                    'tailings_fe_percent': 400,
                    'concentrate_mass_flow': 600,
                    'tailings_mass_flow': 700,
                    'default': 500
                },
                dead_times_s={
                    'concentrate_fe_percent': 60,
                    'tailings_fe_percent': 80,
                    'concentrate_mass_flow': 120,
                    'tailings_mass_flow': 140,
                    'default': 90
                },
                true_model_type=global_config.get('plant_model_type', 'rf'),
                seed=global_config.get('seed', 42)
            )
            
            # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –∞–Ω–æ–º–∞–ª—ñ–π (—Ç–µ–ø–µ—Ä TRUE –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
            anomaly_cfg = None
            if global_config.get('use_anomalies', True):  # –í–ò–ü–†–ê–í–õ–ï–ù–û: True –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
                anomaly_cfg = self._create_anomaly_config(global_config)
                print(f"   üî¥ –ê–Ω–æ–º–∞–ª—ñ—ó –ê–ö–¢–ò–í–û–í–ê–ù–Ü: {len(anomaly_cfg) if anomaly_cfg else 0} –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π")
            else:
                print("   ‚ö™ –ê–Ω–æ–º–∞–ª—ñ—ó –í–Ü–î–ö–õ–Æ–ß–ï–ù–Ü")
            
            # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –£–∑–≥–æ–¥–∂–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ N_data vs T
            n_data = global_config.get('N_data', global_config.get('T', 5000))
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –±–∞–∑–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
            df_base = data_gen.generate(
                T=n_data,  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ N_data
                control_pts=global_config.get('control_pts', 500),
                n_neighbors=global_config.get('n_neighbors', 5),
                noise_level=global_config.get('noise_level', 'mild'),  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –¥–æ–¥–∞–Ω–æ —à—É–º
                anomaly_config=anomaly_cfg
            )
            
            # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ü—Ä–∞–≤–∏–ª—å–Ω–∞ –ø–µ—Ä–µ–¥–∞—á–∞ –∞–Ω–æ–º–∞–ª—ñ–π –≤ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç
            if global_config.get('enable_nonlinear', False):
                print("   üîÑ –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π...")
                df = data_gen.generate_nonlinear_variant(
                    base_df=df_base,
                    non_linear_factors=global_config.get('nonlinear_config', {}),
                    noise_level='mild',  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –¥–æ–¥–∞–Ω–æ —à—É–º –≤ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç
                    anomaly_config=anomaly_cfg  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ø–µ—Ä–µ–¥–∞—î–º–æ –∞–Ω–æ–º–∞–ª—ñ—ó!
                )
                print(f"   üìà –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω–∞: {global_config.get('nonlinear_config', {})}")
            else:
                df = df_base
                print("   üìä –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ª—ñ–Ω—ñ–π–Ω–∞ –º–æ–¥–µ–ª—å")
            
        else:
            print("üìÅ –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–∞–¥–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö...")
            if self.reference_df is None:
                raise ValueError("–î–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —Ä–µ–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –ø–æ—Ç—Ä—ñ–±–µ–Ω reference_df")
            df = self.reference_df.copy()
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ª–∞–≥–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
        lag_depth = global_config.get('lag_depth', 3)
        X, Y = self._create_lag_features(df, lag_depth)
        
        print(f"   üéØ –õ–∞–≥–æ–≤–∞ –≥–ª–∏–±–∏–Ω–∞: {lag_depth}")
        print(f"   üìä –†–æ–∑–º—ñ—Ä –¥–∞–Ω–∏—Ö –ø—ñ—Å–ª—è –ª–∞–≥—É–≤–∞–Ω–Ω—è: X={X.shape}, Y={Y.shape}")
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –î–æ–¥–∞–Ω–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–∞–Ω–∏—Ö
        print(f"   üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Y: min={Y.min():.3f}, max={Y.max():.3f}, std={Y.std():.3f}")
        
        # –†–æ–∑–±–∏—Ç—Ç—è –¥–∞–Ω–∏—Ö
        train_size = global_config.get('train_size', 0.8)
        val_size = global_config.get('val_size', 0.1)
        
        n_train = int(len(X) * train_size)
        n_val = int(len(X) * val_size)
        
        X_train, Y_train = X[:n_train], Y[:n_train]
        X_val, Y_val = X[n_train:n_train+n_val], Y[n_train:n_train+n_val]
        X_test, Y_test = X[n_train+n_val:], Y[n_train+n_val:]
        
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
        X_train_scaled = self.scaler_x.fit_transform(X_train)
        Y_train_scaled = self.scaler_y.fit_transform(Y_train)
        
        X_val_scaled = self.scaler_x.transform(X_val) if len(X_val) > 0 else None
        Y_val_scaled = self.scaler_y.transform(Y_val) if len(Y_val) > 0 else None
        
        X_test_scaled = self.scaler_x.transform(X_test)
        Y_test_scaled = self.scaler_y.transform(Y_test)
        
        print(f"‚úÖ –î–∞–Ω—ñ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: Train={X_train.shape}, Val={X_val.shape}, Test={X_test.shape}")
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –î–æ–¥–∞–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö
        if np.allclose(Y_train, Y_train.mean(), rtol=1e-3):
            print("‚ö†Ô∏è  –£–í–ê–ì–ê: Y_train –º–∞—î –¥—É–∂–µ –º–∞–ª–æ –≤–∞—Ä—ñ–∞—Ü—ñ—ó - –º–æ–∂–ª–∏–≤–æ –ø–æ–º–∏–ª–∫–∞ –≤ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó!")
        
        if len(np.unique(Y_test.round(3))) < 10:
            print("‚ö†Ô∏è  –£–í–ê–ì–ê: Y_test –º–∞—î –¥—É–∂–µ –º–∞–ª–æ —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å!")
        
        return (X_train_scaled, Y_train_scaled, X_val_scaled, 
                Y_val_scaled, X_test_scaled, Y_test, X_test, Y_test_scaled) 
     
    def _create_anomaly_config(self, global_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô –º–µ—Ç–æ–¥ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –∞–Ω–æ–º–∞–ª—ñ–π.
        
        –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø:
        - –ì–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é –∞–Ω–æ–º–∞–ª—ñ–π —É —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        - –î–æ–¥–∞–Ω–æ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –∞–Ω–æ–º–∞–ª—ñ–π —É —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        - –ü—ñ–¥–≤–∏—â–µ–Ω–æ —ñ–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ñ—Å—Ç—å –∞–Ω–æ–º–∞–ª—ñ–π –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó –æ–±–º–µ–∂–µ–Ω—å –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
        """
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è StatefulDataGenerator.generate_anomaly_config
        N_data = global_config.get('N_data', global_config.get('T', 5000))
        train_frac = global_config.get('train_size', 0.8)
        val_frac = global_config.get('val_size', 0.1)
        test_frac = global_config.get('test_size', 0.1)
        seed = global_config.get('seed', 42)
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Å—Ç–∞—Ç–∏—á–Ω–∏–π –º–µ—Ç–æ–¥
        base_anomaly_config = StatefulDataGenerator.generate_anomaly_config(
            N_data=N_data,
            train_frac=train_frac,
            val_frac=val_frac,
            test_frac=test_frac,
            seed=seed
        )
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –î–æ–¥–∞—î–º–æ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ –∞–Ω–æ–º–∞–ª—ñ—ó –¥–ª—è –∫—Ä–∞—â–æ—ó –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó
        severity = global_config.get('anomaly_severity', 'medium')
        include_train = global_config.get('anomaly_in_train', True)  # –ó–º—ñ–Ω–µ–Ω–æ –Ω–∞ True
        
        # –†–æ–∑—Ä–∞—Ö–æ–≤—É—î–º–æ —ñ–Ω–¥–µ–∫—Å–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —á–∞—Å—Ç–∏–Ω –¥–∞–Ω–∏—Ö
        n_train = int(N_data * train_frac)
        n_val = int(N_data * val_frac)
        
        enhanced_anomaly_config = base_anomaly_config.copy() if base_anomaly_config else {}
        
        # –î–æ–¥–∞—î–º–æ –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω—ñ –∞–Ω–æ–º–∞–ª—ñ—ó —É —Ç–µ—Å—Ç–æ–≤—ñ–π —á–∞—Å—Ç–∏–Ω—ñ
        test_start = n_train + n_val
        
        if severity == 'mild':
            # 3-5 –∞–Ω–æ–º–∞–ª—ñ–π —É —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
            n_test_anomalies = 3
            anomaly_strength = 1.5
        elif severity == 'medium':
            # 5-8 –∞–Ω–æ–º–∞–ª—ñ–π —É —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
            n_test_anomalies = 6
            anomaly_strength = 2.0
        elif severity == 'strong':
            # 8-12 –∞–Ω–æ–º–∞–ª—ñ–π —É —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
            n_test_anomalies = 10
            anomaly_strength = 3.0
        else:
            n_test_anomalies = 5
            anomaly_strength = 2.0
        
        # –ì–µ–Ω–µ—Ä—É—î–º–æ –∞–Ω–æ–º–∞–ª—ñ—ó –≤ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
        np.random.seed(seed + 100)  # –û–∫—Ä–µ–º–∏–π seed –¥–ª—è –∞–Ω–æ–º–∞–ª—ñ–π
        test_indices = np.random.choice(
            range(test_start, N_data - 10), 
            size=n_test_anomalies, 
            replace=False
        )
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—Ä–∞–≤–∏–ª—å–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–Ω–æ–º–∞–ª—ñ–π –¥–ª—è StatefulDataGenerator
        if 'feed_fe_percent' not in enhanced_anomaly_config:
            enhanced_anomaly_config['feed_fe_percent'] = []
        if 'ore_mass_flow' not in enhanced_anomaly_config:
            enhanced_anomaly_config['ore_mass_flow'] = []
        
        for idx in test_indices:
            # –ê–Ω–æ–º–∞–ª—ñ—ó –¥–ª—è feed_fe_percent
            enhanced_anomaly_config['feed_fe_percent'].append({
                'start': idx,                           # –í–ò–ü–†–ê–í–õ–ï–ù–û: 'start' –∑–∞–º—ñ—Å—Ç—å 'index'
                'duration': np.random.randint(3, 8),
                'magnitude': anomaly_strength,
                'type': 'spike'                         # –î–û–î–ê–ù–û: –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–π —Ç–∏–ø
            })
            
            # –ê–Ω–æ–º–∞–ª—ñ—ó –¥–ª—è ore_mass_flow  
            enhanced_anomaly_config['ore_mass_flow'].append({
                'start': idx + 2,                       # –í–ò–ü–†–ê–í–õ–ï–ù–û: 'start' –∑–∞–º—ñ—Å—Ç—å 'index'
                'duration': np.random.randint(2, 6),
                'magnitude': anomaly_strength * 0.8,
                'type': 'spike'                         # –î–û–î–ê–ù–û: –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–π —Ç–∏–ø
            })
        
        # –î–æ–¥–∞—î–º–æ –∞–Ω–æ–º–∞–ª—ñ—ó –≤ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if include_train:
            n_train_anomalies = max(1, n_test_anomalies // 3)  # –ú–µ–Ω—à–µ –∞–Ω–æ–º–∞–ª—ñ–π —É train
            
            train_indices = np.random.choice(
                range(100, n_train - 100), 
                size=n_train_anomalies, 
                replace=False
            )
            
            # –ü–µ—Ä–µ–∫–æ–Ω—É—î–º–æ—Å—è, —â–æ –∫–ª—é—á—ñ —ñ—Å–Ω—É—é—Ç—å
            if 'feed_fe_percent' not in enhanced_anomaly_config:
                enhanced_anomaly_config['feed_fe_percent'] = []
            
            for idx in train_indices:
                # –í–ò–ü–†–ê–í–õ–ï–ù–û: –î–æ–¥–∞—î–º–æ –¥–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–ª—é—á–∞ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é —Å—Ç—Ä—É–∫—Ç—É—Ä–æ—é
                enhanced_anomaly_config['feed_fe_percent'].append({
                    'start': idx,                           # –í–ò–ü–†–ê–í–õ–ï–ù–û: 'start' –∑–∞–º—ñ—Å—Ç—å 'index'
                    'duration': np.random.randint(2, 5),
                    'magnitude': anomaly_strength * 0.7,    # –°–ª–∞–±—à—ñ –∞–Ω–æ–º–∞–ª—ñ—ó –≤ train
                    'type': 'spike'                         # –î–û–î–ê–ù–û: –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–π —Ç–∏–ø
                })
            
            print(f"   üî¥ –î–æ–¥–∞–Ω–æ {n_train_anomalies} –∞–Ω–æ–º–∞–ª—ñ–π —É —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ")
        
        print(f"   üî¥ –î–æ–¥–∞–Ω–æ {n_test_anomalies * 2} –∞–Ω–æ–º–∞–ª—ñ–π —É —Ç–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ")  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∞–≤–∏–ª—å–Ω–∏–π –ø—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫
        print(f"   üìä –†—ñ–≤–µ–Ω—å –∞–Ω–æ–º–∞–ª—ñ–π: {severity} (—Å–∏–ª–∞: {anomaly_strength})")
        
        return enhanced_anomaly_config
    
    
    def diagnose_data_quality(self, X: np.ndarray, Y: np.ndarray, 
                             df_original: pd.DataFrame = None) -> Dict[str, Any]:
        """
        –î—ñ–∞–≥–Ω–æ—Å—Ç–∏—á–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –≤–∏—è–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º –∑ —è–∫—ñ—Å—Ç—é –¥–∞–Ω–∏—Ö.
        
        –ü–µ—Ä–µ–≤—ñ—Ä—è—î:
        - –ù–∞—è–≤–Ω—ñ—Å—Ç—å –≤–∞—Ä—ñ–∞—Ü—ñ—ó –≤ –¥–∞–Ω–∏—Ö
        - –ü—Ä–∏—Å—É—Ç–Ω—ñ—Å—Ç—å –∞–Ω–æ–º–∞–ª—ñ–π
        - –ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ–π
        - –Ø–∫—ñ—Å—Ç—å —Ä–æ–∑–±–∏—Ç—Ç—è –Ω–∞ train/test
        """
        
        print("üîç –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê –Ø–ö–û–°–¢–Ü –î–ê–ù–ò–•")
        print("=" * 35)
        
        diagnostics = {
            'data_variability': {},
            'anomaly_presence': {},
            'nonlinearity_check': {},
            'data_quality_score': 0.0,
            'warnings': []
        }
        
        # 1. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞—Ä—ñ–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö
        print("\nüìä –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞—Ä—ñ–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö:")
        
        for i in range(Y.shape[1]):
            col_name = f'Y_col_{i}'
            y_col = Y[:, i]
            
            std_val = np.std(y_col)
            coef_var = std_val / (np.mean(y_col) + 1e-12)
            unique_ratio = len(np.unique(y_col.round(6))) / len(y_col)
            
            diagnostics['data_variability'][col_name] = {
                'std': std_val,
                'coef_variation': coef_var,
                'unique_ratio': unique_ratio,
                'range': [np.min(y_col), np.max(y_col)]
            }
            
            print(f"   {col_name}: std={std_val:.4f}, CV={coef_var:.4f}, unique={unique_ratio:.3f}")
            
            # –í–∏—è–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º
            if coef_var < 0.01:
                warning = f"–î–£–ñ–ï –ú–ê–õ–ê –í–ê–†–Ü–ê–¶–Ü–Ø –≤ {col_name} (CV={coef_var:.6f})"
                diagnostics['warnings'].append(warning)
                print(f"   ‚ö†Ô∏è  {warning}")
                
            if unique_ratio < 0.1:
                warning = f"–î–£–ñ–ï –ú–ê–õ–û –£–ù–Ü–ö–ê–õ–¨–ù–ò–• –ó–ù–ê–ß–ï–ù–¨ –≤ {col_name} ({unique_ratio:.3f})"
                diagnostics['warnings'].append(warning)
                print(f"   ‚ö†Ô∏è  {warning}")
        
        # 2. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª—ñ–π
        print("\nüî¥ –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –∞–Ω–æ–º–∞–ª—ñ–π:")
        
        for i in range(Y.shape[1]):
            col_name = f'Y_col_{i}'
            y_col = Y[:, i]
            
            # –í–∏—è–≤–ª–µ–Ω–Ω—è –≤–∏–∫–∏–¥—ñ–≤ –º–µ—Ç–æ–¥–æ–º IQR
            q1, q3 = np.percentile(y_col, [25, 75])
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            outliers = np.where((y_col < lower_bound) | (y_col > upper_bound))[0]
            outlier_ratio = len(outliers) / len(y_col)
            
            diagnostics['anomaly_presence'][col_name] = {
                'n_outliers': len(outliers),
                'outlier_ratio': outlier_ratio,
                'outlier_indices': outliers.tolist()[:20]  # –¢—ñ–ª—å–∫–∏ –ø–µ—Ä—à—ñ 20
            }
            
            print(f"   {col_name}: {len(outliers)} –≤–∏–∫–∏–¥—ñ–≤ ({outlier_ratio:.3f})")
            
            if outlier_ratio < 0.01:
                warning = f"–î–£–ñ–ï –ú–ê–õ–û –ê–ù–û–ú–ê–õ–Ü–ô –≤ {col_name} ({outlier_ratio:.4f})"
                diagnostics['warnings'].append(warning)
                print(f"   ‚ö†Ô∏è  {warning}")
        
        # 3. –ó–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ
        quality_score = 100.0
        
        # –ó–º–µ–Ω—à—É—î–º–æ –±–∞–ª –∑–∞ –∫–æ–∂–Ω—É –ø—Ä–æ–±–ª–µ–º—É
        for warning in diagnostics['warnings']:
            if '–î–£–ñ–ï –ú–ê–õ–ê –í–ê–†–Ü–ê–¶–Ü–Ø' in warning:
                quality_score -= 30
            elif '–î–£–ñ–ï –ú–ê–õ–û –£–ù–Ü–ö–ê–õ–¨–ù–ò–•' in warning:
                quality_score -= 20
            elif '–î–£–ñ–ï –ú–ê–õ–û –ê–ù–û–ú–ê–õ–Ü–ô' in warning:
                quality_score -= 25
            elif '–°–õ–ê–ë–ö–ê –ù–ï–õ–Ü–ù–Ü–ô–ù–Ü–°–¢–¨' in warning:
                quality_score -= 15
        
        diagnostics['data_quality_score'] = max(0, quality_score)
        
        print(f"\nüéØ –ó–ê–ì–ê–õ–¨–ù–ê –û–¶–Ü–ù–ö–ê –Ø–ö–û–°–¢–Ü –î–ê–ù–ò–•: {diagnostics['data_quality_score']:.1f}/100")
        
        if quality_score < 50:
            print("‚ùå –ö–†–ò–¢–ò–ß–ù–Ü –ü–†–û–ë–õ–ï–ú–ò –ó –î–ê–ù–ò–ú–ò - –ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é!")
        elif quality_score < 75:
            print("‚ö†Ô∏è  –ü–û–ú–Ü–†–ù–Ü –ü–†–û–ë–õ–ï–ú–ò –ó –î–ê–ù–ò–ú–ò - —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è")
        else:
            print("‚úÖ –Ø–ö–Ü–°–¢–¨ –î–ê–ù–ò–• –ü–†–ò–ô–ù–Ø–¢–ù–ê")
        
        return diagnostics    
    
    def _create_lag_features(self, df: pd.DataFrame, lag_depth: int) -> Tuple[np.ndarray, np.ndarray]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ª–∞–≥–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ –¥–ª—è ARX –º–æ–¥–µ–ª–µ–π."""
        
        # –í—Ö—ñ–¥–Ω—ñ —Ç–∞ –≤–∏—Ö—ñ–¥–Ω—ñ –∑–º—ñ–Ω–Ω—ñ (–±–∞–∑—É—é—á–∏—Å—å –Ω–∞ –≤–∞—à—ñ–π –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó)
        input_cols = ['feed_fe_percent', 'ore_mass_flow', 'solid_feed_percent']
        output_cols = ['concentrate_fe_percent', 'concentrate_mass_flow']
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ª–∞–≥–æ–≤–∏—Ö –æ–∑–Ω–∞–∫
        lag_features = []
        lag_names = []
        
        # –õ–∞–≥–∏ –¥–ª—è –≤—Ö—ñ–¥–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö (–µ–∫–∑–æ–≥–µ–Ω–Ω—ñ –≤—Ö–æ–¥–∏)
        for col in input_cols:
            for lag in range(lag_depth):
                lag_col = df[col].shift(lag)
                lag_features.append(lag_col)
                lag_names.append(f"{col}_lag_{lag}")
        
        # –õ–∞–≥–∏ –¥–ª—è –≤–∏—Ö—ñ–¥–Ω–∏—Ö –∑–º—ñ–Ω–Ω–∏—Ö (–∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—ñ–π–Ω—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏)  
        for col in output_cols:
            for lag in range(1, lag_depth + 1):  # –ü–æ—á–∏–Ω–∞—î–º–æ –∑ 1, –±–æ y_t –Ω–µ –º–æ–∂–µ –∑–∞–ª–µ–∂–∞—Ç–∏ –≤—ñ–¥ y_t
                lag_col = df[col].shift(lag)
                lag_features.append(lag_col)
                lag_names.append(f"{col}_lag_{lag}")
        
        # –û–±'—î–¥–Ω–∞–Ω–Ω—è –≤ DataFrame
        lag_df = pd.concat(lag_features, axis=1, keys=lag_names)
        
        # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Ä—è–¥–∫—ñ–≤ –∑ NaN (—á–µ—Ä–µ–∑ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ª–∞–≥—ñ–≤)
        valid_idx = lag_df.dropna().index
        X = lag_df.loc[valid_idx].values
        Y = df.loc[valid_idx, output_cols].values
        
        print(f"üìä –°—Ç–≤–æ—Ä–µ–Ω–æ –ª–∞–≥–æ–≤—ñ –æ–∑–Ω–∞–∫–∏: {X.shape[1]} features, {X.shape[0]} samples")
        
        return X, Y
    
    def _train_single_model(self, config: Dict[str, Any], 
                          X_train: np.ndarray, Y_train: np.ndarray,
                          X_val: Optional[np.ndarray], Y_val: Optional[np.ndarray]) -> Dict[str, Any]:
        """–ù–∞–≤—á–∞–Ω–Ω—è –æ–¥–Ω—ñ—î—ó –ª—ñ–Ω—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ –∑–≥—ñ–¥–Ω–æ –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—î—é."""
        
        model_name = config['name']
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
        model_params = {k: v for k, v in config.items() if k != 'name'}
        
        model = KernelModel(
            model_type='linear',
            **model_params
        )
        
        # –ù–∞–≤—á–∞–Ω–Ω—è
        start_time = time.time()
        try:
            if X_val is not None and Y_val is not None:
                model.fit(X_train, Y_train, X_val=X_val, Y_val=Y_val)
            else:
                model.fit(X_train, Y_train)
        except TypeError:
            model.fit(X_train, Y_train)
        
        train_time = time.time() - start_time
        
        # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        Y_train_pred = model.predict(X_train)
        train_mse = mean_squared_error(Y_train, Y_train_pred)
        train_r2 = r2_score(Y_train, Y_train_pred)
        
        print(f"   ‚è±Ô∏è –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {train_time:.3f} —Å–µ–∫")
        print(f"   üìä Train MSE: {train_mse:.6f}")
        print(f"   üìä Train R¬≤: {train_r2:.4f}")
        
        return {
            'model': model,
            'metrics': {
                'train_time': train_time,
                'train_mse': train_mse,
                'train_r2': train_r2,
                'config': config
            }
        }
    
    def _evaluate_all_models(self, X_test: np.ndarray, Y_test: np.ndarray,
                           training_results: Dict[str, Any]) -> Dict[str, Any]:
        """–û—Ü—ñ–Ω–∫–∞ –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö."""
        
        print("\nüéØ –û–¶–Ü–ù–ö–ê –ú–û–î–ï–õ–ï–ô –ù–ê –¢–ï–°–¢–û–í–ò–• –î–ê–ù–ò–•")
        print("-" * 50)
        
        evaluation_results = {}
        
        for model_name, model in self.models.items():
            print(f"–û—Ü—ñ–Ω–∫–∞ {model_name}...")
            
            # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
            Y_pred_scaled = model.predict(X_test)
            Y_pred = self.scaler_y.inverse_transform(Y_pred_scaled)
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            mse = mean_squared_error(Y_test, Y_pred)
            rmse_fe = np.sqrt(mean_squared_error(Y_test[:, 0], Y_pred[:, 0]))
            rmse_mass = np.sqrt(mean_squared_error(Y_test[:, 1], Y_pred[:, 1]))
            mae = mean_absolute_error(Y_test, Y_pred)
            r2 = r2_score(Y_test, Y_pred)
            
            # MAPE (–∑ –∑–∞—Ö–∏—Å—Ç–æ–º –≤—ñ–¥ –¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –Ω—É–ª—å)
            mape = np.mean(np.abs((Y_test - Y_pred) / (Y_test + 1e-8))) * 100
            
            evaluation_results[model_name] = {
                'mse': mse,
                'rmse_fe': rmse_fe,
                'rmse_mass': rmse_mass,
                'mae': mae,
                'mape': mape,
                'r2': r2,
                'predictions': Y_pred,
                'train_time': training_results[model_name]['train_time']
            }
            
            print(f"   MSE: {mse:.6f}, R¬≤: {r2:.4f}")
        
        return evaluation_results
    
    def _run_residual_diagnostics(self, X_test: np.ndarray, Y_test: np.ndarray) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤ –¥–ª—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π."""
        
        print("\nüîç –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê –†–ï–ó–ò–î–£–ê–õ–Ü–í")
        print("-" * 40)
        
        diagnostics = {}
        
        for model_name, model in self.models.items():
            print(f"–î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ {model_name}...")
            
            # –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤
            Y_pred_scaled = model.predict(X_test)
            Y_pred = self.scaler_y.inverse_transform(Y_pred_scaled)
            residuals = Y_test - Y_pred
            
            # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏—á–Ω—ñ —Ç–µ—Å—Ç–∏
            model_diagnostics = {}
            
            # 1. –¢–µ—Å—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—ñ (Shapiro-Wilk –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤–∏—Ö–æ–¥—É)
            for i, output_name in enumerate(['Fe_concentration', 'Mass_flow']):
                res_i = residuals[:, i]
                
                # Shapiro-Wilk —Ç–µ—Å—Ç (–æ–±–º–µ–∂–µ–Ω–∏–π –¥–æ 5000 –∑—Ä–∞–∑–∫—ñ–≤)
                sample_size = min(len(res_i), 5000)
                sample_residuals = res_i[:sample_size]
                shapiro_stat, shapiro_p = stats.shapiro(sample_residuals)
                
                # Jarque-Bera —Ç–µ—Å—Ç
                jb_stat, jb_p = stats.jarque_bera(res_i)
                
                model_diagnostics[f'{output_name}_normality'] = {
                    'shapiro_statistic': shapiro_stat,
                    'shapiro_p_value': shapiro_p,
                    'jb_statistic': jb_stat,
                    'jb_p_value': jb_p,
                    'is_normal': shapiro_p > 0.05 and jb_p > 0.05
                }
            
            # 2. –¢–µ—Å—Ç –∞–≤—Ç–æ–∫–æ—Ä–µ–ª—è—Ü—ñ—ó (Ljung-Box)
            try:
                # –û–±'—î–¥–Ω—É—î–º–æ —Ä–µ–∑–∏–¥—É–∞–ª–∏ –¥–ª—è –∑–∞–≥–∞–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç—É
                combined_residuals = np.mean(residuals, axis=1)
                lb_result = acorr_ljungbox(combined_residuals, lags=10, return_df=True)
                
                model_diagnostics['autocorrelation'] = {
                    'ljung_box_stats': lb_result['lb_stat'].tolist(),
                    'ljung_box_p_values': lb_result['lb_pvalue'].tolist(),
                    'has_autocorr': any(lb_result['lb_pvalue'] < 0.05)
                }
            except Exception as e:
                print(f"   ‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –≤ —Ç–µ—Å—Ç—ñ –∞–≤—Ç–æ–∫–æ—Ä–µ–ª—è—Ü—ñ—ó: {e}")
                model_diagnostics['autocorrelation'] = {'error': str(e)}
            
            # 3. –¢–µ—Å—Ç –≥–µ—Ç–µ—Ä–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—ñ (Breusch-Pagan)
            try:
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–µ—Ä—à–∏–π –≤–∏—Ö—ñ–¥ –¥–ª—è —Ç–µ—Å—Ç—É
                bp_stat, bp_p, _, _ = het_breuschpagan(residuals[:, 0], X_test)
                
                model_diagnostics['heteroscedasticity'] = {
                    'breusch_pagan_stat': bp_stat,
                    'breusch_pagan_p': bp_p,
                    'is_homoscedastic': bp_p > 0.05
                }
            except Exception as e:
                print(f"   ‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –≤ —Ç–µ—Å—Ç—ñ –≥–µ—Ç–µ—Ä–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—ñ: {e}")
                model_diagnostics['heteroscedasticity'] = {'error': str(e)}
            
            # 4. –û—Å–Ω–æ–≤–Ω—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤
            model_diagnostics['residual_stats'] = {
                'mean': np.mean(residuals, axis=0).tolist(),
                'std': np.std(residuals, axis=0).tolist(), 
                'skewness': stats.skew(residuals, axis=0).tolist(),
                'kurtosis': stats.kurtosis(residuals, axis=0).tolist()
            }
            
            diagnostics[model_name] = model_diagnostics
            
            # –î—Ä—É–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            print(f"   ‚úì –ù–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å: {model_diagnostics.get('Fe_concentration_normality', {}).get('is_normal', 'Unknown')}")
            print(f"   ‚úì –ê–≤—Ç–æ–∫–æ—Ä–µ–ª—è—Ü—ñ—è: {'–Ñ' if model_diagnostics.get('autocorrelation', {}).get('has_autocorr', True) else '–ù–µ–º–∞—î'}")
            print(f"   ‚úì –ì–æ–º–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å: {model_diagnostics.get('heteroscedasticity', {}).get('is_homoscedastic', 'Unknown')}")
        
        return diagnostics
    
    def _run_robustness_analysis(self, global_config: Dict[str, Any], 
                               model_configs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª–µ–π –¥–æ —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –∑–±—É—Ä–µ–Ω—å."""
        
        print("\nüí™ –ê–ù–ê–õ–Ü–ó –†–û–ë–ê–°–¢–ù–û–°–¢–Ü")
        print("-" * 30)
        
        robustness_results = {}
        
        # 1. –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É
        noise_levels = [0.01, 0.05, 0.10, 0.20]  # 1%, 5%, 10%, 20% —à—É–º—É
        
        robustness_results['noise_robustness'] = self._test_noise_robustness(
            global_config, model_configs, noise_levels
        )
        
        # 2. –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
        nonlinearity_levels = [
            ('linear', {}),
            ('weak', {'concentrate_fe_percent': ('pow', 1.2)}),
            ('moderate', {'concentrate_fe_percent': ('pow', 1.8)}), 
            ('strong', {'concentrate_fe_percent': ('pow', 2.5)})
        ]
        
        robustness_results['nonlinearity_robustness'] = self._test_nonlinearity_robustness(
            global_config, model_configs, nonlinearity_levels
        )
        
        return robustness_results
    
    def _test_noise_robustness(self, global_config: Dict[str, Any],
                             model_configs: List[Dict[str, Any]], 
                             noise_levels: List[float]) -> Dict[str, Any]:
        """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ –¥–æ —à—É–º—É."""
        
        noise_results = {model_config['name']: {} for model_config in model_configs}
        
        for noise_level in noise_levels:
            print(f"üîä –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–∏ —Ä—ñ–≤–Ω—ñ —à—É–º—É: {noise_level*100:.1f}%")
            
            # –ú–æ–¥–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è —à—É–º—É
            noisy_config = global_config.copy()
            noisy_config['noise_level'] = 'custom'
            noisy_config['custom_noise_std'] = noise_level
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–∞—à—É–º–ª–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö (—Ç–∏–º—á–∞—Å–æ–≤–æ —Å–ø—Ä–æ—â–µ–Ω–∞ —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ—è)
            try:
                temp_results = self._prepare_data(noisy_config)
                X_train_noisy, Y_train_noisy, _, _, X_test_noisy, Y_test_noisy, _, _ = temp_results
            except Exception as e:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∑–∞—à—É–º–ª–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö: {e}")
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±–∞–∑–æ–≤—ñ –¥–∞–Ω—ñ –∑ —à—Ç—É—á–Ω–∏–º —à—É–º–æ–º
                base_results = self._prepare_data(global_config)
                X_train_base, Y_train_base, _, _, X_test_base, Y_test_base, _, _ = base_results
                
                # –î–æ–¥–∞—î–º–æ —à—É–º –≤—Ä—É—á–Ω—É
                noise_X = np.random.normal(0, noise_level, X_train_base.shape)
                noise_Y = np.random.normal(0, noise_level, Y_train_base.shape)
                
                X_train_noisy = X_train_base + noise_X
                Y_train_noisy = Y_train_base + noise_Y
                X_test_noisy = X_test_base + np.random.normal(0, noise_level, X_test_base.shape)
                Y_test_noisy = Y_test_base + np.random.normal(0, noise_level, Y_test_base.shape)
            
            # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π
            for config in model_configs:
                model_name = config['name']
                
                # –ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –∑–∞—à—É–º–ª–µ–Ω–∏—Ö –¥–∞–Ω–∏—Ö
                model_result = self._train_single_model(
                    config, X_train_noisy, Y_train_noisy, None, None
                )
                
                # –û—Ü—ñ–Ω–∫–∞
                Y_pred_scaled = model_result['model'].predict(X_test_noisy)
                Y_pred = self.scaler_y.inverse_transform(Y_pred_scaled)
                
                mse = mean_squared_error(Y_test_noisy, Y_pred)
                r2 = r2_score(Y_test_noisy, Y_pred)
                
                noise_results[model_name][f'noise_{noise_level}'] = {
                    'mse': mse,
                    'r2': r2
                }
        
        return noise_results
    
    def _test_nonlinearity_robustness(self, global_config: Dict[str, Any],
                                    model_configs: List[Dict[str, Any]],
                                    nonlinearity_levels: List[Tuple[str, Dict]]) -> Dict[str, Any]:
        """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ –¥–æ —Ä—ñ–∑–Ω–∏—Ö —Ä—ñ–≤–Ω—ñ–≤ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ."""
        
        nonlinearity_results = {model_config['name']: {} for model_config in model_configs}
        
        for level_name, nonlinear_config in nonlinearity_levels:
            print(f"üìà –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ: {level_name}")
            
            # –ú–æ–¥–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
            nl_config = global_config.copy()
            nl_config['enable_nonlinear'] = len(nonlinear_config) > 0
            nl_config['nonlinear_config'] = nonlinear_config
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö –∑ –∑–∞–¥–∞–Ω–æ—é –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—é
            try:
                temp_results = self._prepare_data(nl_config)
                X_train_nl, Y_train_nl, _, _, X_test_nl, Y_test_nl, _, _ = temp_results
            except Exception as e:
                print(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö: {e}")
                # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –±–∞–∑–æ–≤—ñ –¥–∞–Ω—ñ
                temp_results = self._prepare_data(global_config)
                X_train_nl, Y_train_nl, _, _, X_test_nl, Y_test_nl, _, _ = temp_results
            
            # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –≤—Å—ñ—Ö –º–æ–¥–µ–ª–µ–π
            for config in model_configs:
                model_name = config['name']
                
                # –ù–∞–≤—á–∞–Ω–Ω—è
                model_result = self._train_single_model(
                    config, X_train_nl, Y_train_nl, None, None
                )
                
                # –û—Ü—ñ–Ω–∫–∞
                Y_pred_scaled = model_result['model'].predict(X_test_nl)
                Y_pred = self.scaler_y.inverse_transform(Y_pred_scaled)
                
                mse = mean_squared_error(Y_test_nl, Y_pred)
                r2 = r2_score(Y_test_nl, Y_pred)
                
                nonlinearity_results[model_name][level_name] = {
                    'mse': mse,
                    'r2': r2
                }
        
        return nonlinearity_results
    
    def _generate_comprehensive_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏."""
        
        print("\nüìù –ì–ï–ù–ï–†–ê–¶–Ü–Ø –ó–í–Ü–¢–£")
        print("-" * 25)
        
        # 1. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —É JSON
        json_path = self.dirs['data'] / f'linear_comparison_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è numpy arrays –¥–ª—è JSON —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó
        json_results = self._convert_results_for_json(self.results)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {json_path}")
        
        # 2. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π
        self._create_comparison_visualizations()
        
        # 3. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è LaTeX —Ç–∞–±–ª–∏—Ü—ñ
        self._generate_latex_table()
        
        # 4. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–≤—ñ—Ç—É
        self._generate_text_report()
    
    def _convert_results_for_json(self, results: Dict) -> Dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è JSON —Å–µ—Ä—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó."""
        
        def convert_value(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, dict):
                return {k: convert_value(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_value(item) for item in obj]
            else:
                return obj
        
        return convert_value(results)
    
    def _create_comparison_visualizations(self):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–æ–¥–µ–ª–µ–π."""
        
        print("üé® –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ–π...")
        
        # 1. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç—ñ
        self._plot_accuracy_comparison()
        
        # 2. –î—ñ–∞–≥—Ä–∞–º–∏ —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤
        self._plot_residual_analysis()
        
        # 3. –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É
        self._plot_noise_robustness()
        
        # 4. –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
        self._plot_nonlinearity_robustness()
        
        print("‚úÖ –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó —Å—Ç–≤–æ—Ä–µ–Ω–æ")
    
    def _plot_accuracy_comparison(self):
        """–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª–µ–π."""
        
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π', fontsize=16, fontweight='bold')
        
        # –î–∞–Ω—ñ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
        model_names = list(self.results['evaluation_results'].keys())
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        mse_values = [self.results['evaluation_results'][name]['mse'] for name in model_names]
        r2_values = [self.results['evaluation_results'][name]['r2'] for name in model_names]
        rmse_fe_values = [self.results['evaluation_results'][name]['rmse_fe'] for name in model_names]
        rmse_mass_values = [self.results['evaluation_results'][name]['rmse_mass'] for name in model_names]
        train_times = [self.results['evaluation_results'][name]['train_time'] for name in model_names]
        
        # 1. MSE comparison
        ax = axes[0, 0]
        bars = ax.bar(range(len(model_names)), mse_values, color='lightcoral')
        ax.set_xlabel('–ú–æ–¥–µ–ª—ñ')
        ax.set_ylabel('MSE')
        ax.set_title('Mean Squared Error')
        ax.set_xticks(range(len(model_names)))
        ax.set_xticklabels(model_names, rotation=45, ha='right')
        ax.grid(True, alpha=0.3)
        
        # –î–æ–¥–∞–≤–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –Ω–∞ —Å—Ç–æ–≤–ø—Ü—ñ
        for bar, value in zip(bars, mse_values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(mse_values)*0.02,
                   f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
        
        # 2. R¬≤ comparison
        ax = axes[0, 1]
        bars = ax.bar(range(len(model_names)), r2_values, color='lightgreen')
        ax.set_xlabel('–ú–æ–¥–µ–ª—ñ')
        ax.set_ylabel('R¬≤')
        ax.set_title('Coefficient of Determination')
        ax.set_xticks(range(len(model_names)))
        ax.set_xticklabels(model_names, rotation=45, ha='right')
        ax.grid(True, alpha=0.3)
        
        for bar, value in zip(bars, r2_values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(r2_values)*0.02,
                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 3. RMSE Fe concentration
        ax = axes[0, 2]
        bars = ax.bar(range(len(model_names)), rmse_fe_values, color='lightskyblue')
        ax.set_xlabel('–ú–æ–¥–µ–ª—ñ')
        ax.set_ylabel('RMSE (%)')
        ax.set_title('RMSE –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—ó Fe')
        ax.set_xticks(range(len(model_names)))
        ax.set_xticklabels(model_names, rotation=45, ha='right')
        ax.grid(True, alpha=0.3)
        
        for bar, value in zip(bars, rmse_fe_values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(rmse_fe_values)*0.02,
                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 4. RMSE Mass flow
        ax = axes[1, 0]
        bars = ax.bar(range(len(model_names)), rmse_mass_values, color='lightgoldenrodyellow')
        ax.set_xlabel('–ú–æ–¥–µ–ª—ñ')
        ax.set_ylabel('RMSE (—Ç/–≥–æ–¥)')
        ax.set_title('RMSE –º–∞—Å–æ–≤–æ—ó –≤–∏—Ç—Ä–∞—Ç–∏')
        ax.set_xticks(range(len(model_names)))
        ax.set_xticklabels(model_names, rotation=45, ha='right')
        ax.grid(True, alpha=0.3)
        
        for bar, value in zip(bars, rmse_mass_values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(rmse_mass_values)*0.02,
                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 5. Training time comparison
        ax = axes[1, 1]
        bars = ax.bar(range(len(model_names)), train_times, color='plum')
        ax.set_xlabel('–ú–æ–¥–µ–ª—ñ')
        ax.set_ylabel('–ß–∞—Å (—Å–µ–∫)')
        ax.set_title('–ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è')
        ax.set_xticks(range(len(model_names)))
        ax.set_xticklabels(model_names, rotation=45, ha='right')
        ax.grid(True, alpha=0.3)
        
        for bar, value in zip(bars, train_times):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(train_times)*0.02,
                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 6. Overall performance radar
        ax = axes[1, 2]
        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Ä–∞–¥–∞—Ä–Ω–æ—ó –¥—ñ–∞–≥—Ä–∞–º–∏
        normalized_mse = [(max(mse_values) - mse) / (max(mse_values) - min(mse_values) + 1e-8) for mse in mse_values]
        normalized_speed = [(max(train_times) - time) / (max(train_times) - min(train_times) + 1e-8) for time in train_times]
        
        x_pos = np.arange(len(model_names))
        width = 0.35
        
        ax.bar(x_pos - width/2, normalized_mse, width, label='–¢–æ—á–Ω—ñ—Å—Ç—å (–Ω–æ—Ä–º.)', alpha=0.7)
        ax.bar(x_pos + width/2, normalized_speed, width, label='–®–≤–∏–¥–∫—ñ—Å—Ç—å (–Ω–æ—Ä–º.)', alpha=0.7)
        
        ax.set_xlabel('–ú–æ–¥–µ–ª—ñ')
        ax.set_ylabel('–ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è')
        ax.set_title('–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞')
        ax.set_xticks(x_pos)
        ax.set_xticklabels(model_names, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = self.dirs['plots'] / 'accuracy_comparison.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üìä –ì—Ä–∞—Ñ—ñ–∫ —Ç–æ—á–Ω–æ—Å—Ç—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {plot_path}")
    
    def _plot_residual_analysis(self):
        """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–Ω–∞–ª—ñ–∑—É —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤."""
        
        n_models = len(self.models)
        fig, axes = plt.subplots(n_models, 3, figsize=(15, 5*n_models))
        fig.suptitle('–î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤ –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π', fontsize=16, fontweight='bold')
        
        if n_models == 1:
            axes = axes.reshape(1, -1)
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–µ—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        eval_results = self.results.get('evaluation_results', {})
        
        for idx, model_name in enumerate(self.models.keys()):
            if model_name not in eval_results:
                continue
                
            # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –æ—Ü—ñ–Ω–∫–∏
            Y_pred = eval_results[model_name]['predictions']
            
            # –¢–∏–º—á–∞—Å–æ–≤–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤—ñ –¥–∞–Ω—ñ - –±—É–¥–µ –∑–∞–º—ñ–Ω–µ–Ω–æ —Ä–µ–∞–ª—å–Ω–∏–º–∏
            Y_test = np.random.randn(len(Y_pred), Y_pred.shape[1])  # Placeholder
            residuals = Y_test - Y_pred
            
            # 1. QQ-plot –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—ñ
            ax = axes[idx, 0]
            try:
                stats.probplot(residuals[:, 0], dist="norm", plot=ax)
                ax.set_title(f'{model_name}: QQ-plot (Fe –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—è)')
                ax.grid(True, alpha=0.3)
            except Exception as e:
                ax.text(0.5, 0.5, f'–ü–æ–º–∏–ª–∫–∞: {str(e)[:50]}...', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{model_name}: QQ-plot (–ø–æ–º–∏–ª–∫–∞)')
            
            # 2. –†–µ–∑–∏–¥—É–∞–ª–∏ vs fitted values
            ax = axes[idx, 1]
            ax.scatter(Y_pred[:, 0], residuals[:, 0], alpha=0.6, s=20)
            ax.axhline(y=0, color='red', linestyle='--', alpha=0.8)
            ax.set_xlabel('–ü—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è')
            ax.set_ylabel('–†–µ–∑–∏–¥—É–∞–ª–∏')
            ax.set_title(f'{model_name}: –†–µ–∑–∏–¥—É–∞–ª–∏ vs –ü—Ä–æ–≥–Ω–æ–∑')
            ax.grid(True, alpha=0.3)
            
            # 3. –ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∞ —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤
            ax = axes[idx, 2]
            ax.hist(residuals[:, 0], bins=30, alpha=0.7, density=True, color='skyblue')
            
            # –ù–∞–∫–ª–∞–¥–∞–Ω–Ω—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª—É
            try:
                mu, sigma = stats.norm.fit(residuals[:, 0])
                x = np.linspace(residuals[:, 0].min(), residuals[:, 0].max(), 100)
                ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', lw=2, label=f'–ù–æ—Ä–º. —Ä–æ–∑–ø–æ–¥—ñ–ª (Œº={mu:.3f}, œÉ={sigma:.3f})')
                ax.legend()
            except:
                pass
            
            ax.set_xlabel('–†–µ–∑–∏–¥—É–∞–ª–∏')
            ax.set_ylabel('–©—ñ–ª—å–Ω—ñ—Å—Ç—å')
            ax.set_title(f'{model_name}: –†–æ–∑–ø–æ–¥—ñ–ª —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = self.dirs['diagnostics'] / 'residual_analysis.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üîç –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤ –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {plot_path}")
    
    def _plot_noise_robustness(self):
        """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ –¥–æ —à—É–º—É."""
        
        if 'noise_robustness' not in self.results.get('robustness_results', {}):
            return
        
        noise_data = self.results['robustness_results']['noise_robustness']
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        fig.suptitle('–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–æ —à—É–º—É', fontsize=16, fontweight='bold')
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤
        noise_levels = []
        model_names = list(noise_data.keys())
        
        # –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è —Ä—ñ–≤–Ω—ñ–≤ —à—É–º—É
        for model_name in model_names:
            for key in noise_data[model_name].keys():
                if key.startswith('noise_'):
                    noise_level = float(key.split('_')[1])
                    if noise_level not in noise_levels:
                        noise_levels.append(noise_level)
        
        noise_levels = sorted(noise_levels)
        noise_percentages = [level * 100 for level in noise_levels]
        
        # 1. MSE vs —à—É–º
        for model_name in model_names:
            mse_values = []
            for noise_level in noise_levels:
                key = f'noise_{noise_level}'
                if key in noise_data[model_name]:
                    mse_values.append(noise_data[model_name][key]['mse'])
                else:
                    mse_values.append(np.nan)
            
            ax1.plot(noise_percentages, mse_values, marker='o', label=model_name, linewidth=2)
        
        ax1.set_xlabel('–†—ñ–≤–µ–Ω—å —à—É–º—É (%)')
        ax1.set_ylabel('MSE')
        ax1.set_title('–î–µ–≥—Ä–∞–¥–∞—Ü—ñ—è —Ç–æ—á–Ω–æ—Å—Ç—ñ (MSE) –ø—Ä–∏ —à—É–º—ñ')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_yscale('log')
        
        # 2. R¬≤ vs —à—É–º
        for model_name in model_names:
            r2_values = []
            for noise_level in noise_levels:
                key = f'noise_{noise_level}'
                if key in noise_data[model_name]:
                    r2_values.append(noise_data[model_name][key]['r2'])
                else:
                    r2_values.append(np.nan)
            
            ax2.plot(noise_percentages, r2_values, marker='s', label=model_name, linewidth=2)
        
        ax2.set_xlabel('–†—ñ–≤–µ–Ω—å —à—É–º—É (%)')
        ax2.set_ylabel('R¬≤')
        ax2.set_title('–Ø–∫—ñ—Å—Ç—å —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è (R¬≤) –ø—Ä–∏ —à—É–º—ñ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = self.dirs['plots'] / 'noise_robustness.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üîä –ì—Ä–∞—Ñ—ñ–∫ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ –¥–æ —à—É–º—É –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {plot_path}")
    
    def _plot_nonlinearity_robustness(self):
        """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ –¥–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ."""
        
        if 'nonlinearity_robustness' not in self.results.get('robustness_results', {}):
            return
        
        nl_data = self.results['robustness_results']['nonlinearity_robustness']
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        fig.suptitle('–†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ', fontsize=16, fontweight='bold')
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
        model_names = list(nl_data.keys())
        nonlinearity_levels = ['linear', 'weak', 'moderate', 'strong']
        
        # 1. MSE vs –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å
        x_pos = np.arange(len(nonlinearity_levels))
        width = 0.8 / len(model_names)
        
        for i, model_name in enumerate(model_names):
            mse_values = []
            for level in nonlinearity_levels:
                if level in nl_data[model_name]:
                    mse_values.append(nl_data[model_name][level]['mse'])
                else:
                    mse_values.append(np.nan)
            
            ax1.bar(x_pos + i*width, mse_values, width, label=model_name, alpha=0.8)
        
        ax1.set_xlabel('–†—ñ–≤–µ–Ω—å –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ')
        ax1.set_ylabel('MSE')
        ax1.set_title('–í–ø–ª–∏–≤ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ –Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å (MSE)')
        ax1.set_xticks(x_pos + width * (len(model_names) - 1) / 2)
        ax1.set_xticklabels(nonlinearity_levels)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_yscale('log')
        
        # 2. R¬≤ vs –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å
        for i, model_name in enumerate(model_names):
            r2_values = []
            for level in nonlinearity_levels:
                if level in nl_data[model_name]:
                    r2_values.append(nl_data[model_name][level]['r2'])
                else:
                    r2_values.append(np.nan)
            
            ax2.bar(x_pos + i*width, r2_values, width, label=model_name, alpha=0.8)
        
        ax2.set_xlabel('–†—ñ–≤–µ–Ω—å –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ')
        ax2.set_ylabel('R¬≤')
        ax2.set_title('–í–ø–ª–∏–≤ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ –Ω–∞ —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è (R¬≤)')
        ax2.set_xticks(x_pos + width * (len(model_names) - 1) / 2)
        ax2.set_xticklabels(nonlinearity_levels)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = self.dirs['plots'] / 'nonlinearity_robustness.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"üìà –ì—Ä–∞—Ñ—ñ–∫ —Ä–æ–±–∞—Å—Ç–Ω–æ—Å—Ç—ñ –¥–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {plot_path}")
    
    def _generate_latex_table(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è LaTeX —Ç–∞–±–ª–∏—Ü—ñ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏."""
        
        if not self.results:
            raise ValueError("–ù–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è –µ–∫—Å–ø–æ—Ä—Ç—É")
        
        eval_results = self.results['evaluation_results']
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è LaTeX —Ç–∞–±–ª–∏—Ü—ñ
        latex_content = """\\begin{table}[h]
\\centering
\\caption{–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ—Ü–µ—Å—É –º–∞–≥–Ω—ñ—Ç–Ω–æ—ó —Å–µ–ø–∞—Ä–∞—Ü—ñ—ó}
\\label{tab:linear_models_comparison}
\\begin{tabular}{|l|c|c|c|c|c|}
\\hline
\\textbf{–ú–æ–¥–µ–ª—å} & \\textbf{MSE} & \\textbf{R¬≤} & \\textbf{RMSE Fe, \\%} & \\textbf{RMSE Mass, —Ç/–≥–æ–¥} & \\textbf{–ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è, —Å} \\\\
\\hline
"""
        
        for model_name, metrics in eval_results.items():
            latex_content += f"{model_name} & {metrics['mse']:.6f} & {metrics['r2']:.4f} & {metrics['rmse_fe']:.3f} & {metrics['rmse_mass']:.3f} & {metrics['train_time']:.3f} \\\\\n"
            latex_content += "\\hline\n"
        
        latex_content += """\\end{tabular}
\\end{table}"""
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
        latex_path = self.dirs['latex'] / 'linear_models_comparison_table.tex'
        with open(latex_path, 'w', encoding='utf-8') as f:
            f.write(latex_content)
        
        print(f"üìÑ LaTeX —Ç–∞–±–ª–∏—Ü—é –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {latex_path}")
        
        return latex_path
    
    def _generate_text_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∑–≤—ñ—Ç—É –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏."""
        
        report_content = f"""
# –ó–í–Ü–¢ –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –õ–Ü–ù–Ü–ô–ù–ò–• –ú–û–î–ï–õ–ï–ô
–î–∞—Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## –ó–ê–ì–ê–õ–¨–ù–ê –Ü–ù–§–û–†–ú–ê–¶–Ü–Ø
- –ö—ñ–ª—å–∫—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π: {len(self.models)}
- –†–æ–∑–º—ñ—Ä —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–æ—ó –≤–∏–±—ñ—Ä–∫–∏: {self.results['data_info']['train_size']}
- –†–æ–∑–º—ñ—Ä —Ç–µ—Å—Ç–æ–≤–æ—ó –≤–∏–±—ñ—Ä–∫–∏: {self.results['data_info']['test_size']}
- –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {self.results['data_info']['n_features']}

## –†–ï–ó–£–õ–¨–¢–ê–¢–ò –û–¶–Ü–ù–ö–ò –¢–û–ß–ù–û–°–¢–Ü
"""
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ—Ü—ñ–Ω–∫–∏
        eval_results = self.results['evaluation_results']
        
        for model_name, metrics in eval_results.items():
            report_content += f"""
### {model_name}
- MSE: {metrics['mse']:.6f}
- R¬≤: {metrics['r2']:.4f}  
- RMSE Fe: {metrics['rmse_fe']:.3f}%
- RMSE Mass: {metrics['rmse_mass']:.3f} —Ç/–≥–æ–¥
- MAE: {metrics['mae']:.4f}
- MAPE: {metrics['mape']:.2f}%
- –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {metrics['train_time']:.3f} —Å–µ–∫
"""
        
        # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤
        if 'diagnostics_results' in self.results:
            report_content += "\n## –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê –†–ï–ó–ò–î–£–ê–õ–Ü–í\n"
            
            for model_name, diagnostics in self.results['diagnostics_results'].items():
                report_content += f"\n### {model_name}\n"
                
                # –ù–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å
                if 'Fe_concentration_normality' in diagnostics:
                    norm_test = diagnostics['Fe_concentration_normality']
                    report_content += f"- –ù–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤ (Fe): {'–¢–ê–ö' if norm_test['is_normal'] else '–ù–Ü'}\n"
                    report_content += f"  - Shapiro-Wilk p-value: {norm_test['shapiro_p_value']:.4f}\n"
                    report_content += f"  - Jarque-Bera p-value: {norm_test['jb_p_value']:.4f}\n"
                
                # –ê–≤—Ç–æ–∫–æ—Ä–µ–ª—è—Ü—ñ—è
                if 'autocorrelation' in diagnostics:
                    autocorr = diagnostics['autocorrelation']
                    if 'has_autocorr' in autocorr:
                        report_content += f"- –ê–≤—Ç–æ–∫–æ—Ä–µ–ª—è—Ü—ñ—è —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤: {'–Ñ' if autocorr['has_autocorr'] else '–ù–ï–ú–ê–Ñ'}\n"
                
                # –ì–µ—Ç–µ—Ä–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å
                if 'heteroscedasticity' in diagnostics:
                    hetero = diagnostics['heteroscedasticity']
                    if 'is_homoscedastic' in hetero:
                        report_content += f"- –ì–æ–º–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å: {'–¢–ê–ö' if hetero['is_homoscedastic'] else '–ù–Ü'}\n"
                        report_content += f"  - Breusch-Pagan p-value: {hetero['breusch_pagan_p']:.4f}\n"
        
        # –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å
        if 'robustness_results' in self.results:
            report_content += "\n## –ê–ù–ê–õ–Ü–ó –†–û–ë–ê–°–¢–ù–û–°–¢–Ü\n"
            
            # –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É
            if 'noise_robustness' in self.results['robustness_results']:
                report_content += "\n### –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ —à—É–º—É\n"
                noise_data = self.results['robustness_results']['noise_robustness']
                
                for model_name in noise_data.keys():
                    report_content += f"\n#### {model_name}\n"
                    for noise_key, metrics in noise_data[model_name].items():
                        noise_level = float(noise_key.split('_')[1]) * 100
                        report_content += f"- –®—É–º {noise_level:.1f}%: MSE={metrics['mse']:.6f}, R¬≤={metrics['r2']:.4f}\n"
            
            # –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
            if 'nonlinearity_robustness' in self.results['robustness_results']:
                report_content += "\n### –†–æ–±–∞—Å—Ç–Ω—ñ—Å—Ç—å –¥–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ\n"
                nl_data = self.results['robustness_results']['nonlinearity_robustness']
                
                for model_name in nl_data.keys():
                    report_content += f"\n#### {model_name}\n"
                    for level, metrics in nl_data[model_name].items():
                        report_content += f"- {level}: MSE={metrics['mse']:.6f}, R¬≤={metrics['r2']:.4f}\n"
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
        report_path = self.dirs['reports'] / f'linear_comparison_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"üìã –¢–µ–∫—Å—Ç–æ–≤–∏–π –∑–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {report_path}")
        
        return report_path
    
    def analyze_arx_limitations_for_dissertation(self, **kwargs) -> Dict[str, Any]:
        """
        –°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –æ–±–º–µ–∂–µ–Ω—å ARX –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª—É 2.3 –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó.
        
        –ü—Ä–æ–≤–æ–¥–∏—Ç—å:
        1. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è ARX –∑ —Ä—ñ–∑–Ω–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó
        2. –ê–Ω–∞–ª—ñ–∑ –≤–ø–ª–∏–≤—É –ª–∞–≥–æ–≤–æ—ó —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
        3. –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö —Ä—ñ–≤–Ω—è—Ö –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
        4. –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫—É –ø—Ä–∏–ø—É—â–µ–Ω—å –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
        
        Returns:
            Dict –∑ –¥–µ—Ç–∞–ª—å–Ω–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó
        """
        
        print("üéì –ê–ù–ê–õ–Ü–ó –û–ë–ú–ï–ñ–ï–ù–¨ ARX –ú–û–î–ï–õ–ï–ô –î–õ–Ø –î–ò–°–ï–†–¢–ê–¶–Ü–á")
        print("=" * 55)
        
        # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ–π–Ω–æ–≥–æ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è (–≤–∏–ø—Ä–∞–≤–ª–µ–Ω–∞)
        model_configs = [
            {'name': 'ARX_OLS', 'linear_type': 'ols', 'poly_degree': 1, 'include_bias': True},
            {'name': 'ARX_Ridge', 'linear_type': 'ridge', 'alpha': 0.1, 'poly_degree': 1},
            {'name': 'ARX_Lasso', 'linear_type': 'lasso', 'alpha': 0.01, 'poly_degree': 1},
            {'name': 'Ridge_Strong', 'linear_type': 'ridge', 'alpha': 1.0, 'poly_degree': 1},  # –°–∏–ª—å–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è
            {'name': 'Quadratic_OLS', 'linear_type': 'ols', 'poly_degree': 2, 'include_bias': True}  # –ö–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ñ –æ–∑–Ω–∞–∫–∏
        ]
        
        # –ì–ª–æ–±–∞–ª—å–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —É —Ñ–æ—Ä–º–∞—Ç—ñ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤
        global_config = {
            'T': kwargs.get('T', 5000),                    # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ—á–æ–∫
            'control_pts': kwargs.get('control_pts', 500),  # –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ñ —Ç–æ—á–∫–∏
            'n_neighbors': kwargs.get('n_neighbors', 5),    # –î–ª—è k-NN —ñ–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü—ñ—ó
            'train_size': 0.7,
            'val_size': 0.15,
            'test_size': 0.15,
            'lag_depth': kwargs.get('lag_depth', 8),
            'time_step_s': 5,
            'time_constants_s': {                           # –°–ª–æ–≤–Ω–∏–∫ –∫–æ–Ω—Å—Ç–∞–Ω—Ç —á–∞—Å—É
                'concentrate_fe_percent': 300,
                'tailings_fe_percent': 400,
                'concentrate_mass_flow': 600,
                'tailings_mass_flow': 700,
                'default': 500
            },
            'dead_times_s': {                               # –°–ª–æ–≤–Ω–∏–∫ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω–∏—Ö –∑–∞—Ç—Ä–∏–º–æ–∫
                'concentrate_fe_percent': 60,
                'tailings_fe_percent': 80,
                'concentrate_mass_flow': 120,
                'tailings_mass_flow': 140,
                'default': 90
            },
            'plant_model_type': 'rf',            # –¢–∏–ø —Å–∏–º—É–ª—è—Ü—ñ–π–Ω–æ—ó –º–æ–¥–µ–ª—ñ
            'use_simulation': True,
            'use_anomalies': False,                         # –ü–æ–∫–∏ –≤—ñ–¥–∫–ª—é—á–∏–º–æ –∞–Ω–æ–º–∞–ª—ñ—ó
            'seed': 42
        }
        
        # –°–µ—Ä—ñ—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤ –∑ —Ä—ñ–∑–Ω–∏–º–∏ —Ä—ñ–≤–Ω—è–º–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
        nonlinearity_configs = [
            ('–õ—ñ–Ω—ñ–π–Ω–∏–π', {'enable_nonlinear': False}),
            ('–°–ª–∞–±–∫–∏–π', {
                'enable_nonlinear': True,
                'nonlinear_config': {'concentrate_fe_percent': ('pow', 1.3)}
            }),
            ('–ü–æ–º—ñ—Ä–Ω–∏–π', {
                'enable_nonlinear': True, 
                'nonlinear_config': {'concentrate_fe_percent': ('pow', 1.8)}
            }),
            ('–°–∏–ª—å–Ω–∏–π', {
                'enable_nonlinear': True,
                'nonlinear_config': {
                    'concentrate_fe_percent': ('pow', 2.2),
                    'concentrate_mass_flow': ('pow', 1.6)
                }
            })
        ]
        
        dissertation_results = {}
        
        # –ó–∞–ø—É—Å–∫ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
        for nl_name, nl_config in nonlinearity_configs:
            print(f"\nüß™ –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {nl_name} —Ä—ñ–≤–µ–Ω—å –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ")
            
            # –û–±'—î–¥–Ω–∞–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π
            experiment_config = {**global_config, **nl_config}
            
            # –ó–∞–ø—É—Å–∫ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
            experiment_results = self.run_comprehensive_comparison(
                model_configs, experiment_config
            )
            
            dissertation_results[nl_name] = experiment_results
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫–æ–≤–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
        summary_analysis = self._create_dissertation_summary(dissertation_results)
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó
        self._save_dissertation_report(dissertation_results, summary_analysis)
        
        return {
            'detailed_results': dissertation_results,
            'summary_analysis': summary_analysis
        }
    
    def _create_dissertation_summary(self, dissertation_results: Dict) -> Dict[str, Any]:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø—ñ–¥—Å—É–º–∫–æ–≤–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó."""
        
        summary = {
            'key_findings': {},
            'arx_limitations': {},
            'recommendations': {}
        }
        
        # –ê–Ω–∞–ª—ñ–∑ –¥–µ–≥—Ä–∞–¥–∞—Ü—ñ—ó ARX –ø—Ä–∏ –∑—Ä–æ—Å—Ç–∞–Ω–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
        arx_performance = {}
        for nl_level, results in dissertation_results.items():
            arx_metrics = results['evaluation_results']['ARX_OLS']
            arx_performance[nl_level] = {
                'mse': arx_metrics['mse'],
                'r2': arx_metrics['r2'],
                'rmse_fe': arx_metrics['rmse_fe']
            }
        
        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è –¥–µ–≥—Ä–∞–¥–∞—Ü—ñ—ó
        linear_mse = arx_performance['–õ—ñ–Ω—ñ–π–Ω–∏–π']['mse']
        strong_nl_mse = arx_performance['–°–∏–ª—å–Ω–∏–π']['mse']
        
        summary['key_findings'] = {
            'mse_degradation_percent': ((strong_nl_mse - linear_mse) / linear_mse) * 100,
            'worst_case_rmse_fe': max([metrics['rmse_fe'] for metrics in arx_performance.values()]),
            'r2_drop': arx_performance['–õ—ñ–Ω—ñ–π–Ω–∏–π']['r2'] - arx_performance['–°–∏–ª—å–Ω–∏–π']['r2']
        }
        
        # –í–∏—è–≤–ª–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∏ ARX
        best_alternative = None
        best_improvement = 0
        
        for nl_level, results in dissertation_results.items():
            if nl_level == '–°–∏–ª—å–Ω–∏–π':  # –ù–∞–π—Å–∫–ª–∞–¥–Ω—ñ—à–∏–π –≤–∏–ø–∞–¥–æ–∫
                arx_mse = results['evaluation_results']['ARX_OLS']['mse']
                
                for model_name, metrics in results['evaluation_results'].items():
                    if model_name != 'ARX_OLS':
                        improvement = ((arx_mse - metrics['mse']) / arx_mse) * 100
                        if improvement > best_improvement:
                            best_improvement = improvement
                            best_alternative = model_name
        
        summary['recommendations'] = {
            'best_linear_alternative': best_alternative,
            'improvement_percent': best_improvement,
            'transition_to_nonlinear_threshold': '–ü—Ä–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ > 1.8 —Å—Ç—É–ø–µ–Ω—è'
        }
        
        return summary
    
    def _save_dissertation_report(self, dissertation_results: Dict, summary: Dict):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó."""
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è markdown –∑–≤—ñ—Ç—É –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó
        report_content = f"""# –ê–ù–ê–õ–Ü–ó –û–ë–ú–ï–ñ–ï–ù–¨ ARX –ú–û–î–ï–õ–ï–ô - –†–ï–ó–£–õ–¨–¢–ê–¢–ò –î–û–°–õ–Ü–î–ñ–ï–ù–¨

*–°—Ç–≤–æ—Ä–µ–Ω–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*

## –ö–õ–Æ–ß–û–í–Ü –í–ò–°–ù–û–í–ö–ò

### –î–µ–≥—Ä–∞–¥–∞—Ü—ñ—è ARX –ø—Ä–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
- **–ó—Ä–æ—Å—Ç–∞–Ω–Ω—è MSE**: {summary['key_findings']['mse_degradation_percent']:.1f}% –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥—ñ –≤—ñ–¥ –ª—ñ–Ω—ñ–π–Ω–æ–≥–æ –¥–æ —Å–∏–ª—å–Ω–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—É
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑—É Fe**: {summary['key_findings']['worst_case_rmse_fe']:.3f}%
- **–ü–∞–¥—ñ–Ω–Ω—è R¬≤**: {summary['key_findings']['r2_drop']:.3f} –ø—É–Ω–∫—Ç—ñ–≤

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
- **–ù–∞–π–∫—Ä–∞—â–∞ –ª—ñ–Ω—ñ–π–Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞**: {summary['recommendations']['best_linear_alternative']}
- **–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ**: {summary['recommendations']['improvement_percent']:.1f}%
- **–ü–æ—Ä—ñ–≥ –ø–µ—Ä–µ—Ö–æ–¥—É –¥–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤**: {summary['recommendations']['transition_to_nonlinear_threshold']}

## –î–ï–¢–ê–õ–¨–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò –ü–û –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê–•
"""
        
        # –î–µ—Ç–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ä—ñ–≤–Ω—è –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
        for nl_level, results in dissertation_results.items():
            report_content += f"\n### –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {nl_level} –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å\n\n"
            
            # –¢–∞–±–ª–∏—Ü—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            report_content += "| –ú–æ–¥–µ–ª—å | MSE | R¬≤ | RMSE Fe (%) | RMSE Mass (—Ç/–≥–æ–¥) | –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è (—Å) |\n"
            report_content += "|--------|-----|----|-----------|-----------------|-----------------|\n"
            
            for model_name, metrics in results['evaluation_results'].items():
                report_content += f"| {model_name} | {metrics['mse']:.6f} | {metrics['r2']:.4f} | {metrics['rmse_fe']:.3f} | {metrics['rmse_mass']:.3f} | {metrics['train_time']:.3f} |\n"
            
            # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è ARX_OLS
            if 'diagnostics_results' in results and 'ARX_OLS' in results['diagnostics_results']:
                arx_diag = results['diagnostics_results']['ARX_OLS']
                report_content += f"\n**–î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ ARX_OLS –ø—Ä–∏ {nl_level} –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ:**\n"
                
                if 'Fe_concentration_normality' in arx_diag:
                    norm = arx_diag['Fe_concentration_normality']
                    report_content += f"- –ù–æ—Ä–º–∞–ª—å–Ω—ñ—Å—Ç—å —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤: {'‚úÖ' if norm['is_normal'] else '‚ùå'} (p={norm['shapiro_p_value']:.4f})\n"
                
                if 'autocorrelation' in arx_diag and 'has_autocorr' in arx_diag['autocorrelation']:
                    autocorr = arx_diag['autocorrelation']['has_autocorr']
                    report_content += f"- –ê–≤—Ç–æ–∫–æ—Ä–µ–ª—è—Ü—ñ—è: {'‚ùå –Ñ' if autocorr else '‚úÖ –ù–µ–º–∞—î'}\n"
                
                if 'heteroscedasticity' in arx_diag and 'is_homoscedastic' in arx_diag['heteroscedasticity']:
                    homo = arx_diag['heteroscedasticity']['is_homoscedastic']
                    report_content += f"- –ì–æ–º–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å: {'‚úÖ' if homo else '‚ùå'}\n"
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó
        report_content += f"""

## –í–ò–°–ù–û–í–ö–ò –î–õ–Ø –ü–Ü–î–†–û–ó–î–Ü–õ–£ 2.3 –î–ò–°–ï–†–¢–ê–¶–Ü–á

### –ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω—ñ –æ–±–º–µ–∂–µ–Ω–Ω—è ARX –º–æ–¥–µ–ª–µ–π:

1. **–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–µ –ø–æ–≥—ñ—Ä—à–µ–Ω–Ω—è –ø—Ä–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ**
   - MSE –∑—Ä–æ—Å—Ç–∞—î –Ω–∞ {summary['key_findings']['mse_degradation_percent']:.1f}% –ø—Ä–∏ –ø–æ—Å–∏–ª–µ–Ω–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
   - –ö—Ä–∏—Ç–∏—á–Ω–µ –ø–∞–¥—ñ–Ω–Ω—è R¬≤ –Ω–∞ {summary['key_findings']['r2_drop']:.3f} –ø—É–Ω–∫—Ç—ñ–≤

2. **–ü–æ—Ä—É—à–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–∏—Ö –ø—Ä–∏–ø—É—â–µ–Ω—å**
   - –ê–≤—Ç–æ–∫–æ—Ä–µ–ª—è—Ü—ñ—è —Ä–µ–∑–∏–¥—É–∞–ª—ñ–≤ –ø—Ä–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –ø—Ä–æ—Ü–µ—Å–∞—Ö
   - –ì–µ—Ç–µ—Ä–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω—ñ—Å—Ç—å –ø—Ä–∏ —Å–∫–ª–∞–¥–Ω–∏—Ö –≤–∑–∞—î–º–æ–¥—ñ—è—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
   - –í—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –≤—ñ–¥ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —Ä–æ–∑–ø–æ–¥—ñ–ª—É –∑–∞–ª–∏—à–∫—ñ–≤

3. **–û–±–º–µ–∂–µ–Ω–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω—ñ—Å—Ç—å**
   - –§—ñ–∫—Å–æ–≤–∞–Ω–∞ –ª—ñ–Ω—ñ–π–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–µ –≤—Ä–∞—Ö–æ–≤—É—î S-–ø–æ–¥—ñ–±–Ω—ñ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
   - –ù–µ–º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –º–æ–¥–µ–ª—é–≤–∞–Ω–Ω—è –ø–æ—Ä—ñ–≥–æ–≤–∏—Ö –µ—Ñ–µ–∫—Ç—ñ–≤
   - –°–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω—ñ –ø–æ–º–∏–ª–∫–∏ –≤ –∑–æ–Ω–∞—Ö –Ω–∞—Å–∏—á–µ–Ω–Ω—è

### –ú–∞—Ç–µ–º–∞—Ç–∏—á–Ω–µ –æ–±“ë—Ä—É–Ω—Ç—É–≤–∞–Ω–Ω—è –ø–µ—Ä–µ—Ö–æ–¥—É –¥–æ —è–¥–µ—Ä–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤:

–ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂—É—é—Ç—å —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω—ñ –ø–æ–ª–æ–∂–µ–Ω–Ω—è –ø—Ä–æ –Ω–µ—Å–ø—Ä–æ–º–æ–∂–Ω—ñ—Å—Ç—å –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π 
—Ç–æ—á–Ω–æ –æ–ø–∏—Å—É–≤–∞—Ç–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –ø—Ä–æ—Ü–µ—Å—É –º–∞–≥–Ω—ñ—Ç–Ω–æ—ó —Å–µ–ø–∞—Ä–∞—Ü—ñ—ó. –ü–æ–∫–∞–∑–∞–Ω–∞ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ—Å—Ç—å 
–≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —è–¥–µ—Ä–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤ –¥–ª—è –ø–æ–¥–æ–ª–∞–Ω–Ω—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö –æ–±–º–µ–∂–µ–Ω—å ARX –ø—ñ–¥—Ö–æ–¥—É.

### –ö—ñ–ª—å–∫—ñ—Å–Ω—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏ –¥–ª—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ—ó:

- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –ø–æ—Ö–∏–±–∫–∞ ARX**: {summary['key_findings']['worst_case_rmse_fe']:.3f}% (–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü—ñ—è Fe)
- **–ù–∞–π–∫—Ä–∞—â–∞ –ª—ñ–Ω—ñ–π–Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞**: {summary['recommendations']['best_linear_alternative']} 
  (–ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –Ω–∞ {summary['recommendations']['improvement_percent']:.1f}%)
- **–ö—Ä–∏—Ç–∏—á–Ω–∏–π –ø–æ—Ä—ñ–≥ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ**: —Å—Ç—É–ø—ñ–Ω—å > 1.8 –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥—É –¥–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–µ—Ç–æ–¥—ñ–≤
"""
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –¥–∏—Å–µ—Ä—Ç–∞—Ü—ñ–π–Ω–æ–≥–æ –∑–≤—ñ—Ç—É
        dissertation_report_path = self.dirs['reports'] / 'dissertation_section_2_3_analysis.md'
        with open(dissertation_report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"üéì –î–∏—Å–µ—Ä—Ç–∞—Ü—ñ–π–Ω–∏–π –∑–≤—ñ—Ç –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {dissertation_report_path}")
        
        return dissertation_report_path


def compare_linear_models_on_nonlinear_data_fixed(reference_df: Optional[pd.DataFrame] = None,
                                                output_dir: str = "nonlinear_data_comparison_fixed") -> Dict[str, Any]:
    """
    –í–ò–ü–†–ê–í–õ–ï–ù–ò–ô –ø–æ–∑–∞–∫–ª–∞—Å–æ–≤–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –±–∞–∑–æ–≤–∏—Ö –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–∏–ª—å–Ω–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–∏—Ö –¥–∞–Ω–∏—Ö.
    
    –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø:
    - –ê–∫—Ç–∏–≤–æ–≤–∞–Ω–æ –∞–Ω–æ–º–∞–ª—ñ—ó (use_anomalies=True)
    - –î–æ–¥–∞–Ω–æ —à—É–º (noise_level='medium')
    - –í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–µ–ª—ñ–Ω—ñ–π–Ω–æ—Å—Ç—ñ
    - –î–æ–¥–∞–Ω–æ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫—É —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö
    
    Args:
        reference_df: –û–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ —Ä–µ—Ñ–µ—Ä–µ–Ω—Ç–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è —Å–∏–º—É–ª—è—Ü—ñ—ó
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        
    Returns:
        Dict –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ç–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–æ—é –¥–∞–Ω–∏—Ö
    """
    
    print("üî¨ –í–ò–ü–†–ê–í–õ–ï–ù–ï –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –õ–Ü–ù–Ü–ô–ù–ò–• –ú–û–î–ï–õ–ï–ô –ù–ê –ù–ï–õ–Ü–ù–Ü–ô–ù–ò–• –î–ê–ù–ò–•")
    print("=" * 70)
    print("üìã –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ –∞–Ω–æ–º–∞–ª—ñ—è–º–∏ —Ç–∞ —à—É–º–æ–º")
    print()
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–µ—Ä–≤—ñ—Å—É
    comparison_service = LinearModelsComparisonService(
        reference_df=reference_df,
        output_dir=output_dir
    )
    
    # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –±–∞–∑–æ–≤–∏—Ö –ª—ñ–Ω—ñ–π–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π ARX –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
    model_configs = [
        {'name': 'ARX_OLS', 'linear_type': 'ols'},
        {'name': 'ARX_Ridge', 'linear_type': 'ridge', 'alpha': 0.1},
        {'name': 'ARX_Lasso', 'linear_type': 'lasso', 'alpha': 0.01}
    ]
    
    # –í–ò–ü–†–ê–í–õ–ï–ù–ê –≥–ª–æ–±–∞–ª—å–Ω–∞ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
    global_config = {
        'N_data': 4000,           # –ü—Ä–∞–≤–∏–ª—å–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä (–Ω–µ 'T')
        'lag_depth': 8,           
        'enable_nonlinear': True, 
        'use_simulation': True,   
        'use_anomalies': True,    # –í–ò–ü–†–ê–í–õ–ï–ù–û: –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–æ –∞–Ω–æ–º–∞–ª—ñ—ó
        'anomaly_severity': 'medium',  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –¥–æ–¥–∞–Ω–æ —Ä—ñ–≤–µ–Ω—å –∞–Ω–æ–º–∞–ª—ñ–π
        'anomaly_in_train': False, # –í–ò–ü–†–ê–í–õ–ï–ù–û: –∞–Ω–æ–º–∞–ª—ñ—ó –≤ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        'noise_level': 'medium',  # –í–ò–ü–†–ê–í–õ–ï–ù–û: –¥–æ–¥–∞–Ω–æ —à—É–º
        'nonlinear_config': {
            # –°–∏–ª—å–Ω—ñ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó –æ–±–º–µ–∂–µ–Ω—å ARX
            'concentrate_fe_percent': ('pow', 2.5),   
            'concentrate_mass_flow': ('pow', 1.8)    
        },
        'train_size': 0.8,        
        'val_size': 0.1,         
        'test_size': 0.1,         
        'seed': 42
    }
    
    print("üîß –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø –î–õ–Ø –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –¢–ï–°–¢–£–í–ê–ù–ù–Ø:")
    print(f"   üî¥ –ê–Ω–æ–º–∞–ª—ñ—ó: {global_config['use_anomalies']} ({global_config['anomaly_severity']})")
    print(f"   üîä –®—É–º: {global_config['noise_level']}")
    print(f"   üìà –ù–µ–ª—ñ–Ω—ñ–π–Ω—ñ—Å—Ç—å: Fe^{global_config['nonlinear_config']['concentrate_fe_percent'][1]}, Mass^{global_config['nonlinear_config']['concentrate_mass_flow'][1]}")
    print()
    
    # –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –º–µ—Ç–æ–¥—É –∫–ª–∞—Å—É
    print("üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª—ñ–∑—É –∑ –í–ò–ü–†–ê–í–õ–ï–ù–ò–ú–ò –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
    
    try:
        results = comparison_service.run_comprehensive_comparison(model_configs, global_config)
        
        # –î–û–î–ê–ù–û: –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö
        print("\nüîç –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê –Ø–ö–û–°–¢–Ü –ó–ì–ï–ù–ï–†–û–í–ê–ù–ò–• –î–ê–ù–ò–•:")
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        data_results = comparison_service._prepare_data(global_config)
        X_train, Y_train, X_val, Y_val, X_test, Y_test, X_test_unscaled, Y_test_scaled = data_results
        
        # –ó–∞–ø—É—Å–∫–∞—î–º–æ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫—É
        data_diagnostics = comparison_service.diagnose_data_quality(
            X=np.vstack([X_train, X_test]), 
            Y=np.vstack([Y_train, Y_test])
        )
        
        # –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        print("\nüìä –ê–ù–ê–õ–Ü–ó –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í –ú–û–î–ï–õ–ï–ô:")
        realistic_results = {}
        
        for model_name in model_configs:
            model_key = model_name['name']
            if model_key in results['evaluation_results']:
                eval_data = results['evaluation_results'][model_key]
                
                rmse_fe = eval_data.get('rmse_fe', 0)
                rmse_mass = eval_data.get('rmse_mass', 0)
                r2_score = eval_data.get('r2_score', 0)
                
                print(f"   {model_key}:")
                print(f"     RMSE Fe: {rmse_fe:.4f}")
                print(f"     RMSE Mass: {rmse_mass:.4f}") 
                print(f"     R¬≤ Score: {r2_score:.4f}")
                
                # –†–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
                is_too_perfect = (rmse_fe < 0.01 and r2_score > 0.99)
                is_reasonable = (0.5 < r2_score < 0.9 and rmse_fe > 1.0)
                
                realistic_results[model_key] = {
                    'rmse_fe': rmse_fe,
                    'rmse_mass': rmse_mass,
                    'r2_score': r2_score,
                    'is_too_perfect': is_too_perfect,
                    'is_reasonable': is_reasonable
                }
                
                if is_too_perfect:
                    warning = f"–ü–Ü–î–û–ó–†–Ü–õ–û –Ü–î–ï–ê–õ–¨–ù–Ü —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è {model_key}"
                    print(f"     ‚ö†Ô∏è  {warning}")
                    data_diagnostics['warnings'].append(warning)
                elif is_reasonable:
                    print(f"     ‚úÖ –†–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è {model_key}")
        
        # –ü—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ –∫—Ä–∏—Ç–∏—á–Ω–∏—Ö –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        worst_rmse_fe = max([metrics['rmse_fe'] for metrics in realistic_results.values()])
        best_r2 = max([metrics['r2_score'] for metrics in realistic_results.values()])
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–æ—Å—Ç—ñ
        n_perfect_models = sum([1 for m in realistic_results.values() if m['is_too_perfect']])
        n_reasonable_models = sum([1 for m in realistic_results.values() if m['is_reasonable']])
        
        print(f"\nüéØ –ü–Ü–î–°–£–ú–ö–û–í–ê –û–¶–Ü–ù–ö–ê:")
        print(f"   üìä –ù–∞–π–≥—ñ—Ä—à–∞ RMSE Fe: {worst_rmse_fe:.4f}")
        print(f"   üìä –ù–∞–π–∫—Ä–∞—â–∏–π R¬≤: {best_r2:.4f}")
        print(f"   ‚úÖ –†–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π: {n_reasonable_models}/{len(model_configs)}")
        print(f"   ‚ö†Ô∏è  –ü—ñ–¥–æ–∑—Ä—ñ–ª–æ —ñ–¥–µ–∞–ª—å–Ω–∏—Ö: {n_perfect_models}/{len(model_configs)}")
        
        # –§–æ—Ä–º—É–≤–∞–Ω–Ω—è –≤–∏—Å–Ω–æ–≤–∫—ñ–≤
        if n_perfect_models > 0:
            print("\n‚ùå –í–ò–Ø–í–õ–ï–ù–û –ü–†–û–ë–õ–ï–ú–ò: –¥–µ—è–∫—ñ –º–æ–¥–µ–ª—ñ –ø–æ–∫–∞–∑—É—é—Ç—å –Ω–µ—Ä–µ–∞–ª—ñ—Å—Ç–∏—á–Ω–æ —ñ–¥–µ–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏")
            print("   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:")
            print("   1. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó –∞–Ω–æ–º–∞–ª—ñ–π")
            print("   2. –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ –Ω–µ–ª—ñ–Ω—ñ–π–Ω—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –∑–∞—Å—Ç–æ—Å–æ–≤—É—é—Ç—å—Å—è")
            print("   3. –î–æ–¥–∞–π—Ç–µ –±—ñ–ª—å—à–µ —à—É–º—É –¥–æ –¥–∞–Ω–∏—Ö")
            
        key_findings = {
            'worst_case_rmse_fe': worst_rmse_fe,
            'best_linear_r2': best_r2,
            'data_quality_score': data_diagnostics['data_quality_score'],
            'n_perfect_models': n_perfect_models,
            'n_reasonable_models': n_reasonable_models,
            'data_problems': len(data_diagnostics['warnings']),
            'recommendation': 'check_data_generation' if n_perfect_models > 0 else 'results_valid'
        }
        
    except Exception as e:
        print(f"‚ùå –ü–û–ú–ò–õ–ö–ê –ø—ñ–¥ —á–∞—Å –≤–∏–∫–æ–Ω–∞–Ω–Ω—è: {e}")
        return {
            'error': str(e),
            'recommendation': 'fix_implementation'
        }
    
    # –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∑ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–æ—é
    final_results = {
        'comprehensive_results': results,
        'data_diagnostics': data_diagnostics,
        'realistic_analysis': realistic_results,
        'key_findings': key_findings,
        'config_used': {
            'models': model_configs,
            'global': global_config
        }
    }
    
    print(f"\n‚úÖ –ê–ù–ê–õ–Ü–ó –ó–ê–í–ï–†–®–ï–ù–û")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {output_dir}")
    print(f"üîç –Ø–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö: {data_diagnostics['data_quality_score']:.1f}/100")
    print(f"‚ö†Ô∏è  –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω—å: {len(data_diagnostics['warnings'])}")
    
    return final_results
if __name__ == "__main__":
    # –í–∏–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∑–∞–ø—É—Å–∫ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    df = pd.read_parquet('processed.parquet')

    compare_linear_models_on_nonlinear_data_fixed(df, 'nonlinear_data_comparison_fixed')

