# evaluation_database.py - –ë–∞–∑–∞ –¥–∞–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è MPC

import sqlite3
import pandas as pd
import numpy as np
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any, Union
from pathlib import Path
from dataclasses import dataclass, asdict
import logging

try:
    from evaluation_storage import EvaluationPackage, EvaluationResults, SimulationMetadata
except ImportError:
    print("‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è —ñ–º–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ evaluation_storage. –î–µ—è–∫—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –º–æ–∂—É—Ç—å –±—É—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ñ.")
    EvaluationPackage = None
    EvaluationResults = None
    SimulationMetadata = None

# =============================================================================
# === –°–¢–†–£–ö–¢–£–†–ò –î–ê–ù–ò–• –î–õ–Ø –ë–î ===
# =============================================================================

@dataclass
class ExperimentSeries:
    """–°–µ—Ä—ñ—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤ –¥–ª—è –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è –ø–æ–≤'—è–∑–∞–Ω–∏—Ö —Å–∏–º—É–ª—è—Ü—ñ–π"""
    
    series_id: str
    name: str
    description: str
    created_at: str
    tags: List[str] = None
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = []

@dataclass 
class EvaluationRecord:
    """–ó–∞–ø–∏—Å –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö"""
    
    id: Optional[int] = None
    simulation_id: str = ""
    series_id: Optional[str] = None
    timestamp: str = ""
    description: str = ""
    
    # –û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    simulation_steps: int = 0
    ref_fe: float = 0.0
    ref_mass: float = 0.0
    horizon: int = 0
    delta_u_max: float = 0.0
    lambda_u: float = 0.0
    
    # –ö–ª—é—á–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    overall_score: float = 0.0
    process_stability: float = 0.0
    model_r2_fe: float = 0.0
    model_r2_mass: float = 0.0
    ekf_consistency: float = 0.0
    trust_stability_index: float = 0.0
    setpoint_achievement_fe: float = 0.0
    setpoint_achievement_mass: float = 0.0
    
    # –ß–∞—Å–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
    initial_training_time: float = 0.0
    avg_prediction_time: float = 0.0
    
    # –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –∑ –ø–æ–≤–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏
    file_path: str = ""
    file_format: str = ""
    file_size_mb: float = 0.0
    
    # –ú–µ—Ç–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
    data_hash: str = ""
    tags: str = ""  # JSON —Å–ø–∏—Å–æ–∫ —Ç–µ–≥—ñ–≤
    notes: str = ""

# =============================================================================
# === –ú–ï–ù–ï–î–ñ–ï–† –ë–ê–ó–ò –î–ê–ù–ò–• ===
# =============================================================================

class EvaluationDatabase:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–∞ –ø–æ—à—É–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è"""
    
    def __init__(self, db_path: str = "evaluation_results.db"):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
        
        Args:
            db_path: –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É –±–∞–∑–∏ –¥–∞–Ω–∏—Ö SQLite
        """
        self.db_path = Path(db_path)
        self.connection = None
        
        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ/–ø—ñ–¥–∫–ª—é—á–∞—î–º–æ—Å—å –¥–æ –ë–î
        self._initialize_database()
    
    def _initialize_database(self):
        """–°—Ç–≤–æ—Ä—é—î —Ç–∞–±–ª–∏—Ü—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö —è–∫—â–æ –≤–æ–Ω–∏ –Ω–µ —ñ—Å–Ω—É—é—Ç—å"""
        
        self.connection = sqlite3.connect(self.db_path)
        self.connection.row_factory = sqlite3.Row  # –î–ª—è –∑—Ä—É—á–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø—É –¥–æ –∫–æ–ª–æ–Ω–æ–∫
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∞–±–ª–∏—Ü—ñ
        self._create_tables()
        
        self.logger.info(f"–ë–∞–∑–∞ –¥–∞–Ω–∏—Ö —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞: {self.db_path}")
    
    def _create_tables(self):
        """–°—Ç–≤–æ—Ä—é—î —Ç–∞–±–ª–∏—Ü—ñ –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö"""
        
        cursor = self.connection.cursor()
        
        # –¢–∞–±–ª–∏—Ü—è —Å–µ—Ä—ñ–π –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS experiment_series (
                series_id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                created_at TEXT NOT NULL,
                tags TEXT,
                UNIQUE(series_id)
            )
        """)
        
        # –û—Å–Ω–æ–≤–Ω–∞ —Ç–∞–±–ª–∏—Ü—è –æ—Ü—ñ–Ω—é–≤–∞–Ω—å
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS evaluations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                simulation_id TEXT UNIQUE NOT NULL,
                series_id TEXT,
                timestamp TEXT NOT NULL,
                description TEXT,
                
                -- –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Å–∏–º—É–ª—è—Ü—ñ—ó
                simulation_steps INTEGER,
                ref_fe REAL,
                ref_mass REAL,
                horizon INTEGER,
                delta_u_max REAL,
                lambda_u REAL,
                
                -- –ö–ª—é—á–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
                overall_score REAL,
                process_stability REAL,
                model_r2_fe REAL,
                model_r2_mass REAL,
                ekf_consistency REAL,
                trust_stability_index REAL,
                setpoint_achievement_fe REAL,
                setpoint_achievement_mass REAL,
                
                -- –ß–∞—Å–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
                initial_training_time REAL,
                avg_prediction_time REAL,
                
                -- –§–∞–π–ª –∑ –¥–∞–Ω–∏–º–∏
                file_path TEXT,
                file_format TEXT,
                file_size_mb REAL,
                
                -- –ú–µ—Ç–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
                data_hash TEXT,
                tags TEXT,
                notes TEXT,
                
                FOREIGN KEY (series_id) REFERENCES experiment_series (series_id)
            )
        """)
        
        # –î–æ–¥–∞—Ç–∫–æ–≤–∞ —Ç–∞–±–ª–∏—Ü—è –¥–ª—è –ø–æ–≤–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫ (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS detailed_metrics (
                evaluation_id INTEGER,
                metric_name TEXT,
                metric_value REAL,
                metric_category TEXT,
                PRIMARY KEY (evaluation_id, metric_name),
                FOREIGN KEY (evaluation_id) REFERENCES evaluations (id)
            )
        """)
        
        # –¢–∞–±–ª–∏—Ü—è —Ç–µ–≥—ñ–≤ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS evaluation_tags (
                evaluation_id INTEGER,
                tag TEXT,
                PRIMARY KEY (evaluation_id, tag),
                FOREIGN KEY (evaluation_id) REFERENCES evaluations (id)
            )
        """)
        
        # –Ü–Ω–¥–µ–∫—Å–∏ –¥–ª—è —à–≤–∏–¥–∫–æ–≥–æ –ø–æ—à—É–∫—É
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_overall_score ON evaluations (overall_score)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON evaluations (timestamp)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_series ON evaluations (series_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_tags ON evaluation_tags (tag)")
        
        self.connection.commit()
    
    def close(self):
        """–ó–∞–∫—Ä–∏–≤–∞—î –∑'—î–¥–Ω–∞–Ω–Ω—è –∑ –±–∞–∑–æ—é –¥–∞–Ω–∏—Ö"""
        if self.connection:
            self.connection.close()
            self.logger.info("–ó'—î–¥–Ω–∞–Ω–Ω—è –∑ –±–∞–∑–æ—é –¥–∞–Ω–∏—Ö –∑–∞–∫—Ä–∏—Ç–æ")
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
    
    # =============================================================================
    # === –û–ü–ï–†–ê–¶–Ü–á –ó –°–ï–†–Ü–Ø–ú–ò –ï–ö–°–ü–ï–†–ò–ú–ï–ù–¢–Ü–í ===
    # =============================================================================
    
    def create_experiment_series(self, series_id: str, name: str, 
                               description: str = "", tags: List[str] = None) -> bool:
        """
        –°—Ç–≤–æ—Ä—é—î –Ω–æ–≤—É —Å–µ—Ä—ñ—é –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
        
        Args:
            series_id: –£–Ω—ñ–∫–∞–ª—å–Ω–∏–π ID —Å–µ—Ä—ñ—ó
            name: –ù–∞–∑–≤–∞ —Å–µ—Ä—ñ—ó
            description: –û–ø–∏—Å —Å–µ—Ä—ñ—ó
            tags: –°–ø–∏—Å–æ–∫ —Ç–µ–≥—ñ–≤
            
        Returns:
            True —è–∫—â–æ —Å–µ—Ä—ñ—è —Å—Ç–≤–æ—Ä–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ
        """
        
        try:
            cursor = self.connection.cursor()
            
            series = ExperimentSeries(
                series_id=series_id,
                name=name,
                description=description,
                created_at=datetime.now().isoformat(),
                tags=tags or []
            )
            
            cursor.execute("""
                INSERT INTO experiment_series 
                (series_id, name, description, created_at, tags)
                VALUES (?, ?, ?, ?, ?)
            """, (
                series.series_id,
                series.name,
                series.description,
                series.created_at,
                json.dumps(series.tags)
            ))
            
            self.connection.commit()
            self.logger.info(f"–°—Ç–≤–æ—Ä–µ–Ω–∞ —Å–µ—Ä—ñ—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤: {series_id}")
            return True
            
        except sqlite3.IntegrityError:
            self.logger.warning(f"–°–µ—Ä—ñ—è {series_id} –≤–∂–µ —ñ—Å–Ω—É—î")
            return False
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–µ—Ä—ñ—ó {series_id}: {e}")
            return False
    
    def list_experiment_series(self) -> pd.DataFrame:
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –≤—Å—ñ—Ö —Å–µ—Ä—ñ–π –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤"""
        
        query = """
            SELECT s.*, COUNT(e.id) as evaluation_count
            FROM experiment_series s
            LEFT JOIN evaluations e ON s.series_id = e.series_id
            GROUP BY s.series_id
            ORDER BY s.created_at DESC
        """
        
        return pd.read_sql_query(query, self.connection)
    
    # =============================================================================
    # === –û–°–ù–û–í–ù–Ü –û–ü–ï–†–ê–¶–Ü–á –ó –û–¶–Ü–ù–Æ–í–ê–ù–ù–Ø–ú–ò ===
    # =============================================================================
    
    def add_evaluation(self, package: 'EvaluationPackage', 
                      series_id: Optional[str] = None,
                      tags: List[str] = None,
                      notes: str = "") -> int:
        """
        –î–æ–¥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
        
        Args:
            package: –ü–∞–∫–µ—Ç –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è
            series_id: ID —Å–µ—Ä—ñ—ó –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
            tags: –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ç–µ–≥–∏
            notes: –ü—Ä–∏–º—ñ—Ç–∫–∏
            
        Returns:
            ID —Å—Ç–≤–æ—Ä–µ–Ω–æ–≥–æ –∑–∞–ø–∏—Å—É –≤ –ë–î
        """
        
        if package is None:
            raise ValueError("Package –Ω–µ –º–æ–∂–µ –±—É—Ç–∏ None")
        
        cursor = self.connection.cursor()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∑–∞–ø–∏—Å
        record = EvaluationRecord(
            simulation_id=package.metadata.simulation_id,
            series_id=series_id,
            timestamp=package.metadata.timestamp,
            description=package.metadata.description,
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
            simulation_steps=package.metadata.simulation_steps,
            ref_fe=package.metadata.ref_fe,
            ref_mass=package.metadata.ref_mass,
            horizon=package.metadata.horizon,
            delta_u_max=package.metadata.delta_u_max,
            lambda_u=package.parameters.get('lambda_u', 0.0),
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
            overall_score=package.evaluation_results.overall_score,
            process_stability=package.evaluation_results.process_stability,
            model_r2_fe=package.evaluation_results.model_r2_fe,
            model_r2_mass=package.evaluation_results.model_r2_mass,
            ekf_consistency=package.evaluation_results.ekf_consistency,
            trust_stability_index=package.evaluation_results.trust_stability_index,
            setpoint_achievement_fe=package.evaluation_results.setpoint_achievement_fe,
            setpoint_achievement_mass=package.evaluation_results.setpoint_achievement_mass,
            
            # –ß–∞—Å
            initial_training_time=package.evaluation_results.initial_training_time,
            avg_prediction_time=package.evaluation_results.avg_prediction_time,
            
            # –ú–µ—Ç–∞ –¥–∞–Ω—ñ
            data_hash=package.metadata.data_hash,
            tags=json.dumps(tags or []),
            notes=notes
        )
        
        # –í—Å—Ç–∞–≤–ª—è—î–º–æ –∑–∞–ø–∏—Å
        cursor.execute("""
            INSERT INTO evaluations (
                simulation_id, series_id, timestamp, description,
                simulation_steps, ref_fe, ref_mass, horizon, delta_u_max, lambda_u,
                overall_score, process_stability, model_r2_fe, model_r2_mass,
                ekf_consistency, trust_stability_index, 
                setpoint_achievement_fe, setpoint_achievement_mass,
                initial_training_time, avg_prediction_time,
                file_path, file_format, file_size_mb,
                data_hash, tags, notes
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            record.simulation_id, record.series_id, record.timestamp, record.description,
            record.simulation_steps, record.ref_fe, record.ref_mass, 
            record.horizon, record.delta_u_max, record.lambda_u,
            record.overall_score, record.process_stability, 
            record.model_r2_fe, record.model_r2_mass,
            record.ekf_consistency, record.trust_stability_index,
            record.setpoint_achievement_fe, record.setpoint_achievement_mass,
            record.initial_training_time, record.avg_prediction_time,
            record.file_path, record.file_format, record.file_size_mb,
            record.data_hash, record.tags, record.notes
        ))
        
        evaluation_id = cursor.lastrowid
        
        # –î–æ–¥–∞—î–º–æ –¥–µ—Ç–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
        self._add_detailed_metrics(evaluation_id, package.evaluation_results)
        
        # –î–æ–¥–∞—î–º–æ —Ç–µ–≥–∏
        if tags:
            self._add_tags(evaluation_id, tags)
        
        self.connection.commit()
        self.logger.info(f"–î–æ–¥–∞–Ω–æ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è: {package.metadata.simulation_id} (ID: {evaluation_id})")
        
        return evaluation_id
    
    def _add_detailed_metrics(self, evaluation_id: int, eval_results: 'EvaluationResults'):
        """–î–æ–¥–∞—î –¥–µ—Ç–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ –æ–∫—Ä–µ–º–æ—ó —Ç–∞–±–ª–∏—Ü—ñ"""
        
        cursor = self.connection.cursor()
        
        # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó –º–µ—Ç—Ä–∏–∫
        metric_categories = {
            'model': ['model_rmse_fe', 'model_rmse_mass', 'model_mae_fe', 'model_mae_mass', 
                     'model_mape_fe', 'model_mape_mass', 'model_bias_fe', 'model_bias_mass'],
            'ekf': ['ekf_rmse_fe', 'ekf_rmse_mass', 'ekf_normalized_rmse_fe', 
                   'ekf_normalized_rmse_mass', 'ekf_rmse_total', 'ekf_nees_mean', 'ekf_nis_mean'],
            'trust_region': ['trust_radius_mean', 'trust_radius_std', 'trust_radius_min', 
                           'trust_radius_max', 'trust_adaptivity_coeff'],
            'tracking': ['tracking_error_fe', 'tracking_error_mass', 'tracking_mae_fe', 
                        'tracking_mae_mass', 'tracking_mape_fe', 'tracking_mape_mass',
                        'ise_fe', 'ise_mass', 'iae_fe', 'iae_mass'],
            'control': ['control_smoothness', 'control_aggressiveness', 'control_variability',
                       'control_energy', 'control_stability_index', 'control_utilization',
                       'significant_changes_frequency', 'max_control_change']
        }
        
        eval_dict = eval_results.to_dict()
        
        for category, metrics in metric_categories.items():
            for metric in metrics:
                if metric in eval_dict:
                    cursor.execute("""
                        INSERT INTO detailed_metrics (evaluation_id, metric_name, metric_value, metric_category)
                        VALUES (?, ?, ?, ?)
                    """, (evaluation_id, metric, eval_dict[metric], category))
    
    def _add_tags(self, evaluation_id: int, tags: List[str]):
        """–î–æ–¥–∞—î —Ç–µ–≥–∏ –¥–æ —Ç–∞–±–ª–∏—Ü—ñ —Ç–µ–≥—ñ–≤"""
        
        cursor = self.connection.cursor()
        
        for tag in tags:
            cursor.execute("""
                INSERT OR IGNORE INTO evaluation_tags (evaluation_id, tag)
                VALUES (?, ?)
            """, (evaluation_id, tag.strip().lower()))
    
    def update_file_info(self, simulation_id: str, file_path: str, 
                        file_format: str, file_size_mb: float):
        """–û–Ω–æ–≤–ª—é—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ —Ñ–∞–π–ª –∑ –¥–∞–Ω–∏–º–∏"""
        
        cursor = self.connection.cursor()
        
        cursor.execute("""
            UPDATE evaluations 
            SET file_path = ?, file_format = ?, file_size_mb = ?
            WHERE simulation_id = ?
        """, (file_path, file_format, file_size_mb, simulation_id))
        
        self.connection.commit()
    
    # =============================================================================
    # === –ü–û–®–£–ö –¢–ê –§–Ü–õ–¨–¢–†–ê–¶–Ü–Ø ===
    # =============================================================================
    
    def search_evaluations(self, 
                          series_id: Optional[str] = None,
                          min_score: Optional[float] = None,
                          max_score: Optional[float] = None,
                          tags: Optional[List[str]] = None,
                          date_from: Optional[str] = None,
                          date_to: Optional[str] = None,
                          text_search: Optional[str] = None,
                          limit: int = 100) -> pd.DataFrame:
        """
        –ü–æ—à—É–∫ –æ—Ü—ñ–Ω—é–≤–∞–Ω—å –∑–∞ –∫—Ä–∏—Ç–µ—Ä—ñ—è–º–∏
        
        Args:
            series_id: ID —Å–µ—Ä—ñ—ó –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
            min_score: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∑–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
            max_score: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∑–∞–≥–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞
            tags: –°–ø–∏—Å–æ–∫ —Ç–µ–≥—ñ–≤ (OR –ª–æ–≥—ñ–∫–∞)
            date_from: –î–∞—Ç–∞ –≤—ñ–¥ (ISO —Ñ–æ—Ä–º–∞—Ç)
            date_to: –î–∞—Ç–∞ –¥–æ (ISO —Ñ–æ—Ä–º–∞—Ç)
            text_search: –ü–æ—à—É–∫ –≤ –æ–ø–∏—Å—ñ —Ç–∞ –ø—Ä–∏–º—ñ—Ç–∫–∞—Ö
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            
        Returns:
            DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ—à—É–∫—É
        """
        
        query_parts = ["SELECT DISTINCT e.* FROM evaluations e"]
        where_conditions = []
        params = []
        
        # JOIN –∑ —Ç–µ–≥–∞–º–∏ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if tags:
            query_parts.append("JOIN evaluation_tags et ON e.id = et.evaluation_id")
        
        # –£–º–æ–≤–∏ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó
        if series_id:
            where_conditions.append("e.series_id = ?")
            params.append(series_id)
        
        if min_score is not None:
            where_conditions.append("e.overall_score >= ?")
            params.append(min_score)
        
        if max_score is not None:
            where_conditions.append("e.overall_score <= ?")
            params.append(max_score)
        
        if date_from:
            where_conditions.append("e.timestamp >= ?")
            params.append(date_from)
        
        if date_to:
            where_conditions.append("e.timestamp <= ?")
            params.append(date_to)
        
        if text_search:
            where_conditions.append("(e.description LIKE ? OR e.notes LIKE ?)")
            search_term = f"%{text_search}%"
            params.extend([search_term, search_term])
        
        if tags:
            tag_placeholders = ",".join("?" * len(tags))
            where_conditions.append(f"et.tag IN ({tag_placeholders})")
            params.extend([tag.strip().lower() for tag in tags])
        
        # –°–∫–ª–∞–¥–∞—î–º–æ –∑–∞–ø–∏—Ç
        if where_conditions:
            query_parts.append("WHERE " + " AND ".join(where_conditions))
        
        query_parts.append("ORDER BY e.overall_score DESC, e.timestamp DESC")
        query_parts.append(f"LIMIT {limit}")
        
        query = " ".join(query_parts)
        
        return pd.read_sql_query(query, self.connection, params=params)
    
    def get_best_evaluations(self, limit: int = 10, 
                           series_id: Optional[str] = None) -> pd.DataFrame:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –Ω–∞–π–∫—Ä–∞—â—ñ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è –∑–∞ –∑–∞–≥–∞–ª—å–Ω–æ—é –æ—Ü—ñ–Ω–∫–æ—é"""
        
        return self.search_evaluations(
            series_id=series_id,
            limit=limit
        ).head(limit)
    
    def get_recent_evaluations(self, days: int = 7, 
                             limit: int = 20) -> pd.DataFrame:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –Ω–µ–¥–∞–≤–Ω—ñ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è"""
        
        cutoff_date = (datetime.now() - pd.Timedelta(days=days)).isoformat()
        
        return self.search_evaluations(
            date_from=cutoff_date,
            limit=limit
        )
    
    def get_evaluation_by_id(self, evaluation_id: int) -> Optional[Dict]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è –∑–∞ ID"""
        
        cursor = self.connection.cursor()
        
        cursor.execute("SELECT * FROM evaluations WHERE id = ?", (evaluation_id,))
        row = cursor.fetchone()
        
        if row:
            return dict(row)
        return None
    
    def get_evaluation_by_simulation_id(self, simulation_id: str) -> Optional[Dict]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è –∑–∞ simulation_id"""
        
        cursor = self.connection.cursor()
        
        cursor.execute("SELECT * FROM evaluations WHERE simulation_id = ?", (simulation_id,))
        row = cursor.fetchone()
        
        if row:
            return dict(row)
        return None
    
    # =============================================================================
    # === –ê–ù–ê–õ–Ü–¢–ò–ö–ê –¢–ê –ó–í–Ü–¢–ò ===
    # =============================================================================
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–≤–µ—Ä—Ç–∞—î –∑–∞–≥–∞–ª—å–Ω—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"""
        
        cursor = self.connection.cursor()
        
        stats = {}
        
        # –ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å
        cursor.execute("SELECT COUNT(*) FROM evaluations")
        stats['total_evaluations'] = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM experiment_series")
        stats['total_series'] = cursor.fetchone()[0]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ü—ñ–Ω–æ–∫
        cursor.execute("""
            SELECT 
                AVG(overall_score) as avg_score,
                MIN(overall_score) as min_score,
                MAX(overall_score) as max_score,
                AVG(process_stability) as avg_stability,
                AVG(ekf_consistency) as avg_ekf_consistency,
                AVG(trust_stability_index) as avg_trust_stability
            FROM evaluations
        """)
        score_stats = cursor.fetchone()
        if score_stats:
            stats.update({
                'avg_score': round(score_stats[0] or 0, 2),
                'min_score': round(score_stats[1] or 0, 2),
                'max_score': round(score_stats[2] or 0, 2),
                'avg_stability': round(score_stats[3] or 0, 3),
                'avg_ekf_consistency': round(score_stats[4] or 0, 3),
                'avg_trust_stability': round(score_stats[5] or 0, 3)
            })
        
        # –¢–æ–ø —Ç–µ–≥–∏
        cursor.execute("""
            SELECT tag, COUNT(*) as count 
            FROM evaluation_tags 
            GROUP BY tag 
            ORDER BY count DESC 
            LIMIT 10
        """)
        top_tags = cursor.fetchall()
        stats['top_tags'] = [(tag, count) for tag, count in top_tags]
        
        # –î–∏–Ω–∞–º—ñ–∫–∞ –ø–æ —á–∞—Å—É (–æ—Å—Ç–∞–Ω–Ω—ñ 30 –¥–Ω—ñ–≤)
        cutoff_date = (datetime.now() - pd.Timedelta(days=30)).isoformat()
        cursor.execute("""
            SELECT 
                DATE(timestamp) as date,
                COUNT(*) as count,
                AVG(overall_score) as avg_score
            FROM evaluations 
            WHERE timestamp >= ?
            GROUP BY DATE(timestamp)
            ORDER BY date
        """, (cutoff_date,))
        recent_activity = cursor.fetchall()
        stats['recent_activity'] = [(date, count, round(score or 0, 1)) 
                                   for date, count, score in recent_activity]
        
        return stats
    
    def get_series_comparison(self) -> pd.DataFrame:
        """–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å–µ—Ä—ñ–π –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤"""
        
        query = """
            SELECT 
                s.series_id,
                s.name,
                COUNT(e.id) as evaluation_count,
                AVG(e.overall_score) as avg_score,
                MAX(e.overall_score) as best_score,
                AVG(e.process_stability) as avg_stability,
                AVG(e.ekf_consistency) as avg_ekf_consistency,
                AVG(e.initial_training_time) as avg_training_time,
                AVG(e.avg_prediction_time) as avg_prediction_time
            FROM experiment_series s
            LEFT JOIN evaluations e ON s.series_id = e.series_id
            GROUP BY s.series_id, s.name
            ORDER BY avg_score DESC
        """
        
        return pd.read_sql_query(query, self.connection)
    
    def get_parameter_analysis(self) -> pd.DataFrame:
        """–ê–Ω–∞–ª—ñ–∑ –≤–ø–ª–∏–≤—É –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏"""
        
        query = """
            SELECT 
                horizon,
                delta_u_max,
                lambda_u,
                COUNT(*) as count,
                AVG(overall_score) as avg_score,
                AVG(process_stability) as avg_stability,
                AVG(ekf_consistency) as avg_ekf,
                AVG(trust_stability_index) as avg_trust
            FROM evaluations
            GROUP BY horizon, delta_u_max, lambda_u
            HAVING count >= 2
            ORDER BY avg_score DESC
        """
        
        return pd.read_sql_query(query, self.connection)
    
    def get_performance_trends(self, metric: str = 'overall_score', 
                             days: int = 90) -> pd.DataFrame:
        """–ê–Ω–∞–ª—ñ–∑ —Ç—Ä–µ–Ω–¥—ñ–≤ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –∑–∞ —á–∞—Å–æ–º"""
        
        cutoff_date = (datetime.now() - pd.Timedelta(days=days)).isoformat()
        
        query = f"""
            SELECT 
                DATE(timestamp) as date,
                AVG({metric}) as avg_value,
                MIN({metric}) as min_value,
                MAX({metric}) as max_value,
                COUNT(*) as evaluation_count
            FROM evaluations
            WHERE timestamp >= ?
            GROUP BY DATE(timestamp)
            ORDER BY date
        """
        
        return pd.read_sql_query(query, self.connection, params=[cutoff_date])
    
    # =============================================================================
    # === EXPORT/IMPORT ===
    # =============================================================================
    
    def export_to_csv(self, output_dir: str = "database_export") -> List[str]:
        """–ï–∫—Å–ø–æ—Ä—Ç—É—î –≤—Å—é –±–∞–∑—É –¥–∞–Ω–∏—Ö –≤ CSV —Ñ–∞–π–ª–∏"""
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        exported_files = []
        
        # –ï–∫—Å–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω–∏—Ö —Ç–∞–±–ª–∏—Ü—å
        tables = {
            'evaluations': 'SELECT * FROM evaluations',
            'experiment_series': 'SELECT * FROM experiment_series',
            'detailed_metrics': 'SELECT * FROM detailed_metrics',
            'evaluation_tags': 'SELECT * FROM evaluation_tags'
        }
        
        for table_name, query in tables.items():
            df = pd.read_sql_query(query, self.connection)
            
            if not df.empty:
                file_path = output_path / f"{table_name}.csv"
                df.to_csv(file_path, index=False)
                exported_files.append(str(file_path))
                self.logger.info(f"–ï–∫—Å–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ {table_name}: {len(df)} –∑–∞–ø–∏—Å—ñ–≤")
        
        # –ï–∫—Å–ø–æ—Ä—Ç –∞–Ω–∞–ª—ñ—Ç–∏–∫–∏
        stats = self.get_statistics()
        stats_df = pd.DataFrame([stats])
        stats_path = output_path / "statistics.csv"
        stats_df.to_csv(stats_path, index=False)
        exported_files.append(str(stats_path))
        
        return exported_files
    
    def backup_database(self, backup_path: str = None) -> str:
        """–°—Ç–≤–æ—Ä—é—î —Ä–µ–∑–µ—Ä–≤–Ω—É –∫–æ–ø—ñ—é –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"""
        
        if backup_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"evaluation_backup_{timestamp}.db"
        
        # –ü—Ä–æ—Å—Ç–∏–π —Å–ø–æ—Å—ñ–± - –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É
        import shutil
        shutil.copy2(self.db_path, backup_path)
        
        self.logger.info(f"–†–µ–∑–µ—Ä–≤–Ω–∞ –∫–æ–ø—ñ—è —Å—Ç–≤–æ—Ä–µ–Ω–∞: {backup_path}")
        return backup_path
    
    # =============================================================================
    # === –£–ü–†–ê–í–õ–Ü–ù–ù–Ø –î–ê–ù–ò–ú–ò ===
    # =============================================================================
    
    def delete_evaluation(self, evaluation_id: int) -> bool:
        """–í–∏–¥–∞–ª—è—î –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è –∑–∞ ID"""
        
        try:
            cursor = self.connection.cursor()
            
            # –í–∏–¥–∞–ª—è—î–º–æ –ø–æ–≤'—è–∑–∞–Ω—ñ –¥–∞–Ω—ñ
            cursor.execute("DELETE FROM detailed_metrics WHERE evaluation_id = ?", (evaluation_id,))
            cursor.execute("DELETE FROM evaluation_tags WHERE evaluation_id = ?", (evaluation_id,))
            cursor.execute("DELETE FROM evaluations WHERE id = ?", (evaluation_id,))
            
            self.connection.commit()
            
            if cursor.rowcount > 0:
                self.logger.info(f"–í–∏–¥–∞–ª–µ–Ω–æ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è ID: {evaluation_id}")
                return True
            else:
                self.logger.warning(f"–û—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è ID {evaluation_id} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                return False
                
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è {evaluation_id}: {e}")
            return False
    
    def cleanup_old_evaluations(self, days_old: int = 90) -> int:
        """–í–∏–¥–∞–ª—è—î —Å—Ç–∞—Ä—ñ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è"""
        
        cutoff_date = (datetime.now() - pd.Timedelta(days=days_old)).isoformat()
        
        # –û—Ç—Ä–∏–º—É—î–º–æ ID —Å—Ç–∞—Ä–∏—Ö –∑–∞–ø–∏—Å—ñ–≤
        old_ids = pd.read_sql_query("""
            SELECT id FROM evaluations WHERE timestamp < ?
        """, self.connection, params=[cutoff_date])
        
        deleted_count = 0
        for _, row in old_ids.iterrows():
            if self.delete_evaluation(row['id']):
                deleted_count += 1
        
        self.logger.info(f"–í–∏–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä–∏—Ö –æ—Ü—ñ–Ω—é–≤–∞–Ω—å (—Å—Ç–∞—Ä—à–µ {days_old} –¥–Ω—ñ–≤)")
        return deleted_count
    
    def update_tags(self, evaluation_id: int, tags: List[str]) -> bool:
        """–û–Ω–æ–≤–ª—é—î —Ç–µ–≥–∏ –¥–ª—è –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è"""
        
        try:
            cursor = self.connection.cursor()
            
            # –í–∏–¥–∞–ª—è—î–º–æ —Å—Ç–∞—Ä—ñ —Ç–µ–≥–∏
            cursor.execute("DELETE FROM evaluation_tags WHERE evaluation_id = ?", (evaluation_id,))
            
            # –î–æ–¥–∞—î–º–æ –Ω–æ–≤—ñ
            self._add_tags(evaluation_id, tags)
            
            # –û–Ω–æ–≤–ª—é—î–º–æ JSON –≤ –æ—Å–Ω–æ–≤–Ω—ñ–π —Ç–∞–±–ª–∏—Ü—ñ
            cursor.execute("""
                UPDATE evaluations SET tags = ? WHERE id = ?
            """, (json.dumps(tags), evaluation_id))
            
            self.connection.commit()
            return True
            
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–µ–≥—ñ–≤ –¥–ª—è {evaluation_id}: {e}")
            return False
    
    def add_notes(self, evaluation_id: int, notes: str) -> bool:
        """–î–æ–¥–∞—î –∞–±–æ –æ–Ω–æ–≤–ª—é—î –ø—Ä–∏–º—ñ—Ç–∫–∏ –¥–æ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è"""
        
        try:
            cursor = self.connection.cursor()
            
            cursor.execute("""
                UPDATE evaluations SET notes = ? WHERE id = ?
            """, (notes, evaluation_id))
            
            self.connection.commit()
            
            if cursor.rowcount > 0:
                self.logger.info(f"–û–Ω–æ–≤–ª–µ–Ω–æ –ø—Ä–∏–º—ñ—Ç–∫–∏ –¥–ª—è –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è {evaluation_id}")
                return True
            else:
                self.logger.warning(f"–û—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è {evaluation_id} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                return False
                
        except Exception as e:
            self.logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–∏–º—ñ—Ç–æ–∫ –¥–ª—è {evaluation_id}: {e}")
            return False

# =============================================================================
# === –£–¢–Ü–õ–Ü–¢–ò –¢–ê –Ü–ù–¢–ï–ì–†–ê–¶–Ü–Ø ===
# =============================================================================

class EvaluationAnalyzer:
    """–ê–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –¥–ª—è —Ä–æ–±–æ—Ç–∏ –∑ –±–∞–∑–æ—é –¥–∞–Ω–∏—Ö –æ—Ü—ñ–Ω—é–≤–∞–Ω—å"""
    
    def __init__(self, database: EvaluationDatabase):
        self.db = database
    
    def find_optimal_parameters(self, target_metric: str = 'overall_score',
                              min_evaluations: int = 3) -> pd.DataFrame:
        """
        –ó–Ω–∞—Ö–æ–¥–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        
        Args:
            target_metric: –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
            min_evaluations: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ—Ü—ñ–Ω—é–≤–∞–Ω—å –¥–ª—è –≥—Ä—É–ø–∏
            
        Returns:
            DataFrame –∑ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        
        query = f"""
            SELECT 
                horizon,
                delta_u_max,
                lambda_u,
                ref_fe,
                ref_mass,
                COUNT(*) as evaluation_count,
                AVG({target_metric}) as avg_target,
                STDEV({target_metric}) as std_target,
                MAX({target_metric}) as best_target
            FROM evaluations
            GROUP BY horizon, delta_u_max, lambda_u, ref_fe, ref_mass
            HAVING evaluation_count >= ?
            ORDER BY avg_target DESC, std_target ASC
        """
        
        try:
            return pd.read_sql_query(query, self.db.connection, params=[min_evaluations])
        except Exception as e:
            # Fallback –±–µ–∑ STDEV —Ñ—É–Ω–∫—Ü—ñ—ó (SQLite –º–æ–∂–µ –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞—Ç–∏)
            query_simple = f"""
                SELECT 
                    horizon,
                    delta_u_max,
                    lambda_u,
                    ref_fe,
                    ref_mass,
                    COUNT(*) as evaluation_count,
                    AVG({target_metric}) as avg_target,
                    MAX({target_metric}) as best_target
                FROM evaluations
                GROUP BY horizon, delta_u_max, lambda_u, ref_fe, ref_mass
                HAVING evaluation_count >= ?
                ORDER BY avg_target DESC
            """
            return pd.read_sql_query(query_simple, self.db.connection, params=[min_evaluations])
    
    def analyze_failure_patterns(self, score_threshold: float = 50.0) -> Dict[str, Any]:
        """–ê–Ω–∞–ª—ñ–∑—É—î –ø–∞—Ç–µ—Ä–Ω–∏ –Ω–µ—É—Å–ø—ñ—à–Ω–∏—Ö —Å–∏–º—É–ª—è—Ü—ñ–π"""
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –Ω–µ—É—Å–ø—ñ—à–Ω—ñ —Å–∏–º—É–ª—è—Ü—ñ—ó
        failed = self.db.search_evaluations(max_score=score_threshold)
        successful = self.db.search_evaluations(min_score=score_threshold + 20)
        
        analysis = {
            'failed_count': len(failed),
            'successful_count': len(successful),
            'failure_rate': len(failed) / (len(failed) + len(successful)) if (len(failed) + len(successful)) > 0 else 0
        }
        
        if len(failed) > 0:
            analysis['common_failed_params'] = {
                'avg_horizon': failed['horizon'].mean(),
                'avg_delta_u_max': failed['delta_u_max'].mean(),
                'avg_lambda_u': failed['lambda_u'].mean(),
                'avg_ekf_consistency': failed['ekf_consistency'].mean(),
                'avg_trust_stability': failed['trust_stability_index'].mean()
            }
        
        if len(successful) > 0:
            analysis['common_successful_params'] = {
                'avg_horizon': successful['horizon'].mean(),
                'avg_delta_u_max': successful['delta_u_max'].mean(),
                'avg_lambda_u': successful['lambda_u'].mean(),
                'avg_ekf_consistency': successful['ekf_consistency'].mean(),
                'avg_trust_stability': successful['trust_stability_index'].mean()
            }
        
        return analysis
    
    def suggest_experiments(self, current_best_score: float = None) -> List[Dict]:
        """–ü—Ä–æ–ø–æ–Ω—É—î –Ω–æ–≤—ñ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ gap analysis"""
        
        if current_best_score is None:
            # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ—Ç–æ—á–Ω–∏–π –Ω–∞–π–∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            best = self.db.get_best_evaluations(limit=1)
            if len(best) > 0:
                current_best_score = best.iloc[0]['overall_score']
            else:
                current_best_score = 0.0
        
        suggestions = []
        
        # –ê–Ω–∞–ª—ñ–∑ –Ω–µ–¥–æ—Å–ª—ñ–¥–∂–µ–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        param_ranges = pd.read_sql_query("""
            SELECT 
                MIN(horizon) as min_horizon, MAX(horizon) as max_horizon,
                MIN(delta_u_max) as min_delta_u, MAX(delta_u_max) as max_delta_u,
                MIN(lambda_u) as min_lambda, MAX(lambda_u) as max_lambda
            FROM evaluations
        """, self.db.connection)
        
        if len(param_ranges) > 0:
            ranges = param_ranges.iloc[0]
            
            # –ü—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø–æ—Ç–æ—á–Ω–∏—Ö –¥—ñ–∞–ø–∞–∑–æ–Ω—ñ–≤
            suggestions.extend([
                {
                    'type': 'parameter_exploration',
                    'description': '–î–æ—Å–ª—ñ–¥–∏—Ç–∏ –±—ñ–ª—å—à–∏–π –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è',
                    'params': {'horizon': int(ranges['max_horizon'] * 1.5)},
                    'rationale': '–ó–±—ñ–ª—å—à–µ–Ω–Ω—è –≥–æ—Ä–∏–∑–æ–Ω—Ç—É –º–æ–∂–µ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –¥–æ–≤–≥–æ—Å—Ç—Ä–æ–∫–æ–≤–µ –∫–µ—Ä—É–≤–∞–Ω–Ω—è'
                },
                {
                    'type': 'parameter_exploration', 
                    'description': '–î–æ—Å–ª—ñ–¥–∏—Ç–∏ –º–µ–Ω—à–∏–π lambda_u –¥–ª—è –±—ñ–ª—å—à–æ—ó –∞–≥—Ä–µ—Å–∏–≤–Ω–æ—Å—Ç—ñ',
                    'params': {'lambda_u': float(ranges['min_lambda'] * 0.5)},
                    'rationale': '–ó–º–µ–Ω—à–µ–Ω–Ω—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó –º–æ–∂–µ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è'
                },
                {
                    'type': 'robustness_test',
                    'description': '–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è —Å—Ç—ñ–π–∫–æ—Å—Ç—ñ –∑ —Ä—ñ–∑–Ω–∏–º–∏ –∑–±—É—Ä–µ–Ω–Ω—è–º–∏',
                    'params': {'add_disturbances': True},
                    'rationale': '–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–æ–±–æ—Ç–∏ –≤ —É–º–æ–≤–∞—Ö –Ω–µ–≤–∏–∑–Ω–∞—á–µ–Ω–æ—Å—Ç—ñ'
                }
            ])
        
        return suggestions

# =============================================================================
# === –§–£–ù–ö–¶–Ü–á –í–ò–°–û–ö–û–ì–û –†–Ü–í–ù–Ø ===
# =============================================================================

def create_evaluation_database(db_path: str = "evaluation_results.db") -> EvaluationDatabase:
    """–°—Ç–≤–æ—Ä—é—î –∞–±–æ –ø—ñ–¥–∫–ª—é—á–∞—î—Ç—å—Å—è –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –æ—Ü—ñ–Ω—é–≤–∞–Ω—å"""
    return EvaluationDatabase(db_path)

def quick_add_to_database(package: 'EvaluationPackage', 
                         db_path: str = "evaluation_results.db",
                         series_id: Optional[str] = None,
                         tags: List[str] = None) -> int:
    """
    –®–≤–∏–¥–∫–æ –¥–æ–¥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
    
    Args:
        package: –ü–∞–∫–µ—Ç –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è
        db_path: –®–ª—è—Ö –¥–æ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
        series_id: ID —Å–µ—Ä—ñ—ó –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
        tags: –¢–µ–≥–∏ –¥–ª—è –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è
        
    Returns:
        ID —Å—Ç–≤–æ—Ä–µ–Ω–æ–≥–æ –∑–∞–ø–∏—Å—É
    """
    
    with EvaluationDatabase(db_path) as db:
        return db.add_evaluation(package, series_id=series_id, tags=tags)

def search_database(db_path: str = "evaluation_results.db", **kwargs) -> pd.DataFrame:
    """–ü–æ—à—É–∫ –≤ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    
    with EvaluationDatabase(db_path) as db:
        return db.search_evaluations(**kwargs)

def get_database_stats(db_path: str = "evaluation_results.db") -> Dict[str, Any]:
    """–û—Ç—Ä–∏–º—É—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"""
    
    with EvaluationDatabase(db_path) as db:
        return db.get_statistics()

# =============================================================================
# === –ü–†–ò–ö–õ–ê–î –í–ò–ö–û–†–ò–°–¢–ê–ù–ù–Ø ===
# =============================================================================

if __name__ == "__main__":
    print("üóÑÔ∏è evaluation_database.py - –ë–∞–∑–∞ –¥–∞–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –æ—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è")
    print("\n–û—Å–Ω–æ–≤–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó:")
    print("1. EvaluationDatabase() - –ü–æ–≤–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –ë–î")
    print("2. quick_add_to_database() - –®–≤–∏–¥–∫–µ –¥–æ–¥–∞–≤–∞–Ω–Ω—è")
    print("3. search_database() - –ü–æ—à—É–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
    print("4. EvaluationAnalyzer() - –ê–Ω–∞–ª—ñ—Ç–∏—á–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏")
    
    print("\n–ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:")
    print("""
# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
from evaluation_database import EvaluationDatabase, EvaluationAnalyzer

# –û—Å–Ω–æ–≤–Ω–∞ —Ä–æ–±–æ—Ç–∞ –∑ –ë–î
db = EvaluationDatabase("my_experiments.db")

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–µ—Ä—ñ—ó –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
db.create_experiment_series(
    series_id="trust_region_study",
    name="–î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è Trust Region –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤",
    description="–°–∏—Å—Ç–µ–º–∞—Ç–∏—á–Ω–µ –≤–∏–≤—á–µ–Ω–Ω—è –≤–ø–ª–∏–≤—É Trust Region –Ω–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å",
    tags=["trust_region", "optimization", "mpc"]
)

# –î–æ–¥–∞–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ (–ø—Ä–∏–ø—É—Å–∫–∞—î–º–æ —â–æ package –≤–∂–µ —Å—Ç–≤–æ—Ä–µ–Ω–∏–π)
eval_id = db.add_evaluation(
    package=evaluation_package,
    series_id="trust_region_study", 
    tags=["baseline", "test_run"],
    notes="–ü–æ—á–∞—Ç–∫–æ–≤–∏–π —Ç–µ—Å—Ç –∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"
)

# –ü–æ—à—É–∫ –Ω–∞–π–∫—Ä–∞—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
best_results = db.get_best_evaluations(limit=5)
print("–¢–æ–ø 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤:")
print(best_results[['simulation_id', 'overall_score', 'description']])

# –ü–æ—à—É–∫ –∑–∞ –∫—Ä–∏—Ç–µ—Ä—ñ—è–º–∏
high_ekf = db.search_evaluations(
    min_score=70.0,
    tags=["trust_region"],
    date_from="2024-01-01"
)

# –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞
analyzer = EvaluationAnalyzer(db)

# –ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
optimal_params = analyzer.find_optimal_parameters('overall_score')
print("–û–ø—Ç–∏–º–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:")
print(optimal_params.head())

# –ê–Ω–∞–ª—ñ–∑ –Ω–µ—É—Å–ø—ñ—à–Ω–∏—Ö —Å–∏–º—É–ª—è—Ü—ñ–π  
failures = analyzer.analyze_failure_patterns(score_threshold=60.0)
print(f"–ß–∞—Å—Ç–æ—Ç–∞ –Ω–µ–≤–¥–∞—á: {failures['failure_rate']:.2%}")

# –ü—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó –Ω–æ–≤–∏—Ö –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤
suggestions = analyzer.suggest_experiments()
for suggestion in suggestions:
    print(f"- {suggestion['description']}: {suggestion['rationale']}")

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö
stats = db.get_statistics()
print(f"–í—Å—å–æ–≥–æ –æ—Ü—ñ–Ω—é–≤–∞–Ω—å: {stats['total_evaluations']}")
print(f"–°–µ—Ä–µ–¥–Ω—è –æ—Ü—ñ–Ω–∫–∞: {stats['avg_score']}")
print(f"–ù–∞–π–∫—Ä–∞—â—ñ —Ç–µ–≥–∏: {stats['top_tags'][:3]}")

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å–µ—Ä—ñ–π
series_comparison = db.get_series_comparison()
print(series_comparison)

# –¢—Ä–µ–Ω–¥–∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
trends = db.get_performance_trends('overall_score', days=30)
print("–¢—Ä–µ–Ω–¥–∏ –∑–∞ 30 –¥–Ω—ñ–≤:")
print(trends)

# –ï–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–∏—Ö
exported_files = db.export_to_csv("analysis_export")
print(f"–ï–∫—Å–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–∏: {exported_files}")

# –†–µ–∑–µ—Ä–≤–Ω–∞ –∫–æ–ø—ñ—è
backup_path = db.backup_database()
print(f"–†–µ–∑–µ—Ä–≤–Ω–∞ –∫–æ–ø—ñ—è: {backup_path}")

db.close()
""")
    
    print("\n–ú–æ–∂–ª–∏–≤–æ—Å—Ç—ñ –ë–î:")
    print("‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö —Ç–∞ –∫–ª—é—á–æ–≤–∏—Ö –º–µ—Ç—Ä–∏–∫")
    print("‚úÖ –û—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—è –≤ —Å–µ—Ä—ñ—ó –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤")
    print("‚úÖ –¢–µ–≥–∏ —Ç–∞ –ø–æ–≤–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–∏–π –ø–æ—à—É–∫")
    print("‚úÖ –ê–Ω–∞–ª—ñ—Ç–∏–∫–∞ —Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    print("‚úÖ –ü–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤")
    print("‚úÖ –ê–Ω–∞–ª—ñ–∑ –ø–∞—Ç—Ç–µ—Ä–Ω—ñ–≤ –Ω–µ–≤–¥–∞—á")
    print("‚úÖ –ü—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó –Ω–æ–≤–∏—Ö –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤")
    print("‚úÖ –ï–∫—Å–ø–æ—Ä—Ç —Ç–∞ —Ä–µ–∑–µ—Ä–≤–Ω–µ –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è")
    print("‚úÖ –¢—Ä–µ–Ω–¥–∏ —Ç–∞ –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å–µ—Ä—ñ–π")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ë–î
    try:
        print("\nüß™ –î–ï–ú–û–ù–°–¢–†–ê–¶–Ü–Ø –°–¢–í–û–†–ï–ù–ù–Ø –ë–î:")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–µ—Å—Ç–æ–≤—É –ë–î
        test_db = EvaluationDatabase("demo_evaluations.db")
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–µ—Å—Ç–æ–≤—É —Å–µ—Ä—ñ—é
        test_db.create_experiment_series(
            series_id="demo_series",
            name="–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∞ —Å–µ—Ä—ñ—è",
            description="–¢–µ—Å—Ç–æ–≤–∞ —Å–µ—Ä—ñ—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ",
            tags=["demo", "test"]
        )
        
        print("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–∏—Ö —Å—Ç–≤–æ—Ä–µ–Ω–∞")
        print("‚úÖ –¢–µ—Å—Ç–æ–≤–∞ —Å–µ—Ä—ñ—è –¥–æ–¥–∞–Ω–∞")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = test_db.get_statistics()
        print(f"–°–µ—Ä—ñ–π –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤: {stats['total_series']}")
        print(f"–û—Ü—ñ–Ω—é–≤–∞–Ω—å: {stats['total_evaluations']}")
        
        test_db.close()
        print("‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó: {e}")