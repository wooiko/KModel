# anomaly_detector.py - –ü–æ–∫—Ä–∞—â–µ–Ω—ñ —ñ—Å–Ω—É—é—á—ñ –∫–ª–∞—Å–∏

import numpy as np
from collections import deque
from typing import Deque, Dict, Iterable, Optional, Tuple

class SignalAnomalyDetector:
    """
    –ü–û–ö–†–ê–©–ï–ù–ò–ô —Ä–æ–±–∞—Å—Ç–Ω–∏–π —Ñ—ñ–ª—å—Ç—Ä –¥–ª—è –æ–¥–Ω–æ–≤–∏–º—ñ—Ä–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª—É.
    –ó–±–µ—Ä—ñ–≥–∞—î –≤—Å—ñ —ñ—Å–Ω—É—é—á—ñ –º–µ—Ç–æ–¥–∏, –∞–ª–µ –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—é –≤–Ω—É—Ç—Ä—ñ—à–Ω—å–æ—é –ª–æ–≥—ñ–∫–æ—é.
    """
    def __init__(self,
                 window: int = 21,               
                 spike_z: float = 4.0,           
                 drop_rel: float = 0.20,         
                 freeze_len: int = 5,            
                 eps: float = 1e-9,
                 enabled: bool = True,
                 # ‚úÖ –ù–û–í–Ü –ü–ê–†–ê–ú–ï–¢–†–ò (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ, –∑–≤–æ—Ä–æ—Ç–Ω–æ —Å—É–º—ñ—Å–Ω—ñ)
                 signal_type: str = 'auto',      # 'auto', 'composition', 'flow'
                 adaptive_sensitivity: bool = True,
                 correlation_check: bool = True,
                 quality_monitoring: bool = True):
        
        if window % 2 == 0:
            window += 1
        self.window = window
        self.spike_z = spike_z
        self.drop_rel = drop_rel
        self.freeze_len = freeze_len
        self.eps = eps
        self.enabled = enabled
        
        # ‚úÖ –ù–û–í–Ü –ê–¢–†–ò–ë–£–¢–ò –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—ñ
        self.signal_type = signal_type
        self.adaptive_sensitivity = adaptive_sensitivity
        self.correlation_check = correlation_check
        self.quality_monitoring = quality_monitoring

        # –Ü—Å–Ω—É—é—á—ñ –∞—Ç—Ä–∏–±—É—Ç–∏
        self.buf: Deque[float] = deque(maxlen=window)
        self.last_good: float | None = None
        self.freeze_cnt = 0
        
        # ‚úÖ –ù–û–í–Ü –ê–¢–†–ò–ë–£–¢–ò –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω—å
        self.clean_buf: Deque[float] = deque(maxlen=window)
        self.current_spike_z = spike_z
        self.current_drop_rel = drop_rel
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É —Å–∏–≥–Ω–∞–ª—É
        self._auto_signal_type_detector = {'values': [], 'samples_needed': 50}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —è–∫–æ—Å—Ç—ñ
        self.quality_metrics = {
            'total_processed': 0,
            'spikes_detected': 0,
            'drops_detected': 0,
            'freezes_detected': 0,
            'corrections_made': 0,
            'false_positives_estimated': 0
        } if quality_monitoring else None

    def _robust_stats(self) -> tuple[float, float]:
        """–ü–û–ö–†–ê–©–ï–ù–ê –≤–µ—Ä—Å—ñ—è —Ä–æ–±–∞—Å—Ç–Ω–∏—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫."""
        if len(self.buf) < 3:
            return 0.0, 1.0
            
        arr = np.asarray(self.buf)
        
        # ‚úÖ –ü–û–ö–†–ê–©–ï–ù–ù–Ø: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—Ä–∏–º–µ–¥—ñ–∞–Ω –¥–ª—è –±—ñ–ª—å—à–æ—ó —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
        if len(arr) >= 5:
            q1, median, q3 = np.percentile(arr, [25, 50, 75])
            tri_median = (q1 + 2*median + q3) / 4
        else:
            tri_median = np.median(arr)
        
        # ‚úÖ –ü–û–ö–†–ê–©–ï–ù–ù–Ø: –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ MAD
        mad = np.median(np.abs(arr - tri_median))
        if mad < self.eps:
            mad = np.std(arr) * 0.6745  # Normalized std —è–∫ fallback
            
        return tri_median, max(mad, self.eps)

    def _auto_detect_signal_type(self, x: float):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É —Å–∏–≥–Ω–∞–ª—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫."""
        if self.signal_type != 'auto':
            return
            
        detector = self._auto_signal_type_detector
        detector['values'].append(x)
        
        if len(detector['values']) >= detector['samples_needed']:
            values = np.array(detector['values'])
            
            # –ê–Ω–∞–ª—ñ–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å–∏–≥–Ω–∞–ª—É
            mean_val = np.mean(values)
            cv = np.std(values) / (np.abs(mean_val) + self.eps)
            
            # –ï–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É
            if 20 <= mean_val <= 80 and cv < 0.3:  # –°—Ö–æ–∂–µ –Ω–∞ –≤—ñ–¥—Å–æ—Ç–æ–∫ —Å–∫–ª–∞–¥—É
                self.signal_type = 'composition'
                self.current_spike_z = self.spike_z * 0.7  # –ë—ñ–ª—å—à —á—É—Ç–ª–∏–≤–∏–π
                self.current_drop_rel = self.drop_rel * 0.7
            elif mean_val > 50 and cv > 0.2:  # –°—Ö–æ–∂–µ –Ω–∞ –ø–æ—Ç—ñ–∫
                self.signal_type = 'flow'
                self.current_spike_z = self.spike_z * 1.2  # –ú–µ–Ω—à —á—É—Ç–ª–∏–≤–∏–π
                self.current_drop_rel = self.drop_rel * 1.2
            else:
                self.signal_type = 'process'  # –ó–∞–≥–∞–ª—å–Ω–∏–π –ø—Ä–æ—Ü–µ—Å–Ω–∏–π —Å–∏–≥–Ω–∞–ª
                
            # –û—á–∏—â–∞—î–º–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä –ø—ñ—Å–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è
            detector['values'].clear()

    def _update_adaptive_thresholds(self):
        """–ù–û–í–ê —Ñ—É–Ω–∫—Ü—ñ—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–æ—Ä–æ–≥—ñ–≤."""
        if not self.adaptive_sensitivity or not self.quality_metrics:
            return
            
        if self.quality_metrics['total_processed'] < 20:
            return
            
        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —á–∞—Å—Ç–æ—Ç–∏ –∫–æ—Ä–µ–∫—Ü—ñ–π
        correction_rate = self.quality_metrics['corrections_made'] / self.quality_metrics['total_processed']
        
        # –ê–¥–∞–ø—Ç–∞—Ü—ñ—è –ø–æ—Ä–æ–≥—ñ–≤
        if correction_rate > 0.3:  # –ó–∞–Ω–∞–¥—Ç–æ –±–∞–≥–∞—Ç–æ –∫–æ—Ä–µ–∫—Ü—ñ–π
            self.current_spike_z = min(self.current_spike_z * 1.1, self.spike_z * 2)
            self.current_drop_rel = min(self.current_drop_rel * 1.1, self.drop_rel * 2)
        elif correction_rate < 0.05:  # –ó–∞–º–∞–ª–æ –∫–æ—Ä–µ–∫—Ü—ñ–π
            self.current_spike_z = max(self.current_spike_z * 0.95, self.spike_z * 0.5)
            self.current_drop_rel = max(self.current_drop_rel * 0.95, self.drop_rel * 0.5)

    def _validate_correction(self, original: float, corrected: float) -> bool:
        """–ù–û–í–ê —Ñ—É–Ω–∫—Ü—ñ—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –∫–æ—Ä–µ–∫—Ü—ñ—ó."""
        if len(self.clean_buf) < 3:
            return True
            
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç—Ä–µ–Ω–¥
        recent_clean = list(self.clean_buf)[-3:]
        if len(recent_clean) >= 2:
            trend = np.diff(recent_clean)
            if len(trend) >= 1:
                expected_next = recent_clean[-1] + np.mean(trend)
                correction_error = abs(corrected - expected_next)
                original_error = abs(original - expected_next)
                
                # –ö–æ—Ä–µ–∫—Ü—ñ—è –º–∞—î –ø–æ–∫—Ä–∞—â—É–≤–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ—Å—Ç—å —Ç—Ä–µ–Ω–¥—É
                return correction_error <= original_error * 1.3
                
        return True

    def update(self, x: float, correlation_signal: Optional[float] = None) -> float:
        """
        –ü–û–ö–†–ê–©–ï–ù–ê –≤–µ—Ä—Å—ñ—è update –∑ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ—ó —Å–∏–≥–Ω–∞—Ç—É—Ä–∏.
        
        ‚úÖ –ù–æ–≤–∏–π –æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä correlation_signal –¥–ª—è –ø–µ—Ä–µ—Ö—Ä–µ—Å–Ω–æ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        """
        if not self.enabled:
            self.last_good = x
            return x

        # ‚úÖ –ù–û–í–ï: –û–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if self.quality_metrics:
            self.quality_metrics['total_processed'] += 1

        # ‚úÖ –ù–û–í–ï: –ê–≤—Ç–æ–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É —Å–∏–≥–Ω–∞–ª—É
        self._auto_detect_signal_type(x)

        self.buf.append(x)
        if len(self.buf) < 3:
            self.last_good = x
            self.clean_buf.append(x)
            return x

        med, mad = self._robust_stats()
        x_corrected = x
        correction_made = False

        # 1. freeze: n —Ä–∞–∑—ñ–≤ –ø—ñ–¥—Ä—è–¥ ‚âà –æ–¥–Ω–∞–∫–æ–≤–æ (–ø–æ–∫—Ä–∞—â–µ–Ω–∞ –ª–æ–≥—ñ–∫–∞)
        if len(self.buf) >= self.freeze_len:
            recent_range = np.ptp(list(self.buf)[-self.freeze_len:])
            if recent_range < self.eps:
                self.freeze_cnt += 1
            else:
                self.freeze_cnt = 0

            if self.freeze_cnt >= self.freeze_len:
                if self.last_good is not None:
                    # ‚úÖ –ü–û–ö–†–ê–©–ï–ù–ù–Ø: –î–æ–¥–∞—î–º–æ –Ω–µ–≤–µ–ª–∏–∫–∏–π —à—É–º –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–Ω—è
                    noise = np.random.normal(0, mad * 0.01) if mad > 0 else 0
                    x_corrected = self.last_good + noise
                    correction_made = True
                    if self.quality_metrics:
                        self.quality_metrics['freezes_detected'] += 1

        # 2. spike (robust-z) –∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–∏–º –ø–æ—Ä–æ–≥–æ–º
        if not correction_made:
            z = abs(x - med) / mad
            if z > self.current_spike_z:
                x_corrected = med
                correction_made = True
                if self.quality_metrics:
                    self.quality_metrics['spikes_detected'] += 1

        # 3. drop (—Ä–∞–ø—Ç–æ–≤–µ –ø–∞–¥—ñ–Ω–Ω—è) –∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–∏–º –ø–æ—Ä–æ–≥–æ–º
        if not correction_made and abs(med) > self.eps:
            drop_ratio = (med - x) / abs(med)
            if drop_ratio > self.current_drop_rel:
                # ‚úÖ –ü–û–ö–†–ê–©–ï–ù–ù–Ø: –ß–∞—Å—Ç–∫–æ–≤–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è –∑–∞–º—ñ—Å—Ç—å –ø–æ–≤–Ω–æ—ó
                x_corrected = med - (med * self.current_drop_rel * 0.5)
                correction_made = True
                if self.quality_metrics:
                    self.quality_metrics['drops_detected'] += 1

        # ‚úÖ –ù–û–í–ï: –í–∞–ª—ñ–¥–∞—Ü—ñ—è –∫–æ—Ä–µ–∫—Ü—ñ—ó
        if correction_made:
            if not self._validate_correction(x, x_corrected):
                x_corrected = x
                correction_made = False
                if self.quality_metrics:
                    self.quality_metrics['false_positives_estimated'] += 1
            else:
                if self.quality_metrics:
                    self.quality_metrics['corrections_made'] += 1

        # ‚úÖ –ù–û–í–ï: –ü–µ—Ä–µ—Ö—Ä–µ—Å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è –∑ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏–º —Å–∏–≥–Ω–∞–ª–æ–º
        if (correction_made and correlation_signal is not None and 
            self.correlation_check and len(self.buf) >= 5):
            
            # –ü—Ä–æ—Å—Ç–∏–π –ø—Ä–∏–∫–ª–∞–¥ –ø–µ—Ä–µ—Ö—Ä–µ—Å–Ω–æ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
            signal_change = abs(x - x_corrected) / (abs(x) + self.eps)
            if signal_change > 0.5:  # –í–µ–ª–∏–∫–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏–π —Å–∏–≥–Ω–∞–ª —Ç–µ–∂ –∑–º—ñ–Ω–∏–≤—Å—è
                if hasattr(self, '_last_correlation_signal'):
                    corr_change = abs(correlation_signal - self._last_correlation_signal) / (abs(self._last_correlation_signal) + self.eps)
                    if corr_change < 0.05:  # –ö–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏–π —Å–∏–≥–Ω–∞–ª —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π
                        # –ó–º–µ–Ω—à—É—î–º–æ –∫–æ—Ä–µ–∫—Ü—ñ—é
                        x_corrected = x * 0.7 + x_corrected * 0.3
            
            self._last_correlation_signal = correlation_signal

        # ‚úÖ –ù–û–í–ï: –û–Ω–æ–≤–ª–µ–Ω–Ω—è –∞–¥–∞–ø—Ç–∏–≤–Ω–∏—Ö –ø–æ—Ä–æ–≥—ñ–≤
        if self.quality_metrics and self.quality_metrics['total_processed'] % 20 == 0:
            self._update_adaptive_thresholds()

        self.last_good = x_corrected
        self.clean_buf.append(x_corrected)
        
        return x_corrected

    # ‚úÖ –ù–û–í–Ü –ú–ï–¢–û–î–ò (–Ω–µ –ø–æ—Ä—É—à—É—é—Ç—å —ñ—Å–Ω—É—é—á–∏–π API)
    def get_quality_report(self) -> Dict:
        """–ù–æ–≤–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∑–≤—ñ—Ç—É –ø—Ä–æ —è–∫—ñ—Å—Ç—å —Ä–æ–±–æ—Ç–∏."""
        if not self.quality_metrics:
            return {'quality_monitoring': False}
            
        total = max(1, self.quality_metrics['total_processed'])
        return {
            **self.quality_metrics,
            'correction_rate': self.quality_metrics['corrections_made'] / total,
            'spike_rate': self.quality_metrics['spikes_detected'] / total,
            'drop_rate': self.quality_metrics['drops_detected'] / total,
            'freeze_rate': self.quality_metrics['freezes_detected'] / total,
            'false_positive_rate': self.quality_metrics['false_positives_estimated'] / total,
            'current_spike_threshold': self.current_spike_z,
            'current_drop_threshold': self.current_drop_rel,
            'detected_signal_type': self.signal_type,
            'adaptive_mode': self.adaptive_sensitivity
        }

    def reset_quality_metrics(self):
        """–ù–æ–≤–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–∫–∏–¥–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —è–∫–æ—Å—Ç—ñ."""
        if self.quality_metrics:
            self.quality_metrics = {key: 0 for key in self.quality_metrics}

    def get_adaptive_parameters(self) -> Dict:
        """–ù–æ–≤–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ—Ç–æ—á–Ω–∏—Ö –∞–¥–∞–ø—Ç–∏–≤–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤."""
        return {
            'current_spike_z': self.current_spike_z,
            'current_drop_rel': self.current_drop_rel,
            'base_spike_z': self.spike_z,
            'base_drop_rel': self.drop_rel,
            'signal_type': self.signal_type,
            'adaptation_active': self.adaptive_sensitivity
        }


class MultiSignalDetector:
    """
    –ü–û–ö–†–ê–©–ï–ù–ê –≤–µ—Ä—Å—ñ—è –º—É–ª—å—Ç–∏-—Å–∏–≥–Ω–∞–ª—å–Ω–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞.
    –ó–±–µ—Ä—ñ–≥–∞—î –≤—Å—ñ —ñ—Å–Ω—É—é—á—ñ –º–µ—Ç–æ–¥–∏ —Ç–∞ —Å–∏–≥–Ω–∞—Ç—É—Ä–∏.
    """
    def __init__(self, columns: Iterable[str], **det_kwargs):
        # ‚úÖ –ü–û–ö–†–ê–©–ï–ù–ù–Ø: –°—Ç–≤–æ—Ä—é—î–º–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∏ –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—é –ª–æ–≥—ñ–∫–æ—é
        self.detectors: Dict[str, SignalAnomalyDetector] = {}
        self.columns = list(columns)
        
        # ‚úÖ –ù–û–í–ï: –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        self.base_config = det_kwargs
        
        for c in self.columns:
            # ‚úÖ –ü–û–ö–†–ê–©–ï–ù–ù–Ø: –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É —Å–∏–≥–Ω–∞–ª—É
            signal_config = det_kwargs.copy()
            
            # –ï–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ —Å–∏–≥–Ω–∞–ª—ñ–≤
            if 'fe' in c.lower() and 'percent' in c.lower():
                signal_config['signal_type'] = 'composition'
            elif 'flow' in c.lower() or 'mass' in c.lower():
                signal_config['signal_type'] = 'flow'
            else:
                signal_config['signal_type'] = 'auto'
                
            self.detectors[c] = SignalAnomalyDetector(**signal_config)

    def clean_row(self, row: dict) -> dict:
        """
        –ü–û–ö–†–ê–©–ï–ù–ê –≤–µ—Ä—Å—ñ—è clean_row –∑ –ø–µ—Ä–µ—Ö—Ä–µ—Å–Ω–æ—é –≤–∞–ª—ñ–¥–∞—Ü—ñ—î—é.
        –ó–±–µ—Ä—ñ–≥–∞—î –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—É —Å–∏–≥–Ω–∞—Ç—É—Ä—É.
        """
        corr = {}
        
        # –°–ø–æ—á–∞—Ç–∫—É –æ–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–µ–Ω —Å–∏–≥–Ω–∞–ª
        for c, det in self.detectors.items():
            if c in row:
                # ‚úÖ –ü–û–ö–†–ê–©–ï–ù–ù–Ø: –ü–µ—Ä–µ–¥–∞—î–º–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω—ñ —Å–∏–≥–Ω–∞–ª–∏
                correlation_signal = None
                if 'fe' in c.lower() and any('flow' in other.lower() for other in row.keys()):
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –ø–æ—Ç–æ–∫–æ–≤–∏–π —Å–∏–≥–Ω–∞–ª –¥–ª—è Fe
                    flow_cols = [k for k in row.keys() if 'flow' in k.lower()]
                    if flow_cols:
                        correlation_signal = row[flow_cols[0]]
                elif 'flow' in c.lower() and any('fe' in other.lower() for other in row.keys()):
                    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ Fe —Å–∏–≥–Ω–∞–ª –¥–ª—è –ø–æ—Ç–æ–∫—É
                    fe_cols = [k for k in row.keys() if 'fe' in k.lower()]
                    if fe_cols:
                        correlation_signal = row[fe_cols[0]]
                
                corr[c] = det.update(row[c], correlation_signal=correlation_signal)
            else:
                corr[c] = 0.0  # –ó–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
        
        # ‚úÖ –ù–û–í–ï: –ü–æ—Å—Ç-–æ–±—Ä–æ–±–Ω–∞ –ø–µ—Ä–µ—Ö—Ä–µ—Å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è
        corr = self._cross_validate_signals(row, corr)
        
        return corr

    def _cross_validate_signals(self, original: dict, cleaned: dict) -> dict:
        """–ù–û–í–ê —Ñ—É–Ω–∫—Ü—ñ—è –ø–µ—Ä–µ—Ö—Ä–µ—Å–Ω–æ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –º—ñ–∂ —Å–∏–≥–Ω–∞–ª–∞–º–∏."""
        
        # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –ø–∞—Ä–∏ –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        fe_cols = [c for c in cleaned.keys() if 'fe' in c.lower()]
        flow_cols = [c for c in cleaned.keys() if 'flow' in c.lower()]
        
        if not fe_cols or not flow_cols:
            return cleaned
            
        # –í–∞–ª—ñ–¥—É—î–º–æ –Ω–∞–π–±—ñ–ª—å—à –≤–∞–∂–ª–∏–≤—ñ –ø–∞—Ä–∏
        fe_col = fe_cols[0]
        flow_col = flow_cols[0]
        
        if fe_col in original and flow_col in original:
            # –û–±—á–∏—Å–ª—é—î–º–æ –∫–æ—Ä–µ–∫—Ü—ñ—ó
            fe_correction = abs(cleaned[fe_col] - original[fe_col]) / (abs(original[fe_col]) + 1e-6)
            flow_correction = abs(cleaned[flow_col] - original[flow_col]) / (abs(original[flow_col]) + 1e-6)
            
            # –Ø–∫—â–æ –æ–¥–Ω–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è –∑–Ω–∞—á–Ω–æ –±—ñ–ª—å—à–∞ –∑–∞ —ñ–Ω—à—É - —Ü–µ –ø—ñ–¥–æ–∑—Ä—ñ–ª–æ
            if fe_correction > 0.15 and flow_correction < 0.05:
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ª–æ–≥—ñ—á–Ω—ñ—Å—Ç—å –∫–æ—Ä–µ–∫—Ü—ñ—ó Fe
                if original[flow_col] > 80 and fe_correction > 0.1:  # –í–∏—Å–æ–∫–∏–π –ø–æ—Ç—ñ–∫, –≤–µ–ª–∏–∫–∞ –∫–æ—Ä–µ–∫—Ü—ñ—è Fe
                    cleaned[fe_col] = original[fe_col] * 0.8 + cleaned[fe_col] * 0.2  # –ß–∞—Å—Ç–∫–æ–≤–µ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è
                    
            elif flow_correction > 0.2 and fe_correction < 0.05:
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –ª–æ–≥—ñ—á–Ω—ñ—Å—Ç—å –∫–æ—Ä–µ–∫—Ü—ñ—ó –ø–æ—Ç–æ–∫—É
                if original[fe_col] < 35 and cleaned[flow_col] > original[flow_col] * 1.3:  # –ù–∏–∑—å–∫–∏–π Fe, –∑–±—ñ–ª—å—à–µ–Ω–Ω—è –ø–æ—Ç–æ–∫—É
                    cleaned[flow_col] = original[flow_col] * 0.7 + cleaned[flow_col] * 0.3  # –ß–∞—Å—Ç–∫–æ–≤–µ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è
        
        return cleaned

    def clean_dataframe(self, df, in_place=False):
        """–Ü—Å–Ω—É—é—á–∏–π –º–µ—Ç–æ–¥ –±–µ–∑ –∑–º—ñ–Ω —Å–∏–≥–Ω–∞—Ç—É—Ä–∏, –∞–ª–µ –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–æ—é –ª–æ–≥—ñ–∫–æ—é."""
        if not in_place:
            df = df.copy()
            
        # ‚úÖ –ü–û–ö–†–ê–©–ï–ù–ù–Ø: –û–±—Ä–æ–±–ª—è—î–º–æ –ø–æ —Ä—è–¥–∫–∞—Ö –¥–ª—è –ø–µ—Ä–µ—Ö—Ä–µ—Å–Ω–æ—ó –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        for idx in range(len(df)):
            row_dict = {c: df.loc[idx, c] for c in self.detectors.keys() if c in df.columns}
            cleaned_row = self.clean_row(row_dict)
            
            for c in cleaned_row:
                if c in df.columns:
                    df.loc[idx, c] = cleaned_row[c]
                    
        return df

    # ‚úÖ –ù–û–í–Ü –ú–ï–¢–û–î–ò (–Ω–µ –ø–æ—Ä—É—à—É—é—Ç—å —ñ—Å–Ω—É—é—á–∏–π API)
    def get_comprehensive_report(self) -> Dict:
        """–ù–æ–≤–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∑–≤—ñ—Ç—É."""
        reports = {}
        for signal_name, detector in self.detectors.items():
            reports[signal_name] = detector.get_quality_report()
        
        # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_corrections = sum(r.get('corrections_made', 0) for r in reports.values())
        total_processed = sum(r.get('total_processed', 0) for r in reports.values())
        
        return {
            'individual_reports': reports,
            'overall_correction_rate': total_corrections / max(1, total_processed),
            'total_corrections': total_corrections,
            'total_processed': total_processed,
            'signal_types_detected': {name: r.get('detected_signal_type', 'unknown') 
                                    for name, r in reports.items()}
        }

    def reset_all_quality_metrics(self):
        """–ù–æ–≤–∏–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–∫–∏–¥–∞–Ω–Ω—è –≤—Å—ñ—î—ó —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏."""
        for detector in self.detectors.values():
            detector.reset_quality_metrics()

    def get_adaptation_status(self) -> Dict:
        """–ù–æ–≤–∏–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó."""
        status = {}
        for name, detector in self.detectors.items():
            status[name] = detector.get_adaptive_parameters()
        return status


# ===============================================
# –î–û–ü–û–ú–Ü–ñ–ù–Ü –§–£–ù–ö–¶–Ü–á –î–õ–Ø –Ü–ù–¢–ï–ì–†–ê–¶–Ü–á
# ===============================================

def create_enhanced_anomaly_detectors(anomaly_params: dict, columns: list = None) -> MultiSignalDetector:
    """
    –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ–∫—Ä–∞—â–µ–Ω–∏—Ö –¥–µ—Ç–µ–∫—Ç–æ—Ä—ñ–≤ –∑ —ñ—Å–Ω—É—é—á–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
    
    ‚úÖ –ü–æ–≤–Ω—ñ—Å—Ç—é –∑–≤–æ—Ä–æ—Ç–Ω–æ —Å—É–º—ñ—Å–Ω–∞ –∑ —ñ—Å–Ω—É—é—á–∏–º –∫–æ–¥–æ–º
    """
    if columns is None:
        columns = ['feed_fe_percent', 'ore_mass_flow']
    
    # –î–æ–¥–∞—î–º–æ –ø–æ–∫—Ä–∞—â–µ–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–æ —ñ—Å–Ω—É—é—á–∏—Ö
    enhanced_params = anomaly_params.copy()
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∞–∫—Ç–∏–≤—É—î–º–æ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è, —è–∫—â–æ –Ω–µ –≤–∫–∞–∑–∞–Ω–æ —ñ–Ω—à–µ
    if 'adaptive_sensitivity' not in enhanced_params:
        enhanced_params['adaptive_sensitivity'] = True
    if 'correlation_check' not in enhanced_params:
        enhanced_params['correlation_check'] = True
    if 'quality_monitoring' not in enhanced_params:
        enhanced_params['quality_monitoring'] = True
    
    return MultiSignalDetector(columns, **enhanced_params)

def monitor_anomaly_quality(detector: MultiSignalDetector, step: int, report_interval: int = 50):
    """
    –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É —è–∫–æ—Å—Ç—ñ –¥–µ—Ç–µ–∫—Ü—ñ—ó (–º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –≤ —ñ—Å–Ω—É—é—á–∏–π —Ü–∏–∫–ª).
    """
    if step % report_interval != 0 or step == 0:
        return
        
    report = detector.get_comprehensive_report()
    
    print(f"\nüìä –ó–í–Ü–¢ –ü–†–û –î–ï–¢–ï–ö–¶–Ü–Æ –ê–ù–û–ú–ê–õ–Ü–ô (–∫—Ä–æ–∫ {step}):")
    print(f"   üéØ –ó–∞–≥–∞–ª—å–Ω–∞ —á–∞—Å—Ç–æ—Ç–∞ –∫–æ—Ä–µ–∫—Ü—ñ–π: {report['overall_correction_rate']:.1%}")
    
    for signal, stats in report['individual_reports'].items():
        if stats.get('quality_monitoring', True):
            print(f"   üìà {signal}:")
            print(f"      ‚Ä¢ –ö–æ—Ä–µ–∫—Ü—ñ—ó: {stats['correction_rate']:.1%}")
            print(f"      ‚Ä¢ –¢–∏–ø —Å–∏–≥–Ω–∞–ª—É: {stats.get('detected_signal_type', 'auto')}")
            print(f"      ‚Ä¢ –ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π –ø–æ—Ä—ñ–≥: {stats['current_spike_threshold']:.2f}")
            
            if stats['correction_rate'] > 0.3:
                print(f"      ‚ö†Ô∏è  –í–ò–°–û–ö–ê –ß–ê–°–¢–û–¢–ê –ö–û–†–ï–ö–¶–Ü–ô!")


# ===============================================
# –†–ï–ö–û–ú–ï–ù–î–û–í–ê–ù–Ü –ü–ê–†–ê–ú–ï–¢–†–ò
# ===============================================

def get_recommended_anomaly_params():
    """
    –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–æ—ó —Ä–æ–±–æ—Ç–∏.
    –ú–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –∑–∞–º—ñ—Å—Ç—å —ñ—Å–Ω—É—é—á–∏—Ö.
    """
    return {
        "window": 15,                    # ‚úÖ –ó–º–µ–Ω—à–µ–Ω–æ –∑ 25
        "spike_z": 2.5,                  # ‚úÖ –ó–º–µ–Ω—à–µ–Ω–æ –∑ 4.0
        "drop_rel": 0.15,                # ‚úÖ –ó–º–µ–Ω—à–µ–Ω–æ –∑ 0.30
        "freeze_len": 4,                 # ‚úÖ –ó–º–µ–Ω—à–µ–Ω–æ –∑ 5
        "eps": 1e-9,
        "enabled": True,
        # –ù–æ–≤—ñ –ø–æ–∫—Ä–∞—â–µ–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
        "adaptive_sensitivity": True,     # ‚úÖ –ê–¥–∞–ø—Ç–∏–≤–Ω—ñ –ø–æ—Ä–æ–≥–∏
        "correlation_check": True,        # ‚úÖ –ü–µ—Ä–µ—Ö—Ä–µ—Å–Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è
        "quality_monitoring": True        # ‚úÖ –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —è–∫–æ—Å—Ç—ñ
    }